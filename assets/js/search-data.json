{
  
    
        "post0": {
            "title": "小额贷款的划算问题(matlab数学建模)",
            "content": "学校要求的参赛论文，顺手发上博客了，有错漏欢迎在评论区留言。 . 小额贷款的划算问题 . 摘要 . 为解决小额贷款对借贷者是否划算的问题，本文由相关金融理论基础入手，分析四还款方式：等额本金还款法、等额本息还款法、等本等息还款法、先息后本还款法。以此为切入点构造简单的模型，对借贷者的还款行为进行模拟。 . 针对问题一，根据金融理论基础，参考文献[2]，建模预测出该公司在四种还款方式下一年后实际需要还款款项及每期还款金额，比对出还款方式为等额本金还款法时对该广告公司最划算。同时，我还分析了不同还款方法下实际可能带来的危害。 . 针对问题二，参照《中华人民共和国民法典》[8]对各银行贷款利息和现有的合法小额贷款产品的规定，由问题一的模型，对比出对贷款公司的收益最大的还款方式。同时根据各种同类型产品的实际数据，设计出一款合法且有市场竞争力的小额贷款产品。 . 针对问题三，根据对问题一和问题二的探讨研究，以及相关政府政策，推测疫情期间小额贷款对于小微企业及个体商户有很大吸引力，各大银行的小额贷款比民间借贷的优惠更多，民间小额贷款产品的竞争力下降。 . 关键词 等额本金还款法、等额本息还款法、等本等息还款法、先息后本还款法、民法典 . 1 问题重述 . 1.1 问题背景 . 我国小额贷款公司的快速发展始于2008年《指导意见》颁布之后，银监会和中国人民银行虽然还没有正式确立小额贷款公司的金融机构地位（法律地位仍然属于工商企业），但也对小额贷款公司的合法地位给予肯定态度。[1]《指导意见》将小额贷款公司定义为，以面向个体工商户、小微型企业和“三农”发放小额、短期、分散、灵活贷款为主营业务、引导资金流向农村和欠发达地区，改善金融服务，不吸收公众存款的有限责任公司或股份有限公司。其具体相关细则的设置以《指导意见》为基础，各地区根据实际经济发展情况制定具体标准。 . 1.2 相关信息 . 等额本金还款法是指还款期内，把贷款总额等分，每月还相同数额的本金和剩余贷款在该月产生的利息。特点是每月还款金额递减（每月还款本金固定，利息越来越少），借款人起初还款金额较大，越往后每月还款金额越少。 . 等额本息还款法是指还款期内，把本金总额与利息总额相加，然后平均分摊到还款期内的每个月中。特点是每个月的还款金额固定，每月还款中的本金比重逐月递增，利息比重逐月递减。 . 等本等息还款法是指在还款期内，月还款总额不变，月还款本金不变，月还款利息不变。 . 先息后本还款法是指在还款期内，每月先偿还利息，到期后归还本金 . 1.3 需解决的问题 . 本文将问题述问题归结为以下三个部分，建立数学模型进行分析研究。 . 问题一：还款预测 . 原有某公司广告产品，借款1万，日息万分之三，收集相关资料，建模说明一年后实际需要还款多少和实际可能带来的危害。 . 问题二：设计小额贷款产品 . 按照民法典规定允许，比较各银行贷款利息和现有的合法小额贷款产品，为某贷款公司设计一款合法的小额贷款产品，使该产品在市场中具有较强竞争力。 . 问题三：总结小额贷款的实况 . 综合建模信息与相关资料，说明小额贷款的真实情况 . 2 问题分析 . 2.1 问题一 . 分析问题一，建立还款金额的数学模型，首先分析还款利息中的计算方式，以确定还款的确定方式。计算每月还款金额，根据四种不同还款方式分别计算每月还款利息以及本金，最后计算出借款人每月所需要还款的金额。再利用四种不同还款方式的每月还款金额计算出借款人需要还款的总金额，并分别计算出其中对应的总利息。最后分析设计模型给出的参数，优化模型，建立对借款人实际还款的模拟。 . 2.2 问题二 . 分析问题二，根据问题一给出的四个还款模型，比对各还款方式的还款金额与利息，参考相关资料给出的各地区贷款公司和各银行小额贷款的产品，选取对贷款公司有竞争力的还款方式作为小额贷款产品的还款方式。 . 2.3 问题三 . 分析问题三，针对实际的小额贷款生态，相关文献中已有详细论述。再根据前述问题一与问题二的模型分析，模拟小额贷款的真实情况，对其可靠性进行评估。 . 3 模型假设 . 1.该公司还款时间无延期，忽略实际政策影响导致的延期还款。 . 2.该公司具有长期的稳定收入，在还款期内，对还款金额有承受能力，忽略实际还款过程中的公司流动资金浮动造成的还款拖欠。 . 4 模型建立与求解 . 4.1 小额贷款的金融理论基础 . 企业或者个人在进行贷款时，在对贷款的偿还方式进行选择时，有多种选择，本文列举等额本息偿还，等额本金偿还，等本等息偿还和先息后本偿还。 . 4.2 符号说明 . 符号 符号说明 .   | 贷款额度 | .   | 总还贷期 | .   | 月利率 | .   | 现还款期数 | .   | 第n期还款金额 | .   | 总还款金额 | .   | 总还款利息 | . 4.3 等额本金还款法模型的建立 . 建立对每一期还款金额计算的模型[2]。该还款模式每一期所还的钱，为本金与利息之和。其中，本金是恒定不变的，而利息是逐期减少的。 . 第n期还款金额为： . 总还款金额为： . 总还款利息为： . 4.4 等额本息还款法模型的建立 . 建立对每一期还款金额计算的模型[2]。该还款模式每期所还的钱，即本金与利息之和是恒定不变的。它是一个固定的值，设为A。本息的组成，本金与利息两者的变化是不同的，本金是逐期的增加，而利息是逐期的减少，两者相加的和即本息又是一个恒定不变的，保持着一个动态的平衡。 . 第n期还款金额为： . 总还款金额为： . 总利息为： . 4.5 等本等息还款法模型的建立 . 建立对每一期还款金额计算的模型。该还款模式每一期偿还本金是不变的，每期偿还利息也是不变的。 . 第n期还款金额为： . 总还款金额为： . 总利息为： . 4.6 先息后本还款法模型的建立 . 建立对每一期还款金额计算的模型。该还款模式每期还款本金为0，每期只还利息，且利息不变，最后一期还总贷款额本金。 . 第n期还款金额为为： . 总还款金额为： . 总利息为： . 4.7 各模型的比较 . 根据上述各模型求解出的该公司一年后还款金额 . 表 1公司一年后还款金额 .   等额本金 等额本息 等本等息 先息后本 . 总还款金额 | 10585.0000 | 10594.6075 | 11080 | 11080 | . 总利息 | 585.0000 | 594.6075 | 1080 | 1080 | . 根据表中数据比较，针对小额贷款的还款总金额，还款方式为等额本金还款法时对该公司最划算，如果还款方式为等本等息还款法和先息后本还款法，则对发展前景不好的小型公司存在一定危险，还款金额大可能造成该公司资金链断裂。针对公司的运营状况，考虑到公司还款过程中存在的市场竞争，该公司也许更需要一个每期还款数额稳定的还款方式，即还款方式为等额本息还款法或等本等息还款法中的一种，又考虑到还款数额的问题，应当为等额本息还款法此时最为划算。 . 针对公司这一年里的运营情况，若该公司市场近期前景好，预期未来市场前景不明朗，则还款方式为等额本金还款法时最为划算，此时该公司可以在前期多还款后期少还款。如果还款方式为等本等息还款法，则该公司后期还款可能会导致资金链断裂。若该公司预期一年后有大笔盈利，则还款方式为先息后本还款法最为划算，此时该公司可以在前期尽量减少支出，将资金用于开拓市场，在最后一期中再偿还所有本金。若还款方式为先息后本还款法但是该公司预期一年后没有大的盈利，则会有造成公司资不抵债的可能。 . 4.8 模型评价 . 4.8.1 模型的优点 . 根据不同还款方式建立还款推导公式，考虑到每期还款金额的变化，提出计算每期还款金额的方法，模型对问题的模拟良好。 . 4.8.2 模型的缺点 . 四大模型未能引入政策因素以及经济因素对问题进行模拟，模拟结果与实际情况有一定的误差，可靠性较低。 . 4.8.3 模型的改进方案 . 将政策因素和经济因素设置为变量(POL和ECO)，导入模型中，获得更高的可靠性。 . 5 设计小额贷款产品 . 5.1 建设银行的小额贷款产品与某贷款公司的产品 . 5.1.1 建设银行的善融商务个人小额贷款 . 贷款对象： . (1).建行个人网银盾客户，并且已成功注册为善融商务会员和开通商城的账户。 . (2).在建设银行开立个人银行结算账户。 . 贷款期限： . 贷款额度有效期最长不超过1年（含）;个人小额贷款额度项下支用的单笔贷款的期限最长不超过1年（含）。 . 贷款额度： . 个人小额贷款预授信额度最低1000元，最高30万元，客户可在该额度项下申请支用贷款。 . 贷款用途： . 个人小额贷款支持购买建设银行善融商务个人商城签约商户销售的商品或服务。 . 还款方式及还款日： . 个人小额贷款的还款方式为委托扣款方式，还款日为贷款发放日的每月对日，客户须在还款日前将还款金额存入合同约定的还款账户内，建设银行在还款日从约定还款账户自动扣划。 . 利息参考： . 参照2020年6月贷款基准利率表。 . 表 2 2020年6月贷款基准利率表 . 各项贷款 利率 公积金 利率 . 一年以内(含一年) | 4.35 | 五年以下(含五年) | 2.75 | . 一至五年 (含五年) | 4.75 |   |   | . 五年以上 | 4.90 | 五年以上 | 3.25 | . 以上数据来源于金投网[6] . 5.1.2 某公司暖心借产品 . 贷款对象：有实名制手机号正常使用6个月以上，且有社保或信用卡(二选一) . 的22-55周岁公民。 . 贷款所需资料：身份认证、人脸识别、个人基础信息、工作信息、联系人、银行卡、社保或公积金(二选一)。 . 贷款额度：最高50000。 . 贷款周期：12个月。 . 利息参考： . 因为目前暂时找不到该产品的实际使用者，以借款的时候实际的利息费率为准。 . 5.2 小额贷款产品的还款方式 . 图 1四种还款方式每期还款对比图 . [CHART] . 图 2前三种还款方式每期还款对比图 . [CHART] . 根据对问题一的建模分析，可以得到四种还款方式对比图。表格1、图表1和图表2显示，等本等息还款法与先息后本还款法总利息最高，等额本金还款法总利息最低。但是等额本金还款法前期还款压力大，等额本息每月还款金额固定，利息比等额本金还款法更高，比等本等息还款法更低。实际市场中小额贷款公司通常是做无抵押贷款产品，所以贷款公司承担风险较大，因而对贷款收益要求更高。因此小额贷款公司要求贷款利率相对较高，在还款方式上，选择最大化收益的等本等息还款法或先息后本还款法作为公司小额贷款产品的还款方式。考虑贷款公司需要稳定的还款收益，选取等本等息还款法作为公司小额贷款产品的还款方式，该还款方式同时可以避免借款人拖欠债务或资不抵债。 . 5.3 小额贷款产品的利率 . 根据对建设银行的小额贷款产品与某贷款公司的产品的利率以及附录中的部分银行2020年1月贷款利率表，可推断全国范围内银行利率在4.35~5.6之间是合理的。根据央行相关规定，小额贷款公司的自营贷款利率和接受的委托贷款利率，必须控制在央行公布的同期同档次贷款基准利率４倍以内。为追求贷款产品最大化收益且保证在贷款利率上一定的市场竞争力，该小额贷款的利率设计为17.4%。 . 5.4 小额贷款产品的附加条件 . 根据《中华人民共和国民法典》[8]第六百六十九条、第六百七十二条的规定。设计该贷款产品应依据法典合法收集借款人的相关资料以及信用信息作为评判是否发放贷款的标准之一。在贷款期限内，设计要求定期审查借款人的有关财务会计报表或者其他资料。 . 建行的善融小额贷款产品表明，贷款款项可以指定市场对象与特定的客户群体。设计该贷款产品定向投资某一领域的某一用途。参考相关文献[3][4]以及银行信息网的相关资料，设计该产品的客户群体为农牧民，个体工商户以及小微企业。该客户群体是具有生产经营能力以及贷款偿还能力，且得不到所需的金融服务,没有充分享受到金融便利的群体。设计该产品的主要市场对象为网络运营贷款。该市场的前景比较明朗，市场还未饱和。 . 针对该贷款产品对贷款公司的风险，设计符合《民法典》的风险规避措施，以期减小运营风险。根据《中华人民共和国民法典》[8]第六百七十一条，第六百七十六条、第六百七十七条、第六百七十八条、第六百八十条的规定，在合同中添加条款，当借款人逾期未付还款款项时，我方有权申请冻结借款人的银行卡，有权向法院提起诉讼要求借款人及时归还款项。如若借款人没有能力归还贷款，则诉讼要求借款人以不动资产进行抵押还贷。该合同自贷款人提供借款时生效。对于贷款过程中的还款纠纷，以《中华人民共和国民法典》和相关法律条文以及实际案例为标准进行诉讼。 . 6 小额贷款的实况 . 根据参考文献[3][4][5]，可以得出以下结论：小额贷款公司随着国家和地方政府对小额贷款公司政策的变化，小额贷款公司只有具备了充足的财务资源与长远的财务战略、有活力的财务能力与财务执行力、持续的财务创新能力才能保持长久的市场竞争力。结合现今受疫情影响而导致的经济不景气，部分小微企业及个体商户需要资金支撑其度过经济下行期。小额贷款公司只要选好有发展前景或占当地市场份额较大的客户群体发放小额贷款，刺激当地经济运行，可以预见的是当报复性消费到来时，小额贷款公司收益有大幅增长的可能性较大。 . 根据中国人民银行的资料显示，近期尽管实行金融支持稳企业保就业政策，刺激经济增长。疫情期间，第三产业迎来困难期，且恢复起来相对较慢。对于第三产业的相关贷款，各类贷款都要求对相关资料严格审查及评估，避免发生借款人资不抵债的情况。根据中国人民银行对金融支持稳企业保就业政策的预测，该政策预计带动相关地方法人银行新发放普惠小微贷款近1万亿元，大幅提高小微企业信用贷款发放比例。同时，各地方政府也出台了相关的地方性政策给贷款开绿灯，如《深圳市应对新型冠状病毒肺炎疫情中小微企业贷款贴息项目实施办法》中就计划拨款10%的市级产业专项资金重点用于贷款贴息。这些政策的出台会导致各地银行贷款的竞争力上升，民间小额贷款的竞争力相对下降。短期内，商业银行与国有银行的小额贷款将大受欢迎，民间小额贷款前景较差。长期来看，民间小额贷款的无抵押贷款占据优势，结合网络贷款办理手续的简化，市场前景明朗。 . 参考文献 . 戴川业. 四川瀚华小额贷款产品资产证券化研究[D].湘潭大学,2019. . | 赵毓.等额本金与等额本息偿还贷款的利息研究——基于会计视角[J].科技资讯,2018,16(35):243-244. . | 魏玲丽,王能军.小额贷款公司财务竞争力评价指标体系研究[J].经济师,2020(04):124-125. . | 肖方良.当前商业银行发展小额贷款业务的难点与对策[J].海峡科学,2019(06):44-46+49. . | 涂斌. 庐山农村商业银行小额贷款业务存在的问题与对策研究[D].华东交通大学,2019. . | 金投网.2020年6月1日最新银行贷款利率表[EB/OL]. https://bank.cngold.org/c/2020-06-01/c7060639.html,2020-06-01. . | 银行信息港.2020年银行贷款利率表[EB/OL]. https://www.yinhang123.net/lltz/daikuanlilvbiao/1354405.html . | ,2019-12-03. . 人民网－人民日报.中华人民共和国民法典[EB/OL]. http://legal.people.com.cn/n1/2020/0602/c42510-31731656.html | ,2020年06月02日. . 人民银行网站.人民银行举行金融支持稳企业保就业新闻发布会[EB/OL]. http://www.gov.cn/xinwen/2020-06/03/content_5516920.htm,2020-06-03. | 附录 . “problem.mlx “ . clc;close all; . %% 获取数据 . day_i=3/10000 %日息 . number_of_periods_i=day_i*30 %月利率 . P=10000 %借款金额 . N = 12 %还款期数 . %% 模型选取及计算 . Amount=P %借款金额 . rate=number_of_periods_i %月利率 . number_of_periods=N %贷款期数 . command=input(‘输入0为等额本金还款，输入1为等额本息还款，输入2为等本等息还款，输入3为先息后本还款： ‘) . if command==0 . %等额本金还款方式 . M=Amount/number_of_periods %每月偿还本金 . for j=1:number_of_periods-1 . Amount(j+1)=Amount(j)-M %月初余额 . end . R=Amount*rate %月底的利息 . debt=Amount+R %月底欠款 . M1=M+Amount*rate %月还款额 . sum_R=sum(R) %总利息 . sum_Amount=sum(M1) %还款总额 . M=M-zeros(1,number_of_periods) . data=[Amount’,R’,debt’,M’,M1’]%月初所欠金额 利息额 月末所欠金额 偿还本金 月末付款 . [n,m]=size(data) . result= cell(n+1,m) . result(1,:)={‘月初所欠金额’,’利息额’,’月末所欠金额’,’偿还本金’,’月末付款’} . result(2:end,:) = num2cell(data) . xlswrite(‘data.xlsx’,result,’等额本金还款方式’)%输出excel . fprintf(‘sum_R=%f’,sum_R) . fprintf(‘sum_Amount=%f’,sum_Amount) . elseif command==1 . %等额本息还款方式 . M=Amount*(rate*(1+rate)^number_of_periods)/((1+rate)^number_of_periods-1)%月还款额 . for i=1:number_of_periods-1 . Amount(i+1)=Amount(i)*(1+rate)-M %月初金额 . end . R=Amount*rate %月底的利息 . mon=M-R %每月偿还本金 . debt=Amount+R %月底欠款 . sum_R=sum(R) %总利息 . sum_Amount=M*number_of_periods %还款总额 . M=M-zeros(1,number_of_periods) . data=[Amount’,R’,debt’,M’-R’,M’]%月初所欠金额 利息额 月末所欠金额 偿还本金 月末付款 . [n,m]=size(data) . result= cell(n+1,m) . result(1,:)={‘月初所欠金额’,’利息额’,’月末所欠金额’,’偿还本金’,’月末付款’} . result(2:end,:) = num2cell(data) . xlswrite(‘data.xlsx’,result,’等额本息还款方式’)%输出excel . fprintf(‘sum_R=%f’,sum_R) . fprintf(‘sum_Amount=%f’,sum_Amount) . elseif command==2 . %等本等息还款方式 . M=Amount/number_of_periods %月还本金 . for i=1:number_of_periods-1 . Amount(i+1)=Amount(i)-M %月初金额 . end . R=[P*rate,P*rate,P*rate,P*rate,P*rate,P*rate,P*rate,P*rate,P*rate,P*rate,P*rate,P*rate] %月底的利息 . debt=Amount+R %月底欠款 . M2=M+R %月还款额 . sum_R=sum(R) %总利息 . sum_Amount=M*number_of_periods+P*rate*number_of_periods %还款总额 . M=M-zeros(1,number_of_periods) . data=[Amount’,R’,debt’,M’-R’,M2’]%月初所欠金额 利息额 月末所欠金额 偿还本金 月末付款 . [n,m]=size(data) . result= cell(n+1,m) . result(1,:)={‘月初所欠金额’,’利息额’,’月末所欠金额’,’偿还本金’,’月末付款’} . result(2:end,:) = num2cell(data) . xlswrite(‘data.xlsx’,result,’等本等息还款方式’)%输出excel . fprintf(‘sum_R=%f’,sum_R) . fprintf(‘sum_Amount=%f’,sum_Amount) . elseif command==3 . %先息后本还款方式 . M=0 %月还本金 . for i=1:number_of_periods-1 . Amount(i+1)=Amount(i)-M %月初金额 . end . R=Amount*rate %月底的利息 . debt=Amount+R %月底欠款 . M3=R %月还款额 . sum_R=sum(R) %总利息 . M4=debt . sum_Amount=P+sum_R %还款总额 . last_number_of_periods_debt=P+P*rate %最后一个月还款金额 . M=M-zeros(1,number_of_periods) . data=[Amount’,R’,debt’,M’,M3’]%月初所欠金额 利息额 月末所欠金额 偿还利息 月末付款 . [n,m]=size(data) . result= cell(n+1,m) . result(1,:)={‘月初所欠金额’,’利息额’,’月末所欠金额’,’偿还本金’,’月末付款利息’} . result(2:end,:) = num2cell(data) . xlswrite(‘data.xlsx’,result,’先息后本还款方式’)%输出excel . fprintf(‘sum_R=%f’,sum_R) . fprintf(‘sum_Amount=%f’,sum_Amount) . end . 表 3部分银行2020年一月贷款利率表 . 银行 短期贷款 中长期贷款 个人住房公积金贷款 贴现         .   | 六个月（含） | 六个月至一年（含） | 一至三年（含） | 三至五年（含） | 五年以上 | 五年以下（含） | 五年以上 |   | . 央行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 工商银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 农业银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 建设银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 中国银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 交通银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 | 以再贴现利率为下限加点确定 | . 招商银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 中信银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 光大银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 | 以再贴现利率为下限加点确定 | . 浦发银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 深圳发展银行 | 5.6 | 6 | 6.15 | 6.4 | 6.55 | 4 | 4.5 | 以再贴现利率为下限加点确定 | . 平安银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 广发银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 | 以再贴现利率为下限加点确定 | . 华夏银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 民生银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 兴业银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 东亚银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 北京银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 中国邮政储蓄银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 德州银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 盛京银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 贵阳银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 丹东银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 哈尔滨银行 | 4.6 | 4.6 | 5 | 5 | 5.15 | 2.75 | 3.25 |   | . 洛阳银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 富滇银行 | 4.35 | 4.35 | 4.75 | 4.75 | 4.9 | 2.75 | 3.25 |   | . 吉林银行 | 4.6 | 4.6 | 5 | 5 | 5.15 | 2.75 | 3.25 |   | . 以上数据来源于银行信息港[7] .",
            "url": "https://jumbojing.github.io/jupyter_notebook_blogs//jupyter_notebook_blogs/2021/02/17/%E5%B0%8F%E9%A2%9D%E8%B4%B7%E6%AC%BE%E7%9A%84%E5%88%92%E7%AE%97%E9%97%AE%E9%A2%98(Matlab%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1).html",
            "relUrl": "/2021/02/17/%E5%B0%8F%E9%A2%9D%E8%B4%B7%E6%AC%BE%E7%9A%84%E5%88%92%E7%AE%97%E9%97%AE%E9%A2%98(Matlab%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1).html",
            "date": " • Feb 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Examlecn",
            "content": "python hide_input=false #default_exp tutorial from nbdev.showdoc import show_doc . 最小的例子 . 从头开始创建nbdev项目的最小端到端示例。 . 第1步：设置您的nbdev GitHub存储库 . | 步骤2：修改配置文件 . 编辑settings.ini | . | 步骤3：编写代码（或复制/粘贴到笔记本中） . 标志 . | 魔术发生的地方：编写文档和测试 . | 编辑index.ipynb . | . | 步骤4：将笔记本转换为Python模块和文档 . 预览文档 . 注释说明： | . | show_doc . 关于show_doc的重要说明： | . | 刷新文档 . | . | 第5步：将文件推送到GitHub并查看托管文档 . | 步骤6：添加更多代码 . 测验 . 注释说明： | . | . | 步骤7：将Python模块发布到Pypi . | 参考 . 现场演示 | . | . 本节假设您已完成本 教程 。 以下是一个从头开始创建nbdev项目的最小示例，其中有一些注释说明了某些事情为何以这种方式工作。 . 对于此示例，我们将使用 Allen Downey出色著作《 Think Python 2》中的代码 ，尤其是 Card 模块。 我们将 nbdev 专注于为您提供足够的信息以提高生产力， 因此不会涵盖的所有功能 。 我们建议 您在完成本示例后 阅读其余 文档 。 . 第1步：设置您的nbdev GitHub存储库 . 根据 教程中 的说明 ，我们将使用 模板 创建一个新的存储库 。 在这种情况下，我们创建一个名为 deck_of_cards 的存储库 ： . 注意： 如果您打算编写可安装的Python模块，我们强烈建议您将repo命名为python模块。 . . 完成此操作后，您将获得一个包含必要文件的存储库以开始使用。 您还应该 按照 本教程中 的说明 安装nbdev 并 安装githooks 。 . 步骤2：修改配置文件 . 编辑settings.ini . 要使nbdev正常工作，需要 编辑settings.ini 。 这些设置用于填充必需的信息，以供您在 GitHub Pages 上 托管 文档 ，以及 将模块作为软件包发布到pypi 。 . 这些是我们在 settings.ini中 更改的字段 ： . lib_name = deck_of_cards . 描述 = “使用来自Allen Downey的Think Python 2nd Ed的代码编写的nbdev的最小示例” . 关键字 = nbdev . 作者 = Hamel Husain . author_email = hamel@example.com . 版权 = Hamel，inc。 . 用户 = fastai . 注意： 在此示例中，用户的值实际上并未更改，但是您必须将其更改为与GitHub上的GitHub存储库（通常是用户名）相对应的org。 . settings.ini文件中的值会自动为您传播到各种系统，这有助于最小化样板并学习复杂的配置文件。 例如， . 该 author 和 author_email 字段由读取 setup.py 的Python包装。 . | 在 lib_name 被两个由 setup.py 和 杰奇 的配置文件， _config.yaml 以确保呈现文档都在GitHub上的页面配置正确。 . | . 步骤3：编写代码（或复制/粘贴到笔记本中） . 接下来，我们 从ThinkPython2存储库中 获取 Card模块， 并将其写入nbdev中。 . 第一步是将 Card 类从 Card.py 复制并粘贴 到新的Jupyter笔记本中，我们将其命名为 00_card.ipynb 。 从python文件复制和粘贴代码是将现有python脚本转换为Jupyter笔记本的合理方法。 将大量代码复制到笔记本中的一个有用技巧是将整个文件复制到一个单元格中，然后 ctrl-shift-minus 将代码拆分为多个单独的单元格。 . 如果您尝试将现有的python项目转换为 nbdev ，我们建议您逐步将特定文件逐步转换为 nbdev 。 具体来说，我们建议选择一个python文件， card.py 如下所示。 . 注意： 文件名开头的数字不是必需的； 这是一种惯例，当笔记本按文件系统排序时，我们可以使笔记本保持所需的顺序。 . 在 笔记本 的第一个单元格中 ，写一个看起来像这样的注释（这不是必需的，但是我们在这里这样做是为了突出nbdev的重要功能）： . ＃default_exp卡 . 在这种情况下，该参数 card 指定 card.py 默认情况下 将从此笔记本导出的代码放置在目标位置 。 您可以 在此处 阅读有关如何从笔记本创建python模块的更多信息 。 安排笔记本的合理方法如下： . . 标志 . nbdev使用特殊的注释或标志作为一种标记语言，使您可以控制文档的各个方面以及如何将代码导出到模块以及如何测试代码。 除之外 default_exp ，此笔记本中还存在以下其他标志： . #hide . 注意： 此注释指示nbdev在生成文档时隐藏此单元格。 . #export . 注意： 此注释指示nbdev将此单元导出到适当的python文件。 如果未为提供参数 export ，则默认为上述指定的模块 default_exp 。 . 魔术发生的地方：编写文档和测试 . 在原始代码库中，Card的测试是分开的，位于 Card_test.py中 。 此外，的文档 Card 主要位于 Allen仓库 的 book文件夹中 ，而某些文档也位于文档字符串中。 尽管这是Python项目的典型安排，但我们认为 nbdev 可以通过将文档，测试和源代码组织到一个上下文中来简化您的工作流程。 我们认为，这使开发人员可以编写更高质量的文档和代码，并鼓励进行更多测试。 . 这是Card的文档+代码： . . 这些注释和测试由文档系统提供，将在后面的部分中进行讨论。 此外， 默认情况下 ，assert语句自动成为由 持续集成系统设置 在GitHub存储库 中的nbdev 中 运行的测试 。 . 注意： nbdev编程环境 为您 设置了 持续集成（CI） 系统。 您无需执行任何其他操作即可启用它，它会立即开始工作。 对于没有CI经验的人来说，这尤其好。 这是开始使用它的一种温和方法。 . 重要提示： fastcore的测试实用 程序提供了 一些测试实用程序 ，这些 实用程序提供了 对 常用断言语句类型的包装，还提供了更好的默认错误消息。 使用这些是可选的，但建议使用。 . 编辑index.ipynb . nbdev 存储库需要一个名为的笔记本 index.ipynb ，当您使用模板时，该 笔记本将 包含在您的存储库中。 index.ipynb 有两个目的： . 它成为您的仓库的自述文件（此笔记本转换为 README.md ） . | 它成为 index.html 文档的 主页（ ）。 . | 您会注意到以下样板 index.ipynb ： . 从 your_lib.core 导入 * . 您应该删除此行代码或将其注释掉，因为这将导致语法错误。 稍后，当您完成模块的创建时，可以将其替换为适当的import语句。 我们在此故意保留了此行，以便您可以体验持续集成系统（如上所述）如何警告您错误。 . 步骤4：将笔记本转换为Python模块和文档 . nbdev_build_lib 从仓库的根目录运行命令 。 这会将标 #export 有的 笔记本单元导出 到适当的python模块。 例如，笔记本 00_cards.ipynb 被转换到 card.py 。 . 运行命令nbdev_test_nbs 以运行代码并在所有笔记本中进行测试。 nbdev 还会通过 持续集成系统设置为您 运行此命令 ，但是在本地运行这些测试以获得即时反馈很有用。 . 注意： 有一种方法可以通过使用 此处描述的 特殊标签来选择性地跳过某些运行时间长或速度慢的测试 . 预览文档 . 要预览文档，请 make docs_serve 从存储库的根目录 运行命令 。 此命令 nbdev_build_docs 为您在后台 运行CLI命令 ，该 命令 从笔记本生成文档站点。 运行此命令后，您将在终端中看到一个URL，指示文档在本地托管的位置。 对于 本例中使用 的 fastai / deck_of_cards 存储库，URL为 http://127.0.0.1:4000/deck_of_cards/ . 如果您导航到的卡片页面 http://127.0.0.1:4000/deck_of_cards/card.html ，您将看到我们刚刚编写的文档，我们对其进行了注释，以供进一步说明： . . 注释说明： . 标题 卡 与 H1 笔记本中 的第一个 标题 相对应， 并带有注释块 API Details 作为摘要。 . | nbdev 自动为您呈现一个目录。 . | nbdev 自动将您的类或函数的签名呈现为标题。 . | nbdev 在GitHub上自动将链接添加到相应的源代码（这是纯文本python文件）。 请记住， nbdev 使用命令将Jupyter笔记本转换为源代码 nbdev_build_lib 。 . | 这部分文档是从文档字符串自动呈现的。 . | 笔记本的其余部分通过将markdown转换为HTML，显示每个单元格（包括图表和图像）的输入和输出等来呈现。 您可以使用 此页面上描述 的 标志 隐藏整个单元格，仅隐藏单元格输入或仅隐藏输出 。 . | nbdev支持特殊的块引号，这些引号在文档中显示为彩色框。 您可以 在此处 阅读有关它们的更多信息 。 在此特定示例中，我们使用 Note 块引用。 . | 在反引号中包含的单词将在适当的情况下自动超链接到关联的文档。 这是一个简单的情况，其中在 Card 上面紧接定义了类，但是这适用于页面和模块。 在后面的步骤中，我们将看到另一个示例。 . | show_doc . show_doc 允许您控制文档在文档中的显示方式。 您可以控制文档呈现方式的位置，顺序，标题和其他详细信息。 您可以 在此处了解 更多 信息 。 例如，这是您可以 show_doc 用来呈现 __eq__ 方法 的文档的 方式 Card （请注意，测试自然包含在文档下方）： . . 有关的重要说明 show_doc ： . 对于函数和类，show_doc默认情况下会 在定义函数或类的相同位置自动调用 。 这就是为什么 Cards 即使 show_doc 从未显式调用过 上面docs示例中 的类的标题的原因 。 . 您可以通过显式调用 show_doc 所需位置 来覆盖此默认设置 。 | . | 对于方法，您必须要求show_doc 显示文档标题，如 Card.__eq__ 上面 的 方法所示。 这是设计使然，因为与函数不同，您通常在单个连续的代码块中定义类的所有方法。 因此， show_doc 允许您控制方法的文档标题的顺序和位置，从而帮助您为在适当标题下组织的每种方法编写散文和测试。 . | 我们建议您 show_doc 通过编辑笔记本并重新渲染文档（如下所述）进行试验，以了解在不同情况下会发生什么。 . | . 刷新文档 . 如果要编辑文档，则可以对相应的笔记本进行更改并运行， nbdev_build_docs 然后 在浏览器中进行 硬刷新 以重新呈现文档。 . 第5步：将文件推送到GitHub并查看托管文档 . 此步骤假定您已 启用GitHub页面 。 . 至此，我们准备将您的第一个文件推送到GitHub。 如果您已 按照本教程中的说明安装了git hooks ， nbdev 它将自动从笔记本中清除不必要的元数据，以避免冲突和过于冗长的差异。 首次将文件推送到GitHub之前，我们建议运行命令， git status 以便您可以查看由生成的所有文件 nbdev 。 您将注意到已创建以下文件： . .py 与您创建的笔记本相对应的文件，位于与库名称相对应的文件夹中，在本例中，该文件夹称为 deck_of_cards 。 例如，将 __init__.py 在适当的目录中自动创建 一个 文件，以组织一个python模块。 . | 文件 docs/ 夹中的 文档站点 文件。 该目录包含HTML，CSS和其他文件，这些文件用于在GitHub Pages上托管您的文档站点。 . | . git add 在推送到GitHub之前， 请确保将所有这些文件添加到提交中 ，因为 所有这些文件 都可以正常工作。 . 将文件推送到GitHub将 使用GitHub Actions 自动触发 持续集成（CI） 。 CI将自动执行 此处概述 的许多检查 。 您可以通过导航到GitHub存储库中的“操作”选项卡来查看GitHub动作中正在运行的CI流程。 . 推送文件后，GitHub将自动重建您的文档。 您可以通过转至存储库设置并在选项下找到GitHub Pages部分来查看文档构建状态。 当GitHub正在构建您的页面时，它将如下所示： . . 页面构建完成后，颜色和状态消息将变为如下所示： . . 此外，假设您已经 启用了GitHub Pages ，则可以随时查看Github Pages部署的状态。 如果添加 /deployments 到存储库的GitHub URL，您将看到一个部署仪表板。 例如，以下是 在推送新文件后 的 https://github.com/fastai/deck_of_cards/deployments 的屏幕截图 ： . . 步骤6：添加更多代码 . 恭喜，您使用nbdev编写了第一段代码！ 但是，要完全掌握nbdev的工作方式，值得在新笔记本中添加其他代码，以导入您之前编写的代码。 接下来，我们将 Deck类形式的cards.py 添加 到名为 01_deck.ipynb 的新笔记本中 。 该笔记本将导入先前创建的 Card 类，并创建一个 Deck ，它是Card的集合： . . 与先前的笔记本类似，第一个单元格具有nbdev标志 # default_exp deck ，这意味着标记为的代码块 #export 将 deck.py 默认 导出到文件中 。 您可以看到我们导入了 Card 对象，并 deck.py 使用以下代码单元 将该代码导出到 ： . #export . from deck_of_cards.card 导入 卡 . 之所以 能够成功 ，是 因为cli命令已 nbdev_build_lib 转换 00_card.ipynb 为 card.py ，我们已经在此处 将 其导入。 . 测验 . 唐尼的代码 Deck 在名为 Card_test.py 的单独文件中 包含对该 类 的测试 。 该文件是一个很好的例子，突出了的优势 nbdev 。 该文件的内容如下： . “”该文件包含与 . Allen B. Downey . 一起使用的“ Think Stats”代码，可从greenteapress.com获得。版权所有2014 Allen B. Downey . 许可：GNU GPLv3 http://www.gnu.org/licenses/gpl。 html . “”“ . 来自 __future__ import print_function ， 部门 . 从Card导入卡，Deck导入unittest . 类 测试（单元测试，测试用例）： . DEF testDeckRemove （自）： 甲板 = 甲板（） card23 = 卡（2 ， 3 ） 的甲板。remove _card （card23 ） . 如果 __name__ == “ __main__ ” ： . unittest 。主要（） . 由于以下原因，上面显示的代码有问题： . 目前尚不清楚测试的目的是什么。 . | 该测试位于与实现分开的文件中，因此您必须打开多个窗口和/或切换上下文以了解该测试。 . | 测试使用一个api， unittest 您必须学习并考虑一下是否要编写测试。 . | 测试与文档以及与解释类的内容相关的任何散文是分开的 Deck 。 . | . 所有这些问题都在nbdev中处理，因为您可以在同一上下文中编写代码，文档和测试。 以下是 01_deck.ipynb 的相关部分的屏幕快照，该屏幕快照 以更具可读性和表现力的方式表示代码和此测试： . . 上面的代码表示相同的单元测试，但也将文档与Deck的原始实现集成在一起。 您可以在GitHub上查看笔记本电脑 在这里 。 此笔记本中显示的另一个附加工具是 nbdev function show_doc ，它使您可以控制文档的放置。 在此示例中， showdoc(Deck.remove_card) 将在文档中创建一个带有适当标题的部分。 . 如果运行CLI命令 make docs_serve ，则可以在本地预览这些文档的外观。 以下是带注释的屏幕快照： . . 注释说明： . 在编写这些文档时，我们只 Card 用反引号 括起来 。 nbdev 自动将其转换为指向文档相应页面的超链接 Card 。 . | 该方法的标题 Deck.remove_card 是由创建的 show_doc 。 . | nbdev 旨在鼓励您将测试作为文档的一部分编写，如下所示。 . | 您可以在 https://fastai.github.io/deck_of_cards/deck.html 上实时查看此页面 。 . 完成后，请确保在推送到GitHub之前运行以下cli命令。 . nbdev_build_lib ：这会将您的笔记本转换为模块。 . | nbdev_build_docs ：这将生成您的文档站点。 . | nbdev_test_nbs ：这将运行您的所有测试（这是一个好主意，因此您可以捕获错误）。 . | git status 查看哪些文件已更改，这是一个很好的练习，它是首次入门时 nbdev 了解自动生成的文件的方法。 . | . 步骤7：将Python模块发布到Pypi . 您可以按照 以下说明 将模块发布到pypi 。 . 参考 . 该示例的所有代码都可以在GitHub repo fastai / deck_of_cards上找到 。 . 现场演示 . 以下视频显示了此示例的现场演示，最后是“问答”部分。 .",
            "url": "https://jumbojing.github.io/jupyter_notebook_blogs//jupyter_notebook_blogs/2021/02/17/examleCN.html",
            "relUrl": "/2021/02/17/examleCN.html",
            "date": " • Feb 17, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Deck",
            "content": "",
            "url": "https://jumbojing.github.io/jupyter_notebook_blogs//jupyter_notebook_blogs/2021/02/17/deck.html",
            "relUrl": "/2021/02/17/deck.html",
            "date": " • Feb 17, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Card",
            "content": "",
            "url": "https://jumbojing.github.io/jupyter_notebook_blogs//jupyter_notebook_blogs/2021/02/17/card.html",
            "relUrl": "/2021/02/17/card.html",
            "date": " • Feb 17, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "2019新型冠状病毒（COVID-19/2019-nCoV）疫情分析",
            "content": "&#37325;&#35201;&#35828;&#26126; . 分析文档：完成度：代码质量 3:5:2 . 其中分析文档是指你数据分析的过程中，对各问题分析的思路、对结果的解释、说明(要求言简意赅，不要为写而写) . ps:你自己写的代码胜过一切的代笔，无关美丑，只问今日比昨日更长进！加油！ . 由于数据过多，查看数据尽量使用head()或tail()，以免程序长时间无响应 . ======================= . 本项目数据来源于丁香园。本项目主要目的是通过对疫情历史数据的分析研究，以更好的了解疫情与疫情的发展态势，为抗击疫情之决策提供数据支持。 . &#19968;. &#25552;&#20986;&#38382;&#39064; . 从全国范围，你所在省市，国外疫情等三个方面主要研究以下几个问题： . （一）全国累计确诊/疑似/治愈/死亡情况随时间变化趋势如何？ . （二）全国新增确诊/疑似/治愈/死亡情况随时间变化趋势如何？ . （三）全国新增境外输入随时间变化趋势如何？ . （四）你所在的省市情况如何？ . （五）国外疫情态势如何？ . （六）结合你的分析结果，对个人和社会在抗击疫情方面有何建议？ . &#20108;. &#29702;&#35299;&#25968;&#25454; . 原始数据集：AreaInfo.csv，导入相关包及读取数据： . r_hex = &#39;#dc2624&#39; # red, RGB = 220,38,36 dt_hex = &#39;#2b4750&#39; # dark teal, RGB = 43,71,80 tl_hex = &#39;#45a0a2&#39; # teal, RGB = 69,160,162 r1_hex = &#39;#e87a59&#39; # red, RGB = 232,122,89 tl1_hex = &#39;#7dcaa9&#39; # teal, RGB = 125,202,169 g_hex = &#39;#649E7D&#39; # green, RGB = 100,158,125 o_hex = &#39;#dc8018&#39; # orange, RGB = 220,128,24 tn_hex = &#39;#C89F91&#39; # tan, RGB = 200,159,145 g50_hex = &#39;#6c6d6c&#39; # grey-50, RGB = 108,109,108 bg_hex = &#39;#4f6268&#39; # blue grey, RGB = 79,98,104 g25_hex = &#39;#c7cccf&#39; # grey-25, RGB = 199,204,207 . import numpy as np import pandas as pd import matplotlib,re import matplotlib.pyplot as plt from matplotlib.pyplot import MultipleLocator data = pd.read_csv(r&#39;data/AreaInfo.csv&#39;) . 查看与统计数据，以对数据有一个大致了解 . data.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 0 北美洲 | North America | 美国 | United States of America | 美国 | United States of America | 971002 | 2306247 | 0.0 | 640198 | 120351 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 南美洲 | South America | 巴西 | Brazil | 巴西 | Brazil | 973003 | 1106470 | 0.0 | 549386 | 51271 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 欧洲 | Europe | 英国 | United Kingdom | 英国 | United Kingdom | 961007 | 305289 | 0.0 | 539 | 42647 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 欧洲 | Europe | 俄罗斯 | Russia | 俄罗斯 | Russia | 964006 | 592280 | 0.0 | 344416 | 8206 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 南美洲 | South America | 智利 | Chile | 智利 | Chile | 973004 | 246963 | 0.0 | 44946 | 4502 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . &#19977;. &#25968;&#25454;&#28165;&#27927; . &#65288;&#19968;&#65289;&#22522;&#26412;&#25968;&#25454;&#22788;&#29702; . 数据清洗主要包括：选取子集，缺失数据处理、数据格式转换、异常值数据处理等。 . &#22269;&#20869;&#30123;&#24773;&#25968;&#25454;&#36873;&#21462;&#65288;&#26368;&#32456;&#36873;&#21462;&#30340;&#25968;&#25454;&#21629;&#21517;&#20026;china&#65289; . 选取国内疫情数据 . | 对于更新时间(updateTime)列，需将其转换为日期类型并提取出年-月-日，并查看处理结果。(提示：dt.date) . | 因数据每天按小时更新，一天之内有很多重复数据，请去重并只保留一天之内最新的数据。 . | 提示：df.drop_duplicates(subset=[&#39;provinceName&#39;, &#39;updateTime&#39;], keep=&#39;first&#39;, inplace=False) . 其中df是你选择的国内疫情数据的DataFrame . 分析：选取countryName一列中值为中国的行组成CHINA。 . CHINA = data.loc[data[&#39;countryName&#39;] == &#39;中国&#39;] CHINA.dropna(subset=[&#39;cityName&#39;], how=&#39;any&#39;, inplace=True) #CHINA . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . 分析：取出含所有中国城市的列表 . cities = list(set(CHINA[&#39;cityName&#39;])) . 分析：遍历取出每一个城市的子dataframe，然后用sort对updateTime进行时间排序 . for city in cities: CHINA.loc[data[&#39;cityName&#39;] == city].sort_values(by = &#39;updateTime&#39;) . 分析：去除空值所在行 . CHINA.dropna(subset=[&#39;cityName&#39;],inplace=True) #CHINA.loc[CHINA[&#39;cityName&#39;] == &#39;秦皇岛&#39;].tail(20) . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . 分析：将CHINA中的updateTime列进行格式化处理 . CHINA.updateTime = pd.to_datetime(CHINA.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date #CHINA.loc[data[&#39;cityName&#39;] == &#39;秦皇岛&#39;].tail(15) . D: Anaconda envs python32 lib site-packages pandas core generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . CHINA.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 136 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 境外输入 | NaN | 0.0 | 72.0 | 0.0 | 65.0 | 0.0 | . 137 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 西安 | Xi&#39;an | 610100.0 | 120.0 | 0.0 | 117.0 | 3.0 | . 138 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 安康 | Ankang | 610900.0 | 26.0 | 0.0 | 26.0 | 0.0 | . 139 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 汉中 | Hanzhong | 610700.0 | 26.0 | 0.0 | 26.0 | 0.0 | . 140 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 咸阳 | Xianyang | 610400.0 | 17.0 | 0.0 | 17.0 | 0.0 | . 分析：每日数据的去重只保留第一个数据，因为前面已经对时间进行排序，第一个数据即为当天最新数据 分析：考虑到合并dataframe需要用到concat，需要创建一个初始china . real = CHINA.loc[data[&#39;cityName&#39;] == cities[1]] real.drop_duplicates(subset=&#39;updateTime&#39;, keep=&#39;first&#39;, inplace=True) china = real . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . 分析：遍历每个城市dataframe进行每日数据的去重，否则会出现相同日期只保留一个城市的数据的情况 . for city in cities[2:]: real_data = CHINA.loc[data[&#39;cityName&#39;] == city] real_data.drop_duplicates(subset=&#39;updateTime&#39;, keep=&#39;first&#39;, inplace=True) china = pd.concat([real_data, china],sort=False) . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This is separate from the ipykernel package so we can avoid doing imports until . 查看数据信息，是否有缺失数据/数据类型是否正确。 . 提示：若不会处理缺失值，可以将其舍弃 . 分析：有的城市不是每日都上报的，如果某日只统计上报的那些城市，那些存在患者却不上报的城市就会被忽略，数据就失真了，需要补全所有城市每日的数据，即便不上报的城市也要每日记录数据统计，所以要进行插值处理补全部分数据，处理方法详见数据透视与分析 . china.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 32812 entries, 96106 to 208267 Data columns (total 19 columns): # Column Non-Null Count Dtype -- -- 0 continentName 32812 non-null object 1 continentEnglishName 32812 non-null object 2 countryName 32812 non-null object 3 countryEnglishName 32812 non-null object 4 provinceName 32812 non-null object 5 provinceEnglishName 32812 non-null object 6 province_zipCode 32812 non-null int64 7 province_confirmedCount 32812 non-null int64 8 province_suspectedCount 32812 non-null float64 9 province_curedCount 32812 non-null int64 10 province_deadCount 32812 non-null int64 11 updateTime 32812 non-null object 12 cityName 32812 non-null object 13 cityEnglishName 31968 non-null object 14 city_zipCode 32502 non-null float64 15 city_confirmedCount 32812 non-null float64 16 city_suspectedCount 32812 non-null float64 17 city_curedCount 32812 non-null float64 18 city_deadCount 32812 non-null float64 dtypes: float64(6), int64(4), object(9) memory usage: 5.0+ MB . china.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 96106 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 254 | 0.0 | 252 | 2 | 2020-04-02 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 125120 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 254 | 0.0 | 250 | 2 | 2020-03-20 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 128762 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 253 | 0.0 | 250 | 2 | 2020-03-18 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 130607 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 253 | 0.0 | 248 | 2 | 2020-03-17 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 131428 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 252 | 0.0 | 248 | 2 | 2020-03-16 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . &#20320;&#25152;&#22312;&#30465;&#24066;&#30123;&#24773;&#25968;&#25454;&#36873;&#21462;&#65288;&#26368;&#32456;&#36873;&#21462;&#30340;&#25968;&#25454;&#21629;&#21517;&#20026;myhome&#65289; . 此步也可在后面用到的再做 . myhome = china.loc[data[&#39;provinceName&#39;] == &#39;广东省&#39;] myhome.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 205259 亚洲 | Asia | 中国 | China | 广东省 | Guangdong | 440000 | 277 | 0.0 | 5 | 0 | 2020-01-29 | 外地来粤人员 | NaN | NaN | 5.0 | 0.0 | 0.0 | 0.0 | . 206335 亚洲 | Asia | 中国 | China | 广东省 | Guangdong | 440000 | 207 | 0.0 | 4 | 0 | 2020-01-28 | 河源市 | NaN | NaN | 1.0 | 0.0 | 0.0 | 0.0 | . 205239 亚洲 | Asia | 中国 | China | 广东省 | Guangdong | 440000 | 277 | 0.0 | 5 | 0 | 2020-01-29 | 外地来穗人员 | NaN | NaN | 5.0 | 0.0 | 0.0 | 0.0 | . 252 亚洲 | Asia | 中国 | China | 广东省 | Guangdong | 440000 | 1634 | 11.0 | 1619 | 8 | 2020-06-23 | 潮州 | Chaozhou | 445100.0 | 6.0 | 0.0 | 6.0 | 0.0 | . 2655 亚洲 | Asia | 中国 | China | 广东省 | Guangdong | 440000 | 1634 | 11.0 | 1614 | 8 | 2020-06-21 | 潮州 | Chaozhou | 445100.0 | 6.0 | 0.0 | 6.0 | 0.0 | . &#22269;&#22806;&#30123;&#24773;&#25968;&#25454;&#36873;&#21462;&#65288;&#26368;&#32456;&#36873;&#21462;&#30340;&#25968;&#25454;&#21629;&#21517;&#20026;world&#65289; . 此步也可在后面用到的再做 . world = data.loc[data[&#39;countryName&#39;] != &#39;中国&#39;] world.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 0 北美洲 | North America | 美国 | United States of America | 美国 | United States of America | 971002 | 2306247 | 0.0 | 640198 | 120351 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 南美洲 | South America | 巴西 | Brazil | 巴西 | Brazil | 973003 | 1106470 | 0.0 | 549386 | 51271 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 欧洲 | Europe | 英国 | United Kingdom | 英国 | United Kingdom | 961007 | 305289 | 0.0 | 539 | 42647 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 欧洲 | Europe | 俄罗斯 | Russia | 俄罗斯 | Russia | 964006 | 592280 | 0.0 | 344416 | 8206 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 南美洲 | South America | 智利 | Chile | 智利 | Chile | 973004 | 246963 | 0.0 | 44946 | 4502 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 数据透视与分析 . 分析：对china进行插值处理补全部分数据 . china.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 96106 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 254 | 0.0 | 252 | 2 | 2020-04-02 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 125120 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 254 | 0.0 | 250 | 2 | 2020-03-20 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 128762 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 253 | 0.0 | 250 | 2 | 2020-03-18 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 130607 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 253 | 0.0 | 248 | 2 | 2020-03-17 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 131428 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 252 | 0.0 | 248 | 2 | 2020-03-16 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 分析：先创建省份列表和日期列表，并初始化一个draft . province = list(set(china[&#39;provinceName&#39;]))#每个省份 #p_city = list(set(china[china[&#39;provinceName&#39;] == province[0]][&#39;cityName&#39;]))#每个省份的城市 date_0 = [] for dt in china.loc[china[&#39;provinceName&#39;] == province[0]][&#39;updateTime&#39;]: date_0.append(str(dt)) date_0 = list(set(date_0)) date_0.sort() start = china.loc[china[&#39;provinceName&#39;] == province[0]][&#39;updateTime&#39;].min() end = china.loc[china[&#39;provinceName&#39;] == province[0]][&#39;updateTime&#39;].max() dates = pd.date_range(start=str(start), end=str(end)) aid_frame = pd.DataFrame({&#39;updateTime&#39;: dates,&#39;provinceName&#39;:[province[0]]*len(dates)}) aid_frame.updateTime = pd.to_datetime(aid_frame.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date #draft = pd.merge(china.loc[china[&#39;provinceName&#39;] == province[1]], aid_frame, on=&#39;updateTime&#39;, how=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) draft = pd.concat([china.loc[china[&#39;provinceName&#39;] == province[0]], aid_frame], join=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) draft.province_confirmedCount.fillna(method=&quot;ffill&quot;,inplace=True) draft.province_suspectedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft.province_curedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft.province_deadCount.fillna(method=&quot;ffill&quot;, inplace=True) . 分析：补全部分时间，取前日的数据进行插值，因为有的省份从4月末开始陆续就不再有新增病患，不再上报，所以这些省份的数据只能补全到4月末，往后的数据逐渐失去真实性 . 分析：同时进行日期格式化 . for p in range(1,len(province)): date_d = [] for dt in china.loc[china[&#39;provinceName&#39;] == province[p]][&#39;updateTime&#39;]: date_d.append(dt) date_d = list(set(date_d)) date_d.sort() start = china.loc[china[&#39;provinceName&#39;] == province[p]][&#39;updateTime&#39;].min() end = china.loc[china[&#39;provinceName&#39;] == province[p]][&#39;updateTime&#39;].max() dates = pd.date_range(start=start, end=end) aid_frame = pd.DataFrame({&#39;updateTime&#39;: dates,&#39;provinceName&#39;:[province[p]]*len(dates)}) aid_frame.updateTime = pd.to_datetime(aid_frame.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date X = china.loc[china[&#39;provinceName&#39;] == province[p]] X.reset_index(drop= True) Y = aid_frame Y.reset_index(drop= True) draft_d = pd.concat([X,Y], join=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) draft = pd.concat([draft,draft_d]) draft.province_confirmedCount.fillna(method=&quot;ffill&quot;,inplace=True) draft.province_suspectedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft.province_curedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft.province_deadCount.fillna(method=&quot;ffill&quot;, inplace=True) #draft[&#39;updateTime&#39;] = draft[&#39;updateTime&#39;].strftime(&#39;%Y-%m-%d&#39;) #draft[&#39;updateTime&#39;] = pd.to_datetime(draft[&#39;updateTime&#39;],format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date . china = draft . china.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 208226 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 14.0 | 0.0 | 0.0 | 0.0 | 2020-01-26 | 外地来津 | NaN | NaN | 2.0 | 0.0 | 0.0 | 0.0 | . 208224 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 14.0 | 0.0 | 0.0 | 0.0 | 2020-01-26 | 河北区 | Hebei District | 120105.0 | 5.0 | 0.0 | 0.0 | 0.0 | . 208228 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 14.0 | 0.0 | 0.0 | 0.0 | 2020-01-26 | 和平区 | Heping District | 120101.0 | 1.0 | 0.0 | 0.0 | 0.0 | . 208227 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 14.0 | 0.0 | 0.0 | 0.0 | 2020-01-26 | 滨海新区 | Binhai New Area | 120116.0 | 1.0 | 0.0 | 0.0 | 0.0 | . 208230 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 14.0 | 0.0 | 0.0 | 0.0 | 2020-01-26 | 西青区 | Xiqing District | 120111.0 | 1.0 | 0.0 | 0.0 | 0.0 | . &#22235;. &#25968;&#25454;&#20998;&#26512;&#21450;&#21487;&#35270;&#21270; . 在进行数据分析及可视化时，依据每个问题选取所需变量并新建DataFrame再进行分析和可视化展示，这样数据不易乱且条理更清晰。 . &#22522;&#30784;&#20998;&#26512; . 基础分析，只允许使用numpy、pandas和matplotlib库。 . 可以在一张图上多个坐标系展示也可以在多张图上展示 . 请根据分析目的选择图形的类型(折线图、饼图、直方图和散点图等等)，实在没有主意可以到百度疫情地图或其他疫情分析的站点激发激发灵感。 . &#65288;&#19968;&#65289;&#20840;&#22269;&#32047;&#35745;&#30830;&#35786;/&#30097;&#20284;/&#27835;&#24840;/&#27515;&#20129;&#24773;&#20917;&#38543;&#26102;&#38388;&#21464;&#21270;&#36235;&#21183;&#22914;&#20309;&#65311; . 分析：要获得全国累计情况随时间变化趋势，首先需要整合每日全国累计确诊情况做成date_confirmed . 分析：要整合每日全国累计确诊情况，首先得提取每个省份每日当天最新累计确诊人数，省份数据求和后形成dataframe， for循环拼接到date_confirmed中 . date = list(set(china[&#39;updateTime&#39;])) date.sort() date . [datetime.date(2020, 1, 24), datetime.date(2020, 1, 25), datetime.date(2020, 1, 26), datetime.date(2020, 1, 27), datetime.date(2020, 1, 28), datetime.date(2020, 1, 29), datetime.date(2020, 1, 30), datetime.date(2020, 1, 31), datetime.date(2020, 2, 1), datetime.date(2020, 2, 2), datetime.date(2020, 2, 3), datetime.date(2020, 2, 4), datetime.date(2020, 2, 5), datetime.date(2020, 2, 6), datetime.date(2020, 2, 7), datetime.date(2020, 2, 8), datetime.date(2020, 2, 9), datetime.date(2020, 2, 10), datetime.date(2020, 2, 11), datetime.date(2020, 2, 12), datetime.date(2020, 2, 13), datetime.date(2020, 2, 14), datetime.date(2020, 2, 15), datetime.date(2020, 2, 16), datetime.date(2020, 2, 17), datetime.date(2020, 2, 18), datetime.date(2020, 2, 19), datetime.date(2020, 2, 20), datetime.date(2020, 2, 21), datetime.date(2020, 2, 22), datetime.date(2020, 2, 23), datetime.date(2020, 2, 24), datetime.date(2020, 2, 25), datetime.date(2020, 2, 26), datetime.date(2020, 2, 27), datetime.date(2020, 2, 28), datetime.date(2020, 2, 29), datetime.date(2020, 3, 1), datetime.date(2020, 3, 2), datetime.date(2020, 3, 3), datetime.date(2020, 3, 4), datetime.date(2020, 3, 5), datetime.date(2020, 3, 6), datetime.date(2020, 3, 7), datetime.date(2020, 3, 8), datetime.date(2020, 3, 9), datetime.date(2020, 3, 10), datetime.date(2020, 3, 11), datetime.date(2020, 3, 12), datetime.date(2020, 3, 13), datetime.date(2020, 3, 14), datetime.date(2020, 3, 15), datetime.date(2020, 3, 16), datetime.date(2020, 3, 17), datetime.date(2020, 3, 18), datetime.date(2020, 3, 19), datetime.date(2020, 3, 20), datetime.date(2020, 3, 21), datetime.date(2020, 3, 22), datetime.date(2020, 3, 23), datetime.date(2020, 3, 24), datetime.date(2020, 3, 25), datetime.date(2020, 3, 26), datetime.date(2020, 3, 27), datetime.date(2020, 3, 28), datetime.date(2020, 3, 29), datetime.date(2020, 3, 30), datetime.date(2020, 3, 31), datetime.date(2020, 4, 1), datetime.date(2020, 4, 2), datetime.date(2020, 4, 3), datetime.date(2020, 4, 4), datetime.date(2020, 4, 5), datetime.date(2020, 4, 6), datetime.date(2020, 4, 7), datetime.date(2020, 4, 8), datetime.date(2020, 4, 9), datetime.date(2020, 4, 10), datetime.date(2020, 4, 11), datetime.date(2020, 4, 12), datetime.date(2020, 4, 13), datetime.date(2020, 4, 14), datetime.date(2020, 4, 15), datetime.date(2020, 4, 16), datetime.date(2020, 4, 17), datetime.date(2020, 4, 18), datetime.date(2020, 4, 19), datetime.date(2020, 4, 20), datetime.date(2020, 4, 21), datetime.date(2020, 4, 22), datetime.date(2020, 4, 23), datetime.date(2020, 4, 24), datetime.date(2020, 4, 25), datetime.date(2020, 4, 26), datetime.date(2020, 4, 27), datetime.date(2020, 4, 28), datetime.date(2020, 4, 29), datetime.date(2020, 4, 30), datetime.date(2020, 5, 1), datetime.date(2020, 5, 2), datetime.date(2020, 5, 3), datetime.date(2020, 5, 4), datetime.date(2020, 5, 5), datetime.date(2020, 5, 6), datetime.date(2020, 5, 7), datetime.date(2020, 5, 8), datetime.date(2020, 5, 9), datetime.date(2020, 5, 10), datetime.date(2020, 5, 11), datetime.date(2020, 5, 12), datetime.date(2020, 5, 13), datetime.date(2020, 5, 14), datetime.date(2020, 5, 15), datetime.date(2020, 5, 16), datetime.date(2020, 5, 17), datetime.date(2020, 5, 18), datetime.date(2020, 5, 19), datetime.date(2020, 5, 20), datetime.date(2020, 5, 21), datetime.date(2020, 5, 22), datetime.date(2020, 5, 23), datetime.date(2020, 5, 24), datetime.date(2020, 5, 25), datetime.date(2020, 5, 26), datetime.date(2020, 5, 27), datetime.date(2020, 5, 28), datetime.date(2020, 5, 29), datetime.date(2020, 5, 30), datetime.date(2020, 5, 31), datetime.date(2020, 6, 1), datetime.date(2020, 6, 2), datetime.date(2020, 6, 3), datetime.date(2020, 6, 4), datetime.date(2020, 6, 5), datetime.date(2020, 6, 6), datetime.date(2020, 6, 7), datetime.date(2020, 6, 8), datetime.date(2020, 6, 9), datetime.date(2020, 6, 10), datetime.date(2020, 6, 11), datetime.date(2020, 6, 12), datetime.date(2020, 6, 13), datetime.date(2020, 6, 14), datetime.date(2020, 6, 15), datetime.date(2020, 6, 16), datetime.date(2020, 6, 17), datetime.date(2020, 6, 18), datetime.date(2020, 6, 19), datetime.date(2020, 6, 20), datetime.date(2020, 6, 21), datetime.date(2020, 6, 22), datetime.date(2020, 6, 23)] . china = china.set_index(&#39;provinceName&#39;) china = china.reset_index() . 分析：循环遍历省份和日期获得每个省份每日累计确诊，因为需要拼接，先初始化一个date_confirmed . list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_confirmedCount&#39;])#该日每省的累计确诊人数 except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_confirmed = pd.DataFrame(list_d,index=list_e) date_confirmed.index.name=&quot;date&quot; date_confirmed.columns=[&quot;China_confirmedCount&quot;] date_confirmed . China_confirmedCount . date . 2020-01-24 1956.0 | . 分析：遍历每个省份拼接每日的总确诊人数的dataframe . l = 0 for i in date[3:]: list_p = [] list_d = [] list_e = [] l +=1 for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[l]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_confirmedCount&#39;])#该日每省的累计确诊人数 except: continue #con_0 = china.loc[china[&#39;updateTime&#39;] == date[0]].loc[china[&#39;provinceName&#39;] == &#39;河北省&#39;].loc[[0]].iloc[0] #list_p.append(con_0[&#39;province_confirmedCount&#39;])#该日每省的累计确诊人数 list_d.append(sum(list_p)) list_e.append(str(date[l])) confirmed = pd.DataFrame(list_d, index=list_e) confirmed.index.name=&quot;date&quot; confirmed.columns=[&quot;China_confirmedCount&quot;] date_confirmed = pd.concat([date_confirmed,confirmed],sort=False) date_confirmed . China_confirmedCount . date . 2020-01-24 1956.0 | . 2020-01-25 2253.0 | . 2020-01-26 1956.0 | . 2020-01-27 2825.0 | . 2020-01-28 4589.0 | . ... ... | . 2020-06-17 8106.0 | . 2020-06-18 6862.0 | . 2020-06-19 6894.0 | . 2020-06-20 6921.0 | . 2020-06-21 6157.0 | . 150 rows × 1 columns . 分析：去除空值和不全的值 . date_confirmed.dropna(subset=[&#39;China_confirmedCount&#39;],inplace=True) date_confirmed.tail(20) . China_confirmedCount . date . 2020-06-02 78782.0 | . 2020-06-03 78780.0 | . 2020-06-04 76903.0 | . 2020-06-05 76908.0 | . 2020-06-06 8777.0 | . 2020-06-07 8782.0 | . 2020-06-08 8628.0 | . 2020-06-09 8634.0 | . 2020-06-10 8638.0 | . 2020-06-11 8649.0 | . 2020-06-12 8658.0 | . 2020-06-13 8665.0 | . 2020-06-14 8733.0 | . 2020-06-15 8772.0 | . 2020-06-16 8055.0 | . 2020-06-17 8106.0 | . 2020-06-18 6862.0 | . 2020-06-19 6894.0 | . 2020-06-20 6921.0 | . 2020-06-21 6157.0 | . 分析：数据从4月末开始到5月末就因为缺失过多省份的数据(部分省份从4月末至今再也没有新增病患)而失真，自2020-06-06起完全失去真实性，所以我删除了2020-06-06往后的数据 . date_confirmed = date_confirmed.drop([&#39;2020-06-06&#39;,&#39;2020-06-07&#39;,&#39;2020-06-08&#39;,&#39;2020-06-09&#39;,&#39;2020-06-10&#39;,&#39;2020-06-11&#39;,&#39;2020-06-12&#39;,&#39;2020-06-13&#39;,&#39;2020-06-14&#39;, &#39;2020-06-15&#39;,&#39;2020-06-16&#39;,&#39;2020-06-19&#39;,&#39;2020-06-18&#39;,&#39;2020-06-20&#39;,&#39;2020-06-17&#39;,&#39;2020-06-21&#39;]) . 分析：构造拼接函数 . def data_frame(self,china,element): l = 0 for i in date[3:]: list_p = [] list_d = [] list_e = [] l +=1 for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[l]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[element]) except: continue #con_0 = china.loc[china[&#39;updateTime&#39;] == date[0]].loc[china[&#39;provinceName&#39;] == &#39;河北省&#39;].loc[[0]].iloc[0] #list_p.append(con_0[&#39;province_confirmedCount&#39;]) list_d.append(sum(list_p)) list_e.append(str(date[l])) link = pd.DataFrame(list_d, index=list_e) link.index.name=&quot;date&quot; link.columns=[&quot;China&quot;] self = pd.concat([self,link],sort=False) self.dropna(subset=[&#39;China&#39;],inplace=True) self = self.drop([&#39;2020-06-06&#39;,&#39;2020-06-07&#39;,&#39;2020-06-08&#39;,&#39;2020-06-09&#39;,&#39;2020-06-10&#39;,&#39;2020-06-11&#39;,&#39;2020-06-12&#39;,&#39;2020-06-13&#39;,&#39;2020-06-14&#39;, &#39;2020-06-15&#39;,&#39;2020-06-16&#39;,&#39;2020-06-19&#39;,&#39;2020-06-18&#39;,&#39;2020-06-20&#39;,&#39;2020-06-17&#39;,&#39;2020-06-21&#39;]) return self . 分析：初始化各个变量 . list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_curedCount&#39;]) except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_cured = pd.DataFrame(list_d, index=list_e) date_cured.index.name=&quot;date&quot; date_cured.columns=[&quot;China&quot;] #累计死亡人数 date_dead list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_deadCount&#39;]) except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_dead = pd.DataFrame(list_d, index=list_e) date_dead.index.name=&quot;date&quot; date_dead.columns=[&quot;China&quot;] . plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] #更改字体,否则无法显示汉字 fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = date_confirmed.index y = date_confirmed.values ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) ax.set_title(&#39;累计确诊患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.set_xticks( range(0,len(x),30)) . [&lt;matplotlib.axis.XTick at 0x255520e4908&gt;, &lt;matplotlib.axis.XTick at 0x255520e49e8&gt;, &lt;matplotlib.axis.XTick at 0x255520af048&gt;, &lt;matplotlib.axis.XTick at 0x2555216b0b8&gt;, &lt;matplotlib.axis.XTick at 0x2555216b4e0&gt;] . date_cured = data_frame(date_cured,china,&#39;province_curedCount&#39;) fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = date_cured.index y = date_cured.values ax.set_title(&#39;累计治愈患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) ax.set_xticks( range(0,len(x),30)) . [&lt;matplotlib.axis.XTick at 0x25550ef60f0&gt;, &lt;matplotlib.axis.XTick at 0x255521cd0b8&gt;, &lt;matplotlib.axis.XTick at 0x255521b7780&gt;, &lt;matplotlib.axis.XTick at 0x2555208ffd0&gt;, &lt;matplotlib.axis.XTick at 0x2555208f0f0&gt;] . 分析：累计疑似无法通过补全数据得到 . date_dead = data_frame(date_dead,china,&#39;province_deadCount&#39;) fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = date_dead.index y = date_dead.values ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) x_major_locator=MultipleLocator(12) ax=plt.gca() ax.set_title(&#39;累计死亡患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.xaxis.set_major_locator(x_major_locator) ax.set_xticks( range(0,len(x),30)) . [&lt;matplotlib.axis.XTick at 0x255521fda90&gt;, &lt;matplotlib.axis.XTick at 0x255521fda58&gt;, &lt;matplotlib.axis.XTick at 0x25552a51550&gt;, &lt;matplotlib.axis.XTick at 0x25552a75470&gt;, &lt;matplotlib.axis.XTick at 0x25552a75908&gt;] . 分析：疫情自1月初开始爆发，到2月末开始减缓增速，到4月末趋于平缓。治愈人数自2月初开始大幅增加，到3月末趋于平缓，死亡人数自1月末开始增加，到2月末趋于平缓，到4月末因为统计因素死亡人数飙升后趋于平缓。 分析总结：确诊人数数据和治愈数据从4月末开始到5月末就因为缺失过多省份的数据(部分省份至今再也没有新增病患)导致失真，其他数据尽量通过补全,越靠近尾部数据越失真。死亡数据补全较为成功，几乎没有错漏。 . &#65288;&#20108;&#65289;&#20840;&#22269;&#26032;&#22686;&#30830;&#35786;/&#30097;&#20284;/&#27835;&#24840;/&#27515;&#20129;&#24773;&#20917;&#38543;&#26102;&#38388;&#21464;&#21270;&#36235;&#21183;&#22914;&#20309;&#65311; . 分析：新增确诊/治愈/死亡的数据需要对china进行运算，每省每日进行diff差值运算 . 分析：首先初始化各个数据，然后仿照上面的拼接函数，作适用于该题的拼接函数 . list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_confirmedCount&#39;])#该日每省的累计确诊人数 except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_new_confirmed = pd.DataFrame(list_d,index=list_e) date_new_confirmed.index.name=&quot;date&quot; date_new_confirmed.columns=[&quot;China&quot;] date_new_confirmed #新增治愈人数 date_new_curedCount list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_curedCount&#39;]) except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_new_cured = pd.DataFrame(list_d, index=list_e) date_new_cured.index.name=&quot;date&quot; date_new_cured.columns=[&quot;China&quot;] #新增死亡人数 date_new_dead list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_deadCount&#39;]) except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_new_dead = pd.DataFrame(list_d, index=list_e) date_new_dead.index.name=&quot;date&quot; date_new_dead.columns=[&quot;China&quot;] . 分析：构造拼接函数 . def data_new_frame(self,china,element): l = 0 for i in date[3:]: list_p = [] list_d = [] list_e = [] l +=1 for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[l]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[element]) except: continue #con_0 = china.loc[china[&#39;updateTime&#39;] == date[0]].loc[china[&#39;provinceName&#39;] == &#39;河北省&#39;].loc[[0]].iloc[0] #list_p.append(con_0[&#39;province_confirmedCount&#39;]) list_d.append(sum(list_p)) list_e.append(str(date[l])) link = pd.DataFrame(list_d, index=list_e) link.index.name=&quot;date&quot; link.columns=[&quot;China&quot;] self = pd.concat([self,link],sort=False) self.dropna(subset=[&#39;China&#39;],inplace=True) return self . 分析：数据补全以及去除含缺失省份的数据 . d = data_new_frame(date_new_confirmed,china,&#39;province_confirmedCount&#39;) for i in range(len(d)): dr = [] for a,b in zip(range(0,len(d)-1),range(1,len(d)-2)): if d.iloc[b].iloc[0] &lt; d.iloc[a].iloc[0]: dr.append(d.iloc[b].iloc[0]) d = d[~d[&#39;China&#39;].isin(dr)] . 分析：做差值运算 . d[&#39;China&#39;] = d[&#39;China&#39;].diff() . 分析：去除两个含缺失省份的日期 . d.drop([&#39;2020-06-20&#39;,&#39;2020-06-21&#39;],inplace=True) . 分析：作折线图表现时间趋势 . fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = d.index y = d.values ax.set_title(&#39;新增确诊患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) ax.set_xticks( range(0,len(x),10)) . [&lt;matplotlib.axis.XTick at 0x25552a9c898&gt;, &lt;matplotlib.axis.XTick at 0x25552a9c860&gt;, &lt;matplotlib.axis.XTick at 0x25552ab7550&gt;, &lt;matplotlib.axis.XTick at 0x25552ad50f0&gt;, &lt;matplotlib.axis.XTick at 0x25552ad5518&gt;, &lt;matplotlib.axis.XTick at 0x25552ad59b0&gt;, &lt;matplotlib.axis.XTick at 0x25552ad5e48&gt;, &lt;matplotlib.axis.XTick at 0x25552adc320&gt;] . 分析：使用初始化数据构造date_new_cured的dataframe，然后作折线图表现时间趋势 . cu = data_new_frame(date_new_cured,china,&#39;province_curedCount&#39;) for i in range(len(cu)): dr = [] for a,b in zip(range(0,len(cu)-1),range(1,len(cu)-2)): if cu.iloc[b].iloc[0] &lt; cu.iloc[a].iloc[0]: dr.append(cu.iloc[b].iloc[0]) cu = cu[~cu[&#39;China&#39;].isin(dr)] cu[&#39;China&#39;] = cu[&#39;China&#39;].diff() cu.drop([&#39;2020-06-20&#39;,&#39;2020-06-21&#39;],inplace=True) #新增治愈患者 date_new_cured fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = cu.index y = cu.values ax.set_title(&#39;新增治愈患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) ax.set_xticks( range(0,len(x),10)) . [&lt;matplotlib.axis.XTick at 0x25552b13b00&gt;, &lt;matplotlib.axis.XTick at 0x25552b13ac8&gt;, &lt;matplotlib.axis.XTick at 0x25552b137b8&gt;, &lt;matplotlib.axis.XTick at 0x25552b3f470&gt;, &lt;matplotlib.axis.XTick at 0x25552b3f908&gt;, &lt;matplotlib.axis.XTick at 0x25552b3fda0&gt;, &lt;matplotlib.axis.XTick at 0x25552b47278&gt;] . 分析：使用初始化数据构造date_new_dead的dataframe，然后作折线图表现时间趋势 . de = data_new_frame( date_new_dead,china,&#39;province_deadCount&#39;) for i in range(len(de)): dr = [] for a,b in zip(range(0,len(de)-1),range(1,len(de)-2)): if de.iloc[b].iloc[0] &lt; de.iloc[a].iloc[0]: dr.append(de.iloc[b].iloc[0]) de = de[~de[&#39;China&#39;].isin(dr)] de[&#39;China&#39;] = de[&#39;China&#39;].diff() de.drop([&#39;2020-06-21&#39;],inplace=True) #新增死亡患者 date_new_dead fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = de.index y = de.values ax.set_title(&#39;新增死亡患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) ax.set_xticks( range(0,len(x),10)) . [&lt;matplotlib.axis.XTick at 0x25553bdfd30&gt;, &lt;matplotlib.axis.XTick at 0x25553bdfcf8&gt;, &lt;matplotlib.axis.XTick at 0x25553c01f60&gt;, &lt;matplotlib.axis.XTick at 0x25553c146a0&gt;, &lt;matplotlib.axis.XTick at 0x25553c14b38&gt;, &lt;matplotlib.axis.XTick at 0x25553c14d68&gt;, &lt;matplotlib.axis.XTick at 0x25553c1b4a8&gt;, &lt;matplotlib.axis.XTick at 0x25553c1b940&gt;, &lt;matplotlib.axis.XTick at 0x25553c1bdd8&gt;] . 分析：新增患者自1月末开始增加，到2月14日前后到达顶点，后增数下降，趋于平缓。 分析：新增治愈患者自1月末开始增加，到3月02日前后达到顶峰，后增数下降，从4月初开始趋于平缓。 分析：新增死亡患者自1月末开始增加，到2月达到高峰，自3月初开始增数平缓，到4月17日前后因为统计因素飙升后回落。 . &#65288;&#19977;&#65289;&#20840;&#22269;&#26032;&#22686;&#22659;&#22806;&#36755;&#20837;&#38543;&#26102;&#38388;&#21464;&#21270;&#36235;&#21183;&#22914;&#20309;&#65311; . 分析：新增境外输入数据需要对CHINA进行运算，逐日相减。 . 分析：先从CHINA取出境外输入的数据，然后补全时间序列并作差。 . imported = CHINA.loc[CHINA[&#39;cityName&#39;] == &#39;境外输入&#39;] imported.updateTime = pd.to_datetime(imported.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date imported . D: Anaconda envs python32 lib site-packages pandas core generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 136 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 境外输入 | NaN | 0.0 | 72.0 | 0.0 | 65.0 | 0.0 | . 150 亚洲 | Asia | 中国 | China | 江苏省 | Jiangsu | 320000 | 654 | 3.0 | 653 | 0 | 2020-06-23 | 境外输入 | NaN | 0.0 | 23.0 | 0.0 | 22.0 | 0.0 | . 201 亚洲 | Asia | 中国 | China | 北京市 | Beijing | 110000 | 843 | 164.0 | 584 | 9 | 2020-06-23 | 境外输入 | NaN | 0.0 | 174.0 | 3.0 | 173.0 | 0.0 | . 214 亚洲 | Asia | 中国 | China | 河北省 | Hebei | 130000 | 346 | 0.0 | 323 | 6 | 2020-06-23 | 境外输入 | NaN | 0.0 | 10.0 | 0.0 | 10.0 | 0.0 | . 218 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000 | 198 | 48.0 | 192 | 3 | 2020-06-23 | 境外输入 | NaN | 0.0 | 61.0 | 0.0 | 59.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 115420 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 250 | 1.0 | 240 | 3 | 2020-03-25 | 境外输入 | NaN | 0.0 | 5.0 | 1.0 | 0.0 | 0.0 | . 115956 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000 | 145 | 0.0 | 133 | 3 | 2020-03-24 | 境外输入 | NaN | 0.0 | 9.0 | 0.0 | 0.0 | 0.0 | . 116164 亚洲 | Asia | 中国 | China | 甘肃省 | Gansu | 620000 | 136 | 0.0 | 119 | 2 | 2020-03-24 | 境外输入 | NaN | 0.0 | 45.0 | 0.0 | 30.0 | 0.0 | . 117171 亚洲 | Asia | 中国 | China | 上海市 | Shanghai | 310000 | 414 | 0.0 | 330 | 4 | 2020-03-24 | 境外输入 | NaN | 0.0 | 75.0 | 0.0 | 3.0 | 0.0 | . 117597 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000 | 142 | 0.0 | 133 | 3 | 2020-03-24 | 境外输入 | NaN | 0.0 | 6.0 | 0.0 | 0.0 | 0.0 | . 607 rows × 19 columns . 分析：补全省份缺失时间的数据 . for i in range(0,len(province)): list_j_d = [] date_b = [] for dt in imported.loc[imported[&#39;provinceName&#39;] == province[i]][&#39;updateTime&#39;]: date_b.append(str(dt)) list_j_d = list(set(date_b)) list_j_d.sort() #imported.loc[imported[&#39;provinceName&#39;] == province[3]] try: start = imported.loc[imported[&#39;provinceName&#39;] == province[i]][&#39;updateTime&#39;].min() end = imported.loc[imported[&#39;provinceName&#39;] == province[i]][&#39;updateTime&#39;].max() dates_b = pd.date_range(start=str(start), end=str(end)) aid_frame_b = pd.DataFrame({&#39;updateTime&#39;: dates_b,&#39;provinceName&#39;:[province[i]]*len(dates_b)}) aid_frame_b.updateTime = pd.to_datetime(aid_frame_b.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date #draft = pd.merge(china.loc[china[&#39;provinceName&#39;] == province[1]], aid_frame, on=&#39;updateTime&#39;, how=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) draft_b = pd.concat([imported.loc[imported[&#39;provinceName&#39;] == province[i]], aid_frame_b], join=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) draft_b.city_confirmedCount.fillna(method=&quot;ffill&quot;,inplace=True) draft_b.city_suspectedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft_b.city_curedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft_b.city_deadCount.fillna(method=&quot;ffill&quot;, inplace=True) draft_b.loc[draft_b[&#39;provinceName&#39;] == province[i]].fillna(0,inplace=True,limit = 1) draft_b.loc[draft_b[&#39;provinceName&#39;] == province[i]].loc[:,&#39;city_confirmedCount&#39;:&#39;city_deadCount&#39;] = draft_b.loc[draft_b[&#39;provinceName&#39;] == province[i]].loc[:,&#39;city_confirmedCount&#39;:&#39;city_deadCount&#39;].diff() draft_b.dropna(subset=[&#39;city_confirmedCount&#39;,&#39;city_suspectedCount&#39;,&#39;city_curedCount&#39;,&#39;city_deadCount&#39;],inplace=True) imported = pd.concat([imported,draft_b], join=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) except: continue imported . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 115956 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 145.0 | 0.0 | 133.0 | 3.0 | 2020-03-24 | 境外输入 | NaN | 0.0 | 9.0 | 0.0 | 0.0 | 0.0 | . 0 NaN | NaN | NaN | NaN | 甘肃省 | NaN | NaN | NaN | NaN | NaN | NaN | 2020-03-24 | NaN | NaN | NaN | 45.0 | 0.0 | 30.0 | 0.0 | . 117597 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 142.0 | 0.0 | 133.0 | 3.0 | 2020-03-24 | 境外输入 | NaN | 0.0 | 6.0 | 0.0 | 0.0 | 0.0 | . 117597 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 142.0 | 0.0 | 133.0 | 3.0 | 2020-03-24 | 境外输入 | NaN | 0.0 | 6.0 | 0.0 | 0.0 | 0.0 | . 116164 亚洲 | Asia | 中国 | China | 甘肃省 | Gansu | 620000.0 | 136.0 | 0.0 | 119.0 | 2.0 | 2020-03-24 | 境外输入 | NaN | 0.0 | 45.0 | 0.0 | 30.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 150 亚洲 | Asia | 中国 | China | 江苏省 | Jiangsu | 320000.0 | 654.0 | 3.0 | 653.0 | 0.0 | 2020-06-23 | 境外输入 | NaN | 0.0 | 23.0 | 0.0 | 22.0 | 0.0 | . 136 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000.0 | 317.0 | 1.0 | 307.0 | 3.0 | 2020-06-23 | 境外输入 | NaN | 0.0 | 72.0 | 0.0 | 65.0 | 0.0 | . 91 NaN | NaN | NaN | NaN | 天津市 | NaN | NaN | NaN | NaN | NaN | NaN | 2020-06-23 | NaN | NaN | NaN | 61.0 | 0.0 | 59.0 | 0.0 | . 136 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000.0 | 317.0 | 1.0 | 307.0 | 3.0 | 2020-06-23 | 境外输入 | NaN | 0.0 | 72.0 | 0.0 | 65.0 | 0.0 | . 201 亚洲 | Asia | 中国 | China | 北京市 | Beijing | 110000.0 | 843.0 | 164.0 | 584.0 | 9.0 | 2020-06-23 | 境外输入 | NaN | 0.0 | 174.0 | 3.0 | 173.0 | 0.0 | . 2524 rows × 19 columns . 分析：作copy()防止数据处理失误使得原数据丢失 . draft_i = imported.copy() . 分析：初始化一个省份数据，保证这个方法可行 . real_s = imported.loc[imported[&#39;provinceName&#39;] == province[0]] real_s.drop_duplicates(subset=&#39;updateTime&#39;, keep=&#39;first&#39;, inplace=True) draft_i = real_s for p in province: real_data = imported.loc[imported[&#39;provinceName&#39;] == p] real_data.drop_duplicates(subset=&#39;updateTime&#39;, keep=&#39;first&#39;, inplace=True) #imported = pd.concat([real_data, china],sort=False) draft_i = pd.concat([real_data,draft_i],sort=False) . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . 分析：确认方法无误，对余下省份进行相同的处理 . imported = draft_i . imported = imported.set_index(&#39;provinceName&#39;) imported = imported.reset_index() . 分析：进行各个省份的数据合并。 . list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = imported.loc[imported[&#39;updateTime&#39;] == date[2]].loc[imported[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;city_confirmedCount&#39;])#该日每省的累计确诊人数 except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_new_foreign_confirmed = pd.DataFrame(list_d,index=list_e) date_new_foreign_confirmed.index.name=&quot;date&quot; date_new_foreign_confirmed.columns=[&quot;imported_confirmedCount&quot;] date_new_foreign_confirmed . imported_confirmedCount . date . 2020-01-24 0 | . l = 0 for i in date[3:]: list_p = [] list_d = [] list_e = [] l +=1 for p in range(0,32): try: con_0 = imported.loc[imported[&#39;updateTime&#39;] == date[l]].loc[imported[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;city_confirmedCount&#39;])#该日每省的累计确诊人数 except: continue #con_0 = imported.loc[imported[&#39;updateTime&#39;] == date[0]].loc[imported[&#39;provinceName&#39;] == &#39;河北省&#39;].loc[[0]].iloc[0] #list_p.append(con_0[&#39;city_confirmedCount&#39;])#该日每省的累计确诊人数 list_d.append(sum(list_p)) list_e.append(str(date[l])) confirmed = pd.DataFrame(list_d, index=list_e) confirmed.index.name=&quot;date&quot; confirmed.columns=[&quot;imported_confirmedCount&quot;] date_new_foreign_confirmed = pd.concat([date_new_foreign_confirmed,confirmed],sort=False) date_new_foreign_confirmed . imported_confirmedCount . date . 2020-01-24 0.0 | . 2020-01-25 0.0 | . 2020-01-26 0.0 | . 2020-01-27 0.0 | . 2020-01-28 0.0 | . ... ... | . 2020-06-17 848.0 | . 2020-06-18 800.0 | . 2020-06-19 800.0 | . 2020-06-20 802.0 | . 2020-06-21 775.0 | . 150 rows × 1 columns . fig = plt.figure( figsize=(16,4), dpi=100) ax = fig.add_subplot(1,1,1) x = date_new_foreign_confirmed.index y = date_new_foreign_confirmed.values plot = ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39;,label=&#39;date_new_foreign_confirmed&#39; ) ax.set_xticks( range(0,len(x),10)) plt.xlabel(&#39;日期&#39;,fontsize=20) plt.ylabel(&#39;人数&#39;,fontsize=20) plt.title(&#39;COVID-19——新增境外输入&#39;,fontsize=30) ax.legend( loc=0, frameon=True ) . &lt;matplotlib.legend.Legend at 0x25553ca5f28&gt; . 分析总结：境外输入病例自3月末开始激增，到5月初增速趋于平缓，到6月初开始增速减缓。 . &#65288;&#22235;&#65289;&#20320;&#25152;&#22312;&#30340;&#30465;&#24066;&#24773;&#20917;&#22914;&#20309;&#65311; . 分析：首先取出广东省的所有时间序列,转换成string类型,然后进行排序 . m_dates = list(set(myhome[&#39;updateTime&#39;])) aid_d = m_dates.copy() for d in aid_d: a = str(d) m_dates.remove(d) m_dates.append(a) m_dates.sort() . myhome = myhome.set_index(&#39;provinceName&#39;) myhome = myhome.reset_index() . 分析：遍历我的城市对应的省份的时间构建对应的dataframe . list_g = [] for i in range(0,len(m_dates)): try: con_m = myhome.loc[myhome[&#39;updateTime&#39;] == date[i]].loc[myhome[&#39;cityName&#39;] == &#39;茂名&#39;].iloc[[0]].iloc[0] list_g.append(con_m[&#39;province_confirmedCount&#39;]) except: list_g.append(0) continue g_date_confirmed = pd.DataFrame(list_g, index=m_dates) g_date_confirmed.index.name=&quot;date&quot; g_date_confirmed.columns=[&quot;g_confirmed&quot;] g_date_confirmed=g_date_confirmed[~g_date_confirmed[&#39;g_confirmed&#39;].isin([0])] #广东省累计治愈人数 list_g = [] for i in range(0,len(m_dates)): try: con_m = myhome.loc[myhome[&#39;updateTime&#39;] == date[i]].loc[myhome[&#39;cityName&#39;] == &#39;茂名&#39;].iloc[[0]].iloc[0] list_g.append(con_m[&#39;province_curedCount&#39;]) except: list_g.append(0) continue g_date_cured = pd.DataFrame(list_g, index=m_dates) g_date_cured.index.name=&quot;date&quot; g_date_cured.columns=[&quot;g_cured&quot;] g_date_cured=g_date_cured[~g_date_cured[&#39;g_cured&#39;].isin([0])] #广东省累计死亡人数 list_g = [] for i in range(0,len(m_dates)): try: con_m = myhome.loc[myhome[&#39;updateTime&#39;] == date[i]].loc[myhome[&#39;cityName&#39;] == &#39;茂名&#39;].iloc[[0]].iloc[0] list_g.append(con_m[&#39;province_deadCount&#39;]) except: list_g.append(0) continue g_date_dead = pd.DataFrame(list_g, index=m_dates) g_date_dead.index.name=&quot;date&quot; g_date_dead.columns=[&quot;g_dead&quot;] g_date_dead=g_date_dead[~g_date_dead[&#39;g_dead&#39;].isin([0])] . 分析：作折线图表现疫情随时间变化趋势 . plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] x= g_date_confirmed.index y1 = g_date_confirmed.values y2 = g_date_cured.values y3 = g_date_dead #font_manager = font_manager.FontProperties(fname = &#39;C:/Windows/Fonts/simsun.ttc&#39;,size = 18) plt.figure(figsize=(20,10),dpi = 80) plt.plot(x,y1,color = r_hex,label = &#39;confirmed&#39;) plt.plot(x,y2,color = g_hex,label = &#39;cured&#39;) x_major_locator=MultipleLocator(12) ax=plt.gca() ax.xaxis.set_major_locator(x_major_locator) plt.title(&#39;COVID-19 —— 广东省&#39;,fontsize=30) plt.xlabel(&#39;日期&#39;,fontsize=20) plt.ylabel(&#39;人数&#39;,fontsize=20) plt.legend(loc=1, bbox_to_anchor=(1.00,0.90), bbox_transform=ax.transAxes) . &lt;matplotlib.legend.Legend at 0x25553d02a90&gt; . plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] fig = plt.figure( figsize=(16,4), dpi=100) ax = fig.add_subplot(1,1,1) x = g_date_dead.index y = g_date_dead.values plot = ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39;,label=&#39;dead&#39; ) ax.set_xticks( range(0,len(x),10)) plt.xlabel(&#39;日期&#39;,fontsize=20) plt.ylabel(&#39;人数&#39;,fontsize=20) plt.title(&#39;COVID-19——广东省&#39;,fontsize=30) ax.legend( loc=0, frameon=True ) . &lt;matplotlib.legend.Legend at 0x25553d94940&gt; . 分析：广东省的数据补全很成功，真实性高。 分析：从折线图来看，广东省自1月末起感染人数激增，直到2月中旬趋于平缓，3月初开始由于检测普及以及统计因素，短期确诊患者人数小幅度增加。广东省自2月初开始治愈人数激增，直到6月初开始因为新增感染人数趋于平缓，所以治愈人数趋于平缓。广东省自3月初开始不再有新增死亡患者。 . &#65288;&#20116;&#65289;&#22269;&#22806;&#30123;&#24773;&#24577;&#21183;&#22914;&#20309;&#65311; . 分析：数据去除空值 . world.dropna(axis=1, how=&#39;any&#39;, inplace=True) #world.set_index(&#39;updateTime&#39;) . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . 分析：创建国家列表country，创建日期列表date_y . country = list(set(world[&#39;provinceName&#39;])) date_y = [] for dt in world.loc[world[&#39;provinceName&#39;] == country[0]][&#39;updateTime&#39;]: date_y.append(str(dt)) date_y = list(set(date_0)) date_y.sort() . 分析：遍历国家列表对world中的updateTime进行处理并去重。 . for c in country: world.loc[world[&#39;provinceName&#39;] == c].sort_values(by = &#39;updateTime&#39;) world.dropna(subset=[&#39;provinceName&#39;],inplace=True) world.updateTime = pd.to_datetime(world.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This is separate from the ipykernel package so we can avoid doing imports until D: Anaconda envs python32 lib site-packages pandas core generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . 分析：取前15个国家的province_confirmedCount透视构成world_confirmed，并进行数据补全处理 . world_confirmed = world.loc[world[&#39;provinceName&#39;] == world.head(15)[&#39;provinceName&#39;][0]].pivot_table(index=&#39;updateTime&#39;, columns=&#39;provinceName&#39;, values=&#39;province_confirmedCount&#39;,aggfunc=np.mean) for i in world.head(15)[&#39;provinceName&#39;][1:]: draft_c = world.loc[world[&#39;provinceName&#39;] == i].pivot_table(index=&#39;updateTime&#39;, columns=&#39;provinceName&#39;, values=&#39;province_confirmedCount&#39;,aggfunc=np.mean) world_confirmed = pd.merge(world_confirmed,draft_c,on=&#39;updateTime&#39;, how=&#39;outer&#39;,sort=True) world_confirmed.fillna(0,inplace=True,limit = 1) world_confirmed.fillna(method=&quot;ffill&quot;,inplace=True) world_confirmed . provinceName 美国 巴西 英国 俄罗斯 智利 印度 巴基斯坦 秘鲁 西班牙 孟加拉国 法国 沙特阿拉伯 瑞典 南非 厄瓜多尔 . updateTime . 2020-01-27 5.000000e+00 | 0.00 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.00 | 0.00 | 3.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 2020-01-29 0.000000e+00 | 0.00 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.00 | 0.00 | 4.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 2020-01-30 0.000000e+00 | 0.00 | 0.000000 | 0.0 | 0.0 | 1.000000 | 0.000000 | 0.000000 | 0.00 | 0.00 | 5.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 2020-01-31 6.000000e+00 | 0.00 | 2.000000 | 2.0 | 0.0 | 1.000000 | 0.000000 | 0.000000 | 0.00 | 0.00 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 2020-02-01 6.000000e+00 | 0.00 | 2.000000 | 2.0 | 0.0 | 1.000000 | 0.000000 | 0.000000 | 4.00 | 0.00 | 5.500000 | 0.000000 | 1.000000 | 0.0 | 0.000000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2020-06-19 2.184912e+06 | 976906.50 | 300469.000000 | 563084.0 | 225103.0 | 371474.666667 | 162935.600000 | 243518.000000 | 245268.00 | 102292.00 | 158641.000000 | 145991.000000 | 55672.750000 | 83020.5 | 48256.400000 | . 2020-06-20 2.221982e+06 | 1038568.00 | 302138.750000 | 573007.5 | 231393.0 | 390209.333333 | 169464.666667 | 247925.000000 | 245665.75 | 105535.00 | 159452.000000 | 151277.250000 | 56201.500000 | 87715.0 | 49519.666667 | . 2020-06-21 2.253118e+06 | 1068977.25 | 303284.428571 | 579160.0 | 236748.0 | 399451.714286 | 174346.222222 | 251338.000000 | 245938.00 | 109657.75 | 160093.000000 | 154715.714286 | 56360.000000 | 92681.0 | 49731.000000 | . 2020-06-22 2.279603e+06 | 1084312.25 | 304331.000000 | 587720.0 | 243276.6 | 416389.400000 | 179148.750000 | 254336.333333 | 246272.00 | 112306.00 | 160336.428571 | 158177.500000 | 57346.000000 | 96377.8 | 50092.600000 | . 2020-06-23 2.299650e+06 | 1106470.00 | 305289.000000 | 592280.0 | 246963.0 | 425282.000000 | 182562.666667 | 257447.000000 | 246504.00 | 115786.00 | 160750.000000 | 161005.000000 | 59060.666667 | 101590.0 | 50487.666667 | . 144 rows × 15 columns . 分析：作前15个国家的疫情随时间变动表 . fig = plt.figure(figsize=(16,10)) plt.plot(world_confirmed) plt.legend(world_confirmed.columns) plt.title(&#39;前15个国家累计确诊人数&#39;,fontsize=20) plt.xlabel(&#39;日期&#39;,fontsize=20) plt.ylabel(&#39;人数/百万&#39;,fontsize=20); . 分析：国外数据的补全较为成功，有一定的真实性。 分析：国外新冠确诊人数自3月末开始激增，排名前四的国家的疫情没有受到控制的趋势，国外疫情的趋势为确诊人数继续激增。 . &#65288;&#20845;&#65289;&#32467;&#21512;&#20320;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#22312;&#25239;&#20987;&#30123;&#24773;&#26041;&#38754;&#26377;&#20309;&#24314;&#35758;&#65311; . 从国内疫情折线图来看，从4月末开始疫情趋于平缓，相反，国外疫情从4月初开始爆发，至今没有看到平缓的趋势。 从境外输入案例来看，我们需要谨防境外输入病例，遏制国内新冠再次传播，一切都不能放松警惕。 对于个人，我们要避免到人员密集的区域，外出一定要戴好口罩，回家要做全面的消毒。 对于社会，在交通发达区域和人员密集区域，需要普及病毒检测和场所消毒措施，切断病毒的传播途径，维护我国疫情防控的成果。 . &#38468;&#21152;&#20998;&#26512;(&#36873;&#20570;&#65292;&#20294;&#20570;&#30340;&#20986;&#24425;&#20250;&#21152;&#20998;&#21734;) . 附加分析，所使用的库不限，比如可以使用seaborn、pyecharts等库。 . 童鞋们，自由发挥吧！！ .",
            "url": "https://jumbojing.github.io/jupyter_notebook_blogs//jupyter_notebook_blogs/2020/07/01/Cov2019Analysis.html",
            "relUrl": "/2020/07/01/Cov2019Analysis.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jumbojing.github.io/jupyter_notebook_blogs//jupyter_notebook_blogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Markdown文档书写简单示例",
            "content": "markdown文档书写的简单示例 . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jumbojing.github.io/jupyter_notebook_blogs//jupyter_notebook_blogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "本博客展示本人的Python学习笔记。 如果对文章还有疑问或者见解，建议点击—&gt;“这里”关注我，里面有我的最新动态。 [^1]:This website is powered by fastpages . [^2].a blogging platform that natively supports Jupyter notebooks in addition to other formats. .",
          "url": "https://jumbojing.github.io/jupyter_notebook_blogs//jupyter_notebook_blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": "Thanks for your reading, this is my study notes for Python, if you find something wrong in my essay, please leave a comment and I will see. . Posts .",
          "url": "https://jumbojing.github.io/jupyter_notebook_blogs//jupyter_notebook_blogs/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jumbojing.github.io/jupyter_notebook_blogs//jupyter_notebook_blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}