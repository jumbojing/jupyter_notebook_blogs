{
  
    
        "post0": {
            "title": "Python新线程使用",
            "content": "创建并使用多线程 . print(&#39;主线程执行代码&#39;) # 从 threading 库中导入Thread类 from threading import Thread from time import sleep # 定义一个函数，作为新线程执行的入口函数 def threadFunc(arg1,arg2): print(&#39;子线程 开始&#39;) print(f&#39;线程函数参数是：{arg1}, {arg2}&#39;) sleep(5) print(&#39;子线程 结束&#39;) # 创建 Thread 类的实例对象， 并且指定新线程的入口函数，此时并没有执行 thread = Thread(target=threadFunc, args=(&#39;参数1&#39;, &#39;参数2&#39;) ) #target=threadFunc对应执行的函数threadFunc #args=(&#39;参数1&#39;, &#39;参数2&#39;)这样新进程添加参数 # 执行start 方法，就会创建新线程， # 并且新线程会去执行入口函数里面的代码。 # 这时候这个进程有两个线程了。↓ thread.start() # 主线程的代码执行 子线程对象的join方法， # 就会等待子线程结束，才继续执行下面的代码 thread.join() print(&#39;主线程结束&#39;) . 运行该程序，解释器执行到下面代码时 . thread = Thread(target=threadFunc, args=(&#39;参数1&#39;, &#39;参数2&#39;) ) . 创建了一个Thread实例对象，其中，Thread类的初始化参数 有两个 . target参数 是指定新线程的 入口函数， 新线程创建后就会 执行该入口函数里面的代码， . args 指定了 传给 入口函数threadFunc 的参数。 线程入口函数 参数，必须放在一个元组里面，里面的元素依次作为入口函数的参数。 . 注意，上面的代码只是创建了一个Thread实例对象， 但这时，新的线程还没有创建。 . 要创建线程，必须要调用 Thread 实例对象的 start方法 。也就是执行完下面代码的时候 . thread.start() . 新的线程才创建成功，并开始执行 入口函数threadFunc 里面的代码。 . 有的时候， 一个线程需要等待其它的线程结束，比如需要根据其他线程运行结束后的结果进行处理。 . 这时可以使用 Thread对象的 join 方法 . thread.join() . 如果一个线程A的代码调用了 对应线程B的Thread对象的 join 方法，线程A就会停止继续执行代码，等待线程B结束。 线程B结束后，线程A才继续执行后续的代码。 . 所以主线程在执行上面的代码时，就暂停在此处， 一直要等到 新线程执行完毕，退出后，才会继续执行后续的代码。 python #错误示例！！！！ thread = Thread(target=threadFunc(&#39;参数1&#39;, &#39;参数2&#39;)) . ↑如果这样写无法创建新线程并执行，这样target传入不是函数，传入的是运行结果(null)，而且是在主线程运行完了，并不是在子线程里运行。 . 共享数据的访问控制 . 做多线程开发，经常遇到这样的情况：多个线程里面的代码需要访问同一个公共的数据对象。 . 这个公共的数据对象可以是任何类型， 比如一个列表、字典、或者自定义类的对象。 . 有的时候，程序需要防止线程的代码同时操作公共数据对象。否则，就有可能导致数据的访问互相冲突影响。 . 请看一个例子。 . 我们用一个简单的程序模拟一个银行系统，用户可以往自己的帐号上存钱。 . 对应代码如下： . from threading import Thread from time import sleep bank = { &#39;byhy&#39; : 0 } # 定义一个函数，作为新线程执行的入口函数 def deposit(theadidx,amount): balance = bank[&#39;byhy&#39;] # 执行一些任务，耗费了0.1秒 sleep(0.1) bank[&#39;byhy&#39;] = balance + amount print(f&#39;子线程 {theadidx} 结束&#39;) theadlist = [] for idx in range(10): thread = Thread(target = deposit, args = (idx,1) ) thread.start() # 把线程对象都存储到 threadlist中 theadlist.append(thread) for thread in theadlist: thread.join() print(&#39;主线程结束&#39;) print(f&#39;最后我们的账号余额为 {bank[&quot;byhy&quot;]}&#39;) . 上面的代码中，一起执行 . 开始的时候， 该帐号的余额为0，随后我们启动了10个线程， 每个线程都deposit函数，往帐号byhy上存1元钱。 . 可以预期，执行完程序后，该帐号的余额应该为 10。 . 然而，我们运行程序后，发现结果如下 . 子线程 0 结束 子线程 3 结束 子线程 2 结束 子线程 4 结束 子线程 1 结束 子线程 7 结束 子线程 5 结束 子线程 9 结束 子线程 6 结束 子线程 8 结束 主线程结束 . 最后我们的账号余额为 1 为什么是 1 呢？ 而不是 10 呢？ . 如果在我们程序代码中，只有一个线程，如下所示 . from time import sleep bank = { &#39;byhy&#39; : 0 } # 定义一个函数，作为新线程执行的入口函数 def deposit(theadidx,amount): balance = bank[&#39;byhy&#39;] # 执行一些任务，耗费了0.1秒 sleep(0.1) bank[&#39;byhy&#39;] = balance + amount for idx in range(10): deposit (idx,1) print(f&#39;最后我们的账号余额为 {bank[&quot;byhy&quot;]}&#39;) . 代码都是串行执行的。不存在多线程同时访问bank对象的问题，运行结果一切都是正常的。 . 现在我们程序代码中，有多个线程，并且在这个几个线程中都会去调用deposit，就有可能同时操作这个bank对象，就有可能出一个线程覆盖另外一个线程的结果的问题。 . 这时，可以使用threading库里面的锁对象Lock去保护。 . 我们修改多线程代码，如下： . from threading import Thread,Lock from time import sleep bank = { &#39;byhy&#39; : 0 } bankLock = Lock() # 定义一个函数，作为新线程执行的入口函数 def deposit(theadidx,amount): # 操作共享数据前，申请获取锁 bankLock.acquire() balance = bank[&#39;byhy&#39;] # 执行一些任务，耗费了0.1秒 sleep(0.1) bank[&#39;byhy&#39;] = balance + amount print(f&#39;子线程 {theadidx} 结束&#39;) # 操作完共享数据后，申请释放锁 bankLock.release() theadlist = [] for idx in range(10): thread = Thread(target = deposit, args = (idx,1) ) thread.start() # 把线程对象都存储到 threadlist中 theadlist.append(thread) for thread in theadlist: thread.join() print(&#39;主线程结束&#39;) print(f&#39;最后我们的账号余额为 {bank[&quot;byhy&quot;]}&#39;) . 执行一下，结果如下 . 子线程 0 结束 子线程 1 结束 子线程 2 结束 子线程 3 结束 子线程 4 结束 子线程 5 结束 子线程 6 结束 子线程 7 结束 子线程 8 结束 子线程 9 结束 主线程结束 最后我们的账号余额为 10 . 正确了。 . 每个线程在操作共享数据对象之前，都应该申请获取操作权，也就是调用该共享数据对象对应的锁对象的acquire方法。 如果线程A执行如下代码，调用acquire方法的时候， python bankLock.acquire() . 别的线程B已经申请到了这个锁，并且还没有释放，那么线程A的代码就在此处等待线程B释放锁，不去执行后面的代码。 . 直到线程B执行了锁的release方法释放了这个锁，线程A才可以获取这个锁，就可以执行下面的代码了。 . 如果这时线程B又执行这个锁的acquire方法，就需要等待线程A执行该锁对象的release方法释放锁，否则也会等待，不去执行后面的代码。 . daemon线程 . from threading import Thread from time import sleep def threadFunc(): sleep(2) print(&#39;子线程 结束&#39;) thread = Thread(target=threadFunc) thread.start() print(&#39;主线程结束&#39;) . 可以发现，主线程先结束，要过个2秒钟，等子线程运行完，整个程序才会结束退出。 . 因为： . Python程序中当所有的 非daemon线程 结束了，整个程序才会结束 主线程是非daemon线程，启动的子线程缺省也是非daemon线程线程。 . 所以，要等到主线程和子线程都结束，程序才会结束。 . 我们可以在创建线程的时候，设置daemon参数值为True，如下 . from threading import Thread from time import sleep def threadFunc(): sleep(2) print(&#39;子线程 结束&#39;) thread = Thread(target=threadFunc, daemon=True # 设置新线程为daemon线程 ) thread.start() print(&#39;主线程结束&#39;) . 再次运行，可以发现，只要主线程结束了，整个程序就结束了。因为只有主线程是非daemon线程。 .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/08/31/%E5%88%9B%E5%BB%BA%E6%96%B0%E7%BA%BF%E7%A8%8B(%E9%80%9A%E7%94%A8).html",
            "relUrl": "/markdown/2020/08/31/%E5%88%9B%E5%BB%BA%E6%96%B0%E7%BA%BF%E7%A8%8B(%E9%80%9A%E7%94%A8).html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "2019新型冠状病毒（COVID-19/2019-nCoV）疫情分析",
            "content": "&#37325;&#35201;&#35828;&#26126; . 分析文档：完成度：代码质量 3:5:2 . 其中分析文档是指你数据分析的过程中，对各问题分析的思路、对结果的解释、说明(要求言简意赅，不要为写而写) . ps:你自己写的代码胜过一切的代笔，无关美丑，只问今日比昨日更长进！加油！ . 由于数据过多，查看数据尽量使用head()或tail()，以免程序长时间无响应 . ======================= . 本项目数据来源于丁香园。本项目主要目的是通过对疫情历史数据的分析研究，以更好的了解疫情与疫情的发展态势，为抗击疫情之决策提供数据支持。 . &#19968;. &#25552;&#20986;&#38382;&#39064; . 从全国范围，你所在省市，国外疫情等三个方面主要研究以下几个问题： . （一）全国累计确诊/疑似/治愈/死亡情况随时间变化趋势如何？ . （二）全国新增确诊/疑似/治愈/死亡情况随时间变化趋势如何？ . （三）全国新增境外输入随时间变化趋势如何？ . （四）你所在的省市情况如何？ . （五）国外疫情态势如何？ . （六）结合你的分析结果，对个人和社会在抗击疫情方面有何建议？ . &#20108;. &#29702;&#35299;&#25968;&#25454; . 原始数据集：AreaInfo.csv，导入相关包及读取数据： . r_hex = &#39;#dc2624&#39; # red, RGB = 220,38,36 dt_hex = &#39;#2b4750&#39; # dark teal, RGB = 43,71,80 tl_hex = &#39;#45a0a2&#39; # teal, RGB = 69,160,162 r1_hex = &#39;#e87a59&#39; # red, RGB = 232,122,89 tl1_hex = &#39;#7dcaa9&#39; # teal, RGB = 125,202,169 g_hex = &#39;#649E7D&#39; # green, RGB = 100,158,125 o_hex = &#39;#dc8018&#39; # orange, RGB = 220,128,24 tn_hex = &#39;#C89F91&#39; # tan, RGB = 200,159,145 g50_hex = &#39;#6c6d6c&#39; # grey-50, RGB = 108,109,108 bg_hex = &#39;#4f6268&#39; # blue grey, RGB = 79,98,104 g25_hex = &#39;#c7cccf&#39; # grey-25, RGB = 199,204,207 . import numpy as np import pandas as pd import matplotlib,re import matplotlib.pyplot as plt from matplotlib.pyplot import MultipleLocator data = pd.read_csv(r&#39;data/AreaInfo.csv&#39;) . 查看与统计数据，以对数据有一个大致了解 . data.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 0 北美洲 | North America | 美国 | United States of America | 美国 | United States of America | 971002 | 2306247 | 0.0 | 640198 | 120351 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 南美洲 | South America | 巴西 | Brazil | 巴西 | Brazil | 973003 | 1106470 | 0.0 | 549386 | 51271 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 欧洲 | Europe | 英国 | United Kingdom | 英国 | United Kingdom | 961007 | 305289 | 0.0 | 539 | 42647 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 欧洲 | Europe | 俄罗斯 | Russia | 俄罗斯 | Russia | 964006 | 592280 | 0.0 | 344416 | 8206 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 南美洲 | South America | 智利 | Chile | 智利 | Chile | 973004 | 246963 | 0.0 | 44946 | 4502 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . &#19977;. &#25968;&#25454;&#28165;&#27927; . &#65288;&#19968;&#65289;&#22522;&#26412;&#25968;&#25454;&#22788;&#29702; . 数据清洗主要包括：选取子集，缺失数据处理、数据格式转换、异常值数据处理等。 . &#22269;&#20869;&#30123;&#24773;&#25968;&#25454;&#36873;&#21462;&#65288;&#26368;&#32456;&#36873;&#21462;&#30340;&#25968;&#25454;&#21629;&#21517;&#20026;china&#65289; . 选取国内疫情数据 . | 对于更新时间(updateTime)列，需将其转换为日期类型并提取出年-月-日，并查看处理结果。(提示：dt.date) . | 因数据每天按小时更新，一天之内有很多重复数据，请去重并只保留一天之内最新的数据。 . | 提示：df.drop_duplicates(subset=[&#39;provinceName&#39;, &#39;updateTime&#39;], keep=&#39;first&#39;, inplace=False) . 其中df是你选择的国内疫情数据的DataFrame . 分析：选取countryName一列中值为中国的行组成CHINA。 . CHINA = data.loc[data[&#39;countryName&#39;] == &#39;中国&#39;] CHINA.dropna(subset=[&#39;cityName&#39;], how=&#39;any&#39;, inplace=True) #CHINA . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . 分析：取出含所有中国城市的列表 . cities = list(set(CHINA[&#39;cityName&#39;])) . 分析：遍历取出每一个城市的子dataframe，然后用sort对updateTime进行时间排序 . for city in cities: CHINA.loc[data[&#39;cityName&#39;] == city].sort_values(by = &#39;updateTime&#39;) . 分析：去除空值所在行 . CHINA.dropna(subset=[&#39;cityName&#39;],inplace=True) #CHINA.loc[CHINA[&#39;cityName&#39;] == &#39;秦皇岛&#39;].tail(20) . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . 分析：将CHINA中的updateTime列进行格式化处理 . CHINA.updateTime = pd.to_datetime(CHINA.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date #CHINA.loc[data[&#39;cityName&#39;] == &#39;秦皇岛&#39;].tail(15) . D: Anaconda envs python32 lib site-packages pandas core generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . CHINA.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 136 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 境外输入 | NaN | 0.0 | 72.0 | 0.0 | 65.0 | 0.0 | . 137 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 西安 | Xi&#39;an | 610100.0 | 120.0 | 0.0 | 117.0 | 3.0 | . 138 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 安康 | Ankang | 610900.0 | 26.0 | 0.0 | 26.0 | 0.0 | . 139 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 汉中 | Hanzhong | 610700.0 | 26.0 | 0.0 | 26.0 | 0.0 | . 140 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 咸阳 | Xianyang | 610400.0 | 17.0 | 0.0 | 17.0 | 0.0 | . 分析：每日数据的去重只保留第一个数据，因为前面已经对时间进行排序，第一个数据即为当天最新数据 分析：考虑到合并dataframe需要用到concat，需要创建一个初始china . real = CHINA.loc[data[&#39;cityName&#39;] == cities[1]] real.drop_duplicates(subset=&#39;updateTime&#39;, keep=&#39;first&#39;, inplace=True) china = real . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . 分析：遍历每个城市dataframe进行每日数据的去重，否则会出现相同日期只保留一个城市的数据的情况 . for city in cities[2:]: real_data = CHINA.loc[data[&#39;cityName&#39;] == city] real_data.drop_duplicates(subset=&#39;updateTime&#39;, keep=&#39;first&#39;, inplace=True) china = pd.concat([real_data, china],sort=False) . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This is separate from the ipykernel package so we can avoid doing imports until . 查看数据信息，是否有缺失数据/数据类型是否正确。 . 提示：若不会处理缺失值，可以将其舍弃 . 分析：有的城市不是每日都上报的，如果某日只统计上报的那些城市，那些存在患者却不上报的城市就会被忽略，数据就失真了，需要补全所有城市每日的数据，即便不上报的城市也要每日记录数据统计，所以要进行插值处理补全部分数据，处理方法详见数据透视与分析 . china.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 32812 entries, 96106 to 208267 Data columns (total 19 columns): # Column Non-Null Count Dtype -- -- 0 continentName 32812 non-null object 1 continentEnglishName 32812 non-null object 2 countryName 32812 non-null object 3 countryEnglishName 32812 non-null object 4 provinceName 32812 non-null object 5 provinceEnglishName 32812 non-null object 6 province_zipCode 32812 non-null int64 7 province_confirmedCount 32812 non-null int64 8 province_suspectedCount 32812 non-null float64 9 province_curedCount 32812 non-null int64 10 province_deadCount 32812 non-null int64 11 updateTime 32812 non-null object 12 cityName 32812 non-null object 13 cityEnglishName 31968 non-null object 14 city_zipCode 32502 non-null float64 15 city_confirmedCount 32812 non-null float64 16 city_suspectedCount 32812 non-null float64 17 city_curedCount 32812 non-null float64 18 city_deadCount 32812 non-null float64 dtypes: float64(6), int64(4), object(9) memory usage: 5.0+ MB . china.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 96106 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 254 | 0.0 | 252 | 2 | 2020-04-02 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 125120 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 254 | 0.0 | 250 | 2 | 2020-03-20 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 128762 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 253 | 0.0 | 250 | 2 | 2020-03-18 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 130607 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 253 | 0.0 | 248 | 2 | 2020-03-17 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 131428 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 252 | 0.0 | 248 | 2 | 2020-03-16 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . &#20320;&#25152;&#22312;&#30465;&#24066;&#30123;&#24773;&#25968;&#25454;&#36873;&#21462;&#65288;&#26368;&#32456;&#36873;&#21462;&#30340;&#25968;&#25454;&#21629;&#21517;&#20026;myhome&#65289; . 此步也可在后面用到的再做 . myhome = china.loc[data[&#39;provinceName&#39;] == &#39;广东省&#39;] myhome.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 205259 亚洲 | Asia | 中国 | China | 广东省 | Guangdong | 440000 | 277 | 0.0 | 5 | 0 | 2020-01-29 | 外地来粤人员 | NaN | NaN | 5.0 | 0.0 | 0.0 | 0.0 | . 206335 亚洲 | Asia | 中国 | China | 广东省 | Guangdong | 440000 | 207 | 0.0 | 4 | 0 | 2020-01-28 | 河源市 | NaN | NaN | 1.0 | 0.0 | 0.0 | 0.0 | . 205239 亚洲 | Asia | 中国 | China | 广东省 | Guangdong | 440000 | 277 | 0.0 | 5 | 0 | 2020-01-29 | 外地来穗人员 | NaN | NaN | 5.0 | 0.0 | 0.0 | 0.0 | . 252 亚洲 | Asia | 中国 | China | 广东省 | Guangdong | 440000 | 1634 | 11.0 | 1619 | 8 | 2020-06-23 | 潮州 | Chaozhou | 445100.0 | 6.0 | 0.0 | 6.0 | 0.0 | . 2655 亚洲 | Asia | 中国 | China | 广东省 | Guangdong | 440000 | 1634 | 11.0 | 1614 | 8 | 2020-06-21 | 潮州 | Chaozhou | 445100.0 | 6.0 | 0.0 | 6.0 | 0.0 | . &#22269;&#22806;&#30123;&#24773;&#25968;&#25454;&#36873;&#21462;&#65288;&#26368;&#32456;&#36873;&#21462;&#30340;&#25968;&#25454;&#21629;&#21517;&#20026;world&#65289; . 此步也可在后面用到的再做 . world = data.loc[data[&#39;countryName&#39;] != &#39;中国&#39;] world.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 0 北美洲 | North America | 美国 | United States of America | 美国 | United States of America | 971002 | 2306247 | 0.0 | 640198 | 120351 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 南美洲 | South America | 巴西 | Brazil | 巴西 | Brazil | 973003 | 1106470 | 0.0 | 549386 | 51271 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 欧洲 | Europe | 英国 | United Kingdom | 英国 | United Kingdom | 961007 | 305289 | 0.0 | 539 | 42647 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 欧洲 | Europe | 俄罗斯 | Russia | 俄罗斯 | Russia | 964006 | 592280 | 0.0 | 344416 | 8206 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 南美洲 | South America | 智利 | Chile | 智利 | Chile | 973004 | 246963 | 0.0 | 44946 | 4502 | 2020-06-23 10:01:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 数据透视与分析 . 分析：对china进行插值处理补全部分数据 . china.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 96106 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 254 | 0.0 | 252 | 2 | 2020-04-02 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 125120 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 254 | 0.0 | 250 | 2 | 2020-03-20 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 128762 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 253 | 0.0 | 250 | 2 | 2020-03-18 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 130607 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 253 | 0.0 | 248 | 2 | 2020-03-17 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 131428 亚洲 | Asia | 中国 | China | 广西壮族自治区 | Guangxi | 450000 | 252 | 0.0 | 248 | 2 | 2020-03-16 | 贵港 | Guigang | 450800.0 | 8.0 | 0.0 | 8.0 | 0.0 | . 分析：先创建省份列表和日期列表，并初始化一个draft . province = list(set(china[&#39;provinceName&#39;]))#每个省份 #p_city = list(set(china[china[&#39;provinceName&#39;] == province[0]][&#39;cityName&#39;]))#每个省份的城市 date_0 = [] for dt in china.loc[china[&#39;provinceName&#39;] == province[0]][&#39;updateTime&#39;]: date_0.append(str(dt)) date_0 = list(set(date_0)) date_0.sort() start = china.loc[china[&#39;provinceName&#39;] == province[0]][&#39;updateTime&#39;].min() end = china.loc[china[&#39;provinceName&#39;] == province[0]][&#39;updateTime&#39;].max() dates = pd.date_range(start=str(start), end=str(end)) aid_frame = pd.DataFrame({&#39;updateTime&#39;: dates,&#39;provinceName&#39;:[province[0]]*len(dates)}) aid_frame.updateTime = pd.to_datetime(aid_frame.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date #draft = pd.merge(china.loc[china[&#39;provinceName&#39;] == province[1]], aid_frame, on=&#39;updateTime&#39;, how=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) draft = pd.concat([china.loc[china[&#39;provinceName&#39;] == province[0]], aid_frame], join=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) draft.province_confirmedCount.fillna(method=&quot;ffill&quot;,inplace=True) draft.province_suspectedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft.province_curedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft.province_deadCount.fillna(method=&quot;ffill&quot;, inplace=True) . 分析：补全部分时间，取前日的数据进行插值，因为有的省份从4月末开始陆续就不再有新增病患，不再上报，所以这些省份的数据只能补全到4月末，往后的数据逐渐失去真实性 . 分析：同时进行日期格式化 . for p in range(1,len(province)): date_d = [] for dt in china.loc[china[&#39;provinceName&#39;] == province[p]][&#39;updateTime&#39;]: date_d.append(dt) date_d = list(set(date_d)) date_d.sort() start = china.loc[china[&#39;provinceName&#39;] == province[p]][&#39;updateTime&#39;].min() end = china.loc[china[&#39;provinceName&#39;] == province[p]][&#39;updateTime&#39;].max() dates = pd.date_range(start=start, end=end) aid_frame = pd.DataFrame({&#39;updateTime&#39;: dates,&#39;provinceName&#39;:[province[p]]*len(dates)}) aid_frame.updateTime = pd.to_datetime(aid_frame.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date X = china.loc[china[&#39;provinceName&#39;] == province[p]] X.reset_index(drop= True) Y = aid_frame Y.reset_index(drop= True) draft_d = pd.concat([X,Y], join=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) draft = pd.concat([draft,draft_d]) draft.province_confirmedCount.fillna(method=&quot;ffill&quot;,inplace=True) draft.province_suspectedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft.province_curedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft.province_deadCount.fillna(method=&quot;ffill&quot;, inplace=True) #draft[&#39;updateTime&#39;] = draft[&#39;updateTime&#39;].strftime(&#39;%Y-%m-%d&#39;) #draft[&#39;updateTime&#39;] = pd.to_datetime(draft[&#39;updateTime&#39;],format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date . china = draft . china.head() . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 208226 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 14.0 | 0.0 | 0.0 | 0.0 | 2020-01-26 | 外地来津 | NaN | NaN | 2.0 | 0.0 | 0.0 | 0.0 | . 208224 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 14.0 | 0.0 | 0.0 | 0.0 | 2020-01-26 | 河北区 | Hebei District | 120105.0 | 5.0 | 0.0 | 0.0 | 0.0 | . 208228 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 14.0 | 0.0 | 0.0 | 0.0 | 2020-01-26 | 和平区 | Heping District | 120101.0 | 1.0 | 0.0 | 0.0 | 0.0 | . 208227 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 14.0 | 0.0 | 0.0 | 0.0 | 2020-01-26 | 滨海新区 | Binhai New Area | 120116.0 | 1.0 | 0.0 | 0.0 | 0.0 | . 208230 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 14.0 | 0.0 | 0.0 | 0.0 | 2020-01-26 | 西青区 | Xiqing District | 120111.0 | 1.0 | 0.0 | 0.0 | 0.0 | . &#22235;. &#25968;&#25454;&#20998;&#26512;&#21450;&#21487;&#35270;&#21270; . 在进行数据分析及可视化时，依据每个问题选取所需变量并新建DataFrame再进行分析和可视化展示，这样数据不易乱且条理更清晰。 . &#22522;&#30784;&#20998;&#26512; . 基础分析，只允许使用numpy、pandas和matplotlib库。 . 可以在一张图上多个坐标系展示也可以在多张图上展示 . 请根据分析目的选择图形的类型(折线图、饼图、直方图和散点图等等)，实在没有主意可以到百度疫情地图或其他疫情分析的站点激发激发灵感。 . &#65288;&#19968;&#65289;&#20840;&#22269;&#32047;&#35745;&#30830;&#35786;/&#30097;&#20284;/&#27835;&#24840;/&#27515;&#20129;&#24773;&#20917;&#38543;&#26102;&#38388;&#21464;&#21270;&#36235;&#21183;&#22914;&#20309;&#65311; . 分析：要获得全国累计情况随时间变化趋势，首先需要整合每日全国累计确诊情况做成date_confirmed . 分析：要整合每日全国累计确诊情况，首先得提取每个省份每日当天最新累计确诊人数，省份数据求和后形成dataframe， for循环拼接到date_confirmed中 . date = list(set(china[&#39;updateTime&#39;])) date.sort() date . [datetime.date(2020, 1, 24), datetime.date(2020, 1, 25), datetime.date(2020, 1, 26), datetime.date(2020, 1, 27), datetime.date(2020, 1, 28), datetime.date(2020, 1, 29), datetime.date(2020, 1, 30), datetime.date(2020, 1, 31), datetime.date(2020, 2, 1), datetime.date(2020, 2, 2), datetime.date(2020, 2, 3), datetime.date(2020, 2, 4), datetime.date(2020, 2, 5), datetime.date(2020, 2, 6), datetime.date(2020, 2, 7), datetime.date(2020, 2, 8), datetime.date(2020, 2, 9), datetime.date(2020, 2, 10), datetime.date(2020, 2, 11), datetime.date(2020, 2, 12), datetime.date(2020, 2, 13), datetime.date(2020, 2, 14), datetime.date(2020, 2, 15), datetime.date(2020, 2, 16), datetime.date(2020, 2, 17), datetime.date(2020, 2, 18), datetime.date(2020, 2, 19), datetime.date(2020, 2, 20), datetime.date(2020, 2, 21), datetime.date(2020, 2, 22), datetime.date(2020, 2, 23), datetime.date(2020, 2, 24), datetime.date(2020, 2, 25), datetime.date(2020, 2, 26), datetime.date(2020, 2, 27), datetime.date(2020, 2, 28), datetime.date(2020, 2, 29), datetime.date(2020, 3, 1), datetime.date(2020, 3, 2), datetime.date(2020, 3, 3), datetime.date(2020, 3, 4), datetime.date(2020, 3, 5), datetime.date(2020, 3, 6), datetime.date(2020, 3, 7), datetime.date(2020, 3, 8), datetime.date(2020, 3, 9), datetime.date(2020, 3, 10), datetime.date(2020, 3, 11), datetime.date(2020, 3, 12), datetime.date(2020, 3, 13), datetime.date(2020, 3, 14), datetime.date(2020, 3, 15), datetime.date(2020, 3, 16), datetime.date(2020, 3, 17), datetime.date(2020, 3, 18), datetime.date(2020, 3, 19), datetime.date(2020, 3, 20), datetime.date(2020, 3, 21), datetime.date(2020, 3, 22), datetime.date(2020, 3, 23), datetime.date(2020, 3, 24), datetime.date(2020, 3, 25), datetime.date(2020, 3, 26), datetime.date(2020, 3, 27), datetime.date(2020, 3, 28), datetime.date(2020, 3, 29), datetime.date(2020, 3, 30), datetime.date(2020, 3, 31), datetime.date(2020, 4, 1), datetime.date(2020, 4, 2), datetime.date(2020, 4, 3), datetime.date(2020, 4, 4), datetime.date(2020, 4, 5), datetime.date(2020, 4, 6), datetime.date(2020, 4, 7), datetime.date(2020, 4, 8), datetime.date(2020, 4, 9), datetime.date(2020, 4, 10), datetime.date(2020, 4, 11), datetime.date(2020, 4, 12), datetime.date(2020, 4, 13), datetime.date(2020, 4, 14), datetime.date(2020, 4, 15), datetime.date(2020, 4, 16), datetime.date(2020, 4, 17), datetime.date(2020, 4, 18), datetime.date(2020, 4, 19), datetime.date(2020, 4, 20), datetime.date(2020, 4, 21), datetime.date(2020, 4, 22), datetime.date(2020, 4, 23), datetime.date(2020, 4, 24), datetime.date(2020, 4, 25), datetime.date(2020, 4, 26), datetime.date(2020, 4, 27), datetime.date(2020, 4, 28), datetime.date(2020, 4, 29), datetime.date(2020, 4, 30), datetime.date(2020, 5, 1), datetime.date(2020, 5, 2), datetime.date(2020, 5, 3), datetime.date(2020, 5, 4), datetime.date(2020, 5, 5), datetime.date(2020, 5, 6), datetime.date(2020, 5, 7), datetime.date(2020, 5, 8), datetime.date(2020, 5, 9), datetime.date(2020, 5, 10), datetime.date(2020, 5, 11), datetime.date(2020, 5, 12), datetime.date(2020, 5, 13), datetime.date(2020, 5, 14), datetime.date(2020, 5, 15), datetime.date(2020, 5, 16), datetime.date(2020, 5, 17), datetime.date(2020, 5, 18), datetime.date(2020, 5, 19), datetime.date(2020, 5, 20), datetime.date(2020, 5, 21), datetime.date(2020, 5, 22), datetime.date(2020, 5, 23), datetime.date(2020, 5, 24), datetime.date(2020, 5, 25), datetime.date(2020, 5, 26), datetime.date(2020, 5, 27), datetime.date(2020, 5, 28), datetime.date(2020, 5, 29), datetime.date(2020, 5, 30), datetime.date(2020, 5, 31), datetime.date(2020, 6, 1), datetime.date(2020, 6, 2), datetime.date(2020, 6, 3), datetime.date(2020, 6, 4), datetime.date(2020, 6, 5), datetime.date(2020, 6, 6), datetime.date(2020, 6, 7), datetime.date(2020, 6, 8), datetime.date(2020, 6, 9), datetime.date(2020, 6, 10), datetime.date(2020, 6, 11), datetime.date(2020, 6, 12), datetime.date(2020, 6, 13), datetime.date(2020, 6, 14), datetime.date(2020, 6, 15), datetime.date(2020, 6, 16), datetime.date(2020, 6, 17), datetime.date(2020, 6, 18), datetime.date(2020, 6, 19), datetime.date(2020, 6, 20), datetime.date(2020, 6, 21), datetime.date(2020, 6, 22), datetime.date(2020, 6, 23)] . china = china.set_index(&#39;provinceName&#39;) china = china.reset_index() . 分析：循环遍历省份和日期获得每个省份每日累计确诊，因为需要拼接，先初始化一个date_confirmed . list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_confirmedCount&#39;])#该日每省的累计确诊人数 except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_confirmed = pd.DataFrame(list_d,index=list_e) date_confirmed.index.name=&quot;date&quot; date_confirmed.columns=[&quot;China_confirmedCount&quot;] date_confirmed . China_confirmedCount . date . 2020-01-24 1956.0 | . 分析：遍历每个省份拼接每日的总确诊人数的dataframe . l = 0 for i in date[3:]: list_p = [] list_d = [] list_e = [] l +=1 for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[l]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_confirmedCount&#39;])#该日每省的累计确诊人数 except: continue #con_0 = china.loc[china[&#39;updateTime&#39;] == date[0]].loc[china[&#39;provinceName&#39;] == &#39;河北省&#39;].loc[[0]].iloc[0] #list_p.append(con_0[&#39;province_confirmedCount&#39;])#该日每省的累计确诊人数 list_d.append(sum(list_p)) list_e.append(str(date[l])) confirmed = pd.DataFrame(list_d, index=list_e) confirmed.index.name=&quot;date&quot; confirmed.columns=[&quot;China_confirmedCount&quot;] date_confirmed = pd.concat([date_confirmed,confirmed],sort=False) date_confirmed . China_confirmedCount . date . 2020-01-24 1956.0 | . 2020-01-25 2253.0 | . 2020-01-26 1956.0 | . 2020-01-27 2825.0 | . 2020-01-28 4589.0 | . ... ... | . 2020-06-17 8106.0 | . 2020-06-18 6862.0 | . 2020-06-19 6894.0 | . 2020-06-20 6921.0 | . 2020-06-21 6157.0 | . 150 rows × 1 columns . 分析：去除空值和不全的值 . date_confirmed.dropna(subset=[&#39;China_confirmedCount&#39;],inplace=True) date_confirmed.tail(20) . China_confirmedCount . date . 2020-06-02 78782.0 | . 2020-06-03 78780.0 | . 2020-06-04 76903.0 | . 2020-06-05 76908.0 | . 2020-06-06 8777.0 | . 2020-06-07 8782.0 | . 2020-06-08 8628.0 | . 2020-06-09 8634.0 | . 2020-06-10 8638.0 | . 2020-06-11 8649.0 | . 2020-06-12 8658.0 | . 2020-06-13 8665.0 | . 2020-06-14 8733.0 | . 2020-06-15 8772.0 | . 2020-06-16 8055.0 | . 2020-06-17 8106.0 | . 2020-06-18 6862.0 | . 2020-06-19 6894.0 | . 2020-06-20 6921.0 | . 2020-06-21 6157.0 | . 分析：数据从4月末开始到5月末就因为缺失过多省份的数据(部分省份从4月末至今再也没有新增病患)而失真，自2020-06-06起完全失去真实性，所以我删除了2020-06-06往后的数据 . date_confirmed = date_confirmed.drop([&#39;2020-06-06&#39;,&#39;2020-06-07&#39;,&#39;2020-06-08&#39;,&#39;2020-06-09&#39;,&#39;2020-06-10&#39;,&#39;2020-06-11&#39;,&#39;2020-06-12&#39;,&#39;2020-06-13&#39;,&#39;2020-06-14&#39;, &#39;2020-06-15&#39;,&#39;2020-06-16&#39;,&#39;2020-06-19&#39;,&#39;2020-06-18&#39;,&#39;2020-06-20&#39;,&#39;2020-06-17&#39;,&#39;2020-06-21&#39;]) . 分析：构造拼接函数 . def data_frame(self,china,element): l = 0 for i in date[3:]: list_p = [] list_d = [] list_e = [] l +=1 for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[l]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[element]) except: continue #con_0 = china.loc[china[&#39;updateTime&#39;] == date[0]].loc[china[&#39;provinceName&#39;] == &#39;河北省&#39;].loc[[0]].iloc[0] #list_p.append(con_0[&#39;province_confirmedCount&#39;]) list_d.append(sum(list_p)) list_e.append(str(date[l])) link = pd.DataFrame(list_d, index=list_e) link.index.name=&quot;date&quot; link.columns=[&quot;China&quot;] self = pd.concat([self,link],sort=False) self.dropna(subset=[&#39;China&#39;],inplace=True) self = self.drop([&#39;2020-06-06&#39;,&#39;2020-06-07&#39;,&#39;2020-06-08&#39;,&#39;2020-06-09&#39;,&#39;2020-06-10&#39;,&#39;2020-06-11&#39;,&#39;2020-06-12&#39;,&#39;2020-06-13&#39;,&#39;2020-06-14&#39;, &#39;2020-06-15&#39;,&#39;2020-06-16&#39;,&#39;2020-06-19&#39;,&#39;2020-06-18&#39;,&#39;2020-06-20&#39;,&#39;2020-06-17&#39;,&#39;2020-06-21&#39;]) return self . 分析：初始化各个变量 . list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_curedCount&#39;]) except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_cured = pd.DataFrame(list_d, index=list_e) date_cured.index.name=&quot;date&quot; date_cured.columns=[&quot;China&quot;] #累计死亡人数 date_dead list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_deadCount&#39;]) except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_dead = pd.DataFrame(list_d, index=list_e) date_dead.index.name=&quot;date&quot; date_dead.columns=[&quot;China&quot;] . plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] #更改字体,否则无法显示汉字 fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = date_confirmed.index y = date_confirmed.values ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) ax.set_title(&#39;累计确诊患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.set_xticks( range(0,len(x),30)) . [&lt;matplotlib.axis.XTick at 0x255520e4908&gt;, &lt;matplotlib.axis.XTick at 0x255520e49e8&gt;, &lt;matplotlib.axis.XTick at 0x255520af048&gt;, &lt;matplotlib.axis.XTick at 0x2555216b0b8&gt;, &lt;matplotlib.axis.XTick at 0x2555216b4e0&gt;] . date_cured = data_frame(date_cured,china,&#39;province_curedCount&#39;) fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = date_cured.index y = date_cured.values ax.set_title(&#39;累计治愈患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) ax.set_xticks( range(0,len(x),30)) . [&lt;matplotlib.axis.XTick at 0x25550ef60f0&gt;, &lt;matplotlib.axis.XTick at 0x255521cd0b8&gt;, &lt;matplotlib.axis.XTick at 0x255521b7780&gt;, &lt;matplotlib.axis.XTick at 0x2555208ffd0&gt;, &lt;matplotlib.axis.XTick at 0x2555208f0f0&gt;] . 分析：累计疑似无法通过补全数据得到 . date_dead = data_frame(date_dead,china,&#39;province_deadCount&#39;) fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = date_dead.index y = date_dead.values ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) x_major_locator=MultipleLocator(12) ax=plt.gca() ax.set_title(&#39;累计死亡患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.xaxis.set_major_locator(x_major_locator) ax.set_xticks( range(0,len(x),30)) . [&lt;matplotlib.axis.XTick at 0x255521fda90&gt;, &lt;matplotlib.axis.XTick at 0x255521fda58&gt;, &lt;matplotlib.axis.XTick at 0x25552a51550&gt;, &lt;matplotlib.axis.XTick at 0x25552a75470&gt;, &lt;matplotlib.axis.XTick at 0x25552a75908&gt;] . 分析：疫情自1月初开始爆发，到2月末开始减缓增速，到4月末趋于平缓。治愈人数自2月初开始大幅增加，到3月末趋于平缓，死亡人数自1月末开始增加，到2月末趋于平缓，到4月末因为统计因素死亡人数飙升后趋于平缓。 分析总结：确诊人数数据和治愈数据从4月末开始到5月末就因为缺失过多省份的数据(部分省份至今再也没有新增病患)导致失真，其他数据尽量通过补全,越靠近尾部数据越失真。死亡数据补全较为成功，几乎没有错漏。 . &#65288;&#20108;&#65289;&#20840;&#22269;&#26032;&#22686;&#30830;&#35786;/&#30097;&#20284;/&#27835;&#24840;/&#27515;&#20129;&#24773;&#20917;&#38543;&#26102;&#38388;&#21464;&#21270;&#36235;&#21183;&#22914;&#20309;&#65311; . 分析：新增确诊/治愈/死亡的数据需要对china进行运算，每省每日进行diff差值运算 . 分析：首先初始化各个数据，然后仿照上面的拼接函数，作适用于该题的拼接函数 . list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_confirmedCount&#39;])#该日每省的累计确诊人数 except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_new_confirmed = pd.DataFrame(list_d,index=list_e) date_new_confirmed.index.name=&quot;date&quot; date_new_confirmed.columns=[&quot;China&quot;] date_new_confirmed #新增治愈人数 date_new_curedCount list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_curedCount&#39;]) except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_new_cured = pd.DataFrame(list_d, index=list_e) date_new_cured.index.name=&quot;date&quot; date_new_cured.columns=[&quot;China&quot;] #新增死亡人数 date_new_dead list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[2]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;province_deadCount&#39;]) except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_new_dead = pd.DataFrame(list_d, index=list_e) date_new_dead.index.name=&quot;date&quot; date_new_dead.columns=[&quot;China&quot;] . 分析：构造拼接函数 . def data_new_frame(self,china,element): l = 0 for i in date[3:]: list_p = [] list_d = [] list_e = [] l +=1 for p in range(0,32): try: con_0 = china.loc[china[&#39;updateTime&#39;] == date[l]].loc[china[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[element]) except: continue #con_0 = china.loc[china[&#39;updateTime&#39;] == date[0]].loc[china[&#39;provinceName&#39;] == &#39;河北省&#39;].loc[[0]].iloc[0] #list_p.append(con_0[&#39;province_confirmedCount&#39;]) list_d.append(sum(list_p)) list_e.append(str(date[l])) link = pd.DataFrame(list_d, index=list_e) link.index.name=&quot;date&quot; link.columns=[&quot;China&quot;] self = pd.concat([self,link],sort=False) self.dropna(subset=[&#39;China&#39;],inplace=True) return self . 分析：数据补全以及去除含缺失省份的数据 . d = data_new_frame(date_new_confirmed,china,&#39;province_confirmedCount&#39;) for i in range(len(d)): dr = [] for a,b in zip(range(0,len(d)-1),range(1,len(d)-2)): if d.iloc[b].iloc[0] &lt; d.iloc[a].iloc[0]: dr.append(d.iloc[b].iloc[0]) d = d[~d[&#39;China&#39;].isin(dr)] . 分析：做差值运算 . d[&#39;China&#39;] = d[&#39;China&#39;].diff() . 分析：去除两个含缺失省份的日期 . d.drop([&#39;2020-06-20&#39;,&#39;2020-06-21&#39;],inplace=True) . 分析：作折线图表现时间趋势 . fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = d.index y = d.values ax.set_title(&#39;新增确诊患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) ax.set_xticks( range(0,len(x),10)) . [&lt;matplotlib.axis.XTick at 0x25552a9c898&gt;, &lt;matplotlib.axis.XTick at 0x25552a9c860&gt;, &lt;matplotlib.axis.XTick at 0x25552ab7550&gt;, &lt;matplotlib.axis.XTick at 0x25552ad50f0&gt;, &lt;matplotlib.axis.XTick at 0x25552ad5518&gt;, &lt;matplotlib.axis.XTick at 0x25552ad59b0&gt;, &lt;matplotlib.axis.XTick at 0x25552ad5e48&gt;, &lt;matplotlib.axis.XTick at 0x25552adc320&gt;] . 分析：使用初始化数据构造date_new_cured的dataframe，然后作折线图表现时间趋势 . cu = data_new_frame(date_new_cured,china,&#39;province_curedCount&#39;) for i in range(len(cu)): dr = [] for a,b in zip(range(0,len(cu)-1),range(1,len(cu)-2)): if cu.iloc[b].iloc[0] &lt; cu.iloc[a].iloc[0]: dr.append(cu.iloc[b].iloc[0]) cu = cu[~cu[&#39;China&#39;].isin(dr)] cu[&#39;China&#39;] = cu[&#39;China&#39;].diff() cu.drop([&#39;2020-06-20&#39;,&#39;2020-06-21&#39;],inplace=True) #新增治愈患者 date_new_cured fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = cu.index y = cu.values ax.set_title(&#39;新增治愈患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) ax.set_xticks( range(0,len(x),10)) . [&lt;matplotlib.axis.XTick at 0x25552b13b00&gt;, &lt;matplotlib.axis.XTick at 0x25552b13ac8&gt;, &lt;matplotlib.axis.XTick at 0x25552b137b8&gt;, &lt;matplotlib.axis.XTick at 0x25552b3f470&gt;, &lt;matplotlib.axis.XTick at 0x25552b3f908&gt;, &lt;matplotlib.axis.XTick at 0x25552b3fda0&gt;, &lt;matplotlib.axis.XTick at 0x25552b47278&gt;] . 分析：使用初始化数据构造date_new_dead的dataframe，然后作折线图表现时间趋势 . de = data_new_frame( date_new_dead,china,&#39;province_deadCount&#39;) for i in range(len(de)): dr = [] for a,b in zip(range(0,len(de)-1),range(1,len(de)-2)): if de.iloc[b].iloc[0] &lt; de.iloc[a].iloc[0]: dr.append(de.iloc[b].iloc[0]) de = de[~de[&#39;China&#39;].isin(dr)] de[&#39;China&#39;] = de[&#39;China&#39;].diff() de.drop([&#39;2020-06-21&#39;],inplace=True) #新增死亡患者 date_new_dead fig = plt.figure( figsize=(16,6), dpi=100) ax = fig.add_subplot(1,1,1) x = de.index y = de.values ax.set_title(&#39;新增死亡患者&#39;,fontdict={ &#39;color&#39;:&#39;black&#39;, &#39;size&#39;:24 }) ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39; ) ax.set_xticks( range(0,len(x),10)) . [&lt;matplotlib.axis.XTick at 0x25553bdfd30&gt;, &lt;matplotlib.axis.XTick at 0x25553bdfcf8&gt;, &lt;matplotlib.axis.XTick at 0x25553c01f60&gt;, &lt;matplotlib.axis.XTick at 0x25553c146a0&gt;, &lt;matplotlib.axis.XTick at 0x25553c14b38&gt;, &lt;matplotlib.axis.XTick at 0x25553c14d68&gt;, &lt;matplotlib.axis.XTick at 0x25553c1b4a8&gt;, &lt;matplotlib.axis.XTick at 0x25553c1b940&gt;, &lt;matplotlib.axis.XTick at 0x25553c1bdd8&gt;] . 分析：新增患者自1月末开始增加，到2月14日前后到达顶点，后增数下降，趋于平缓。 分析：新增治愈患者自1月末开始增加，到3月02日前后达到顶峰，后增数下降，从4月初开始趋于平缓。 分析：新增死亡患者自1月末开始增加，到2月达到高峰，自3月初开始增数平缓，到4月17日前后因为统计因素飙升后回落。 . &#65288;&#19977;&#65289;&#20840;&#22269;&#26032;&#22686;&#22659;&#22806;&#36755;&#20837;&#38543;&#26102;&#38388;&#21464;&#21270;&#36235;&#21183;&#22914;&#20309;&#65311; . 分析：新增境外输入数据需要对CHINA进行运算，逐日相减。 . 分析：先从CHINA取出境外输入的数据，然后补全时间序列并作差。 . imported = CHINA.loc[CHINA[&#39;cityName&#39;] == &#39;境外输入&#39;] imported.updateTime = pd.to_datetime(imported.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date imported . D: Anaconda envs python32 lib site-packages pandas core generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 136 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 317 | 1.0 | 307 | 3 | 2020-06-23 | 境外输入 | NaN | 0.0 | 72.0 | 0.0 | 65.0 | 0.0 | . 150 亚洲 | Asia | 中国 | China | 江苏省 | Jiangsu | 320000 | 654 | 3.0 | 653 | 0 | 2020-06-23 | 境外输入 | NaN | 0.0 | 23.0 | 0.0 | 22.0 | 0.0 | . 201 亚洲 | Asia | 中国 | China | 北京市 | Beijing | 110000 | 843 | 164.0 | 584 | 9 | 2020-06-23 | 境外输入 | NaN | 0.0 | 174.0 | 3.0 | 173.0 | 0.0 | . 214 亚洲 | Asia | 中国 | China | 河北省 | Hebei | 130000 | 346 | 0.0 | 323 | 6 | 2020-06-23 | 境外输入 | NaN | 0.0 | 10.0 | 0.0 | 10.0 | 0.0 | . 218 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000 | 198 | 48.0 | 192 | 3 | 2020-06-23 | 境外输入 | NaN | 0.0 | 61.0 | 0.0 | 59.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 115420 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000 | 250 | 1.0 | 240 | 3 | 2020-03-25 | 境外输入 | NaN | 0.0 | 5.0 | 1.0 | 0.0 | 0.0 | . 115956 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000 | 145 | 0.0 | 133 | 3 | 2020-03-24 | 境外输入 | NaN | 0.0 | 9.0 | 0.0 | 0.0 | 0.0 | . 116164 亚洲 | Asia | 中国 | China | 甘肃省 | Gansu | 620000 | 136 | 0.0 | 119 | 2 | 2020-03-24 | 境外输入 | NaN | 0.0 | 45.0 | 0.0 | 30.0 | 0.0 | . 117171 亚洲 | Asia | 中国 | China | 上海市 | Shanghai | 310000 | 414 | 0.0 | 330 | 4 | 2020-03-24 | 境外输入 | NaN | 0.0 | 75.0 | 0.0 | 3.0 | 0.0 | . 117597 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000 | 142 | 0.0 | 133 | 3 | 2020-03-24 | 境外输入 | NaN | 0.0 | 6.0 | 0.0 | 0.0 | 0.0 | . 607 rows × 19 columns . 分析：补全省份缺失时间的数据 . for i in range(0,len(province)): list_j_d = [] date_b = [] for dt in imported.loc[imported[&#39;provinceName&#39;] == province[i]][&#39;updateTime&#39;]: date_b.append(str(dt)) list_j_d = list(set(date_b)) list_j_d.sort() #imported.loc[imported[&#39;provinceName&#39;] == province[3]] try: start = imported.loc[imported[&#39;provinceName&#39;] == province[i]][&#39;updateTime&#39;].min() end = imported.loc[imported[&#39;provinceName&#39;] == province[i]][&#39;updateTime&#39;].max() dates_b = pd.date_range(start=str(start), end=str(end)) aid_frame_b = pd.DataFrame({&#39;updateTime&#39;: dates_b,&#39;provinceName&#39;:[province[i]]*len(dates_b)}) aid_frame_b.updateTime = pd.to_datetime(aid_frame_b.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date #draft = pd.merge(china.loc[china[&#39;provinceName&#39;] == province[1]], aid_frame, on=&#39;updateTime&#39;, how=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) draft_b = pd.concat([imported.loc[imported[&#39;provinceName&#39;] == province[i]], aid_frame_b], join=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) draft_b.city_confirmedCount.fillna(method=&quot;ffill&quot;,inplace=True) draft_b.city_suspectedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft_b.city_curedCount.fillna(method=&quot;ffill&quot;, inplace=True) draft_b.city_deadCount.fillna(method=&quot;ffill&quot;, inplace=True) draft_b.loc[draft_b[&#39;provinceName&#39;] == province[i]].fillna(0,inplace=True,limit = 1) draft_b.loc[draft_b[&#39;provinceName&#39;] == province[i]].loc[:,&#39;city_confirmedCount&#39;:&#39;city_deadCount&#39;] = draft_b.loc[draft_b[&#39;provinceName&#39;] == province[i]].loc[:,&#39;city_confirmedCount&#39;:&#39;city_deadCount&#39;].diff() draft_b.dropna(subset=[&#39;city_confirmedCount&#39;,&#39;city_suspectedCount&#39;,&#39;city_curedCount&#39;,&#39;city_deadCount&#39;],inplace=True) imported = pd.concat([imported,draft_b], join=&#39;outer&#39;).sort_values(&#39;updateTime&#39;) except: continue imported . continentName continentEnglishName countryName countryEnglishName provinceName provinceEnglishName province_zipCode province_confirmedCount province_suspectedCount province_curedCount province_deadCount updateTime cityName cityEnglishName city_zipCode city_confirmedCount city_suspectedCount city_curedCount city_deadCount . 115956 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 145.0 | 0.0 | 133.0 | 3.0 | 2020-03-24 | 境外输入 | NaN | 0.0 | 9.0 | 0.0 | 0.0 | 0.0 | . 0 NaN | NaN | NaN | NaN | 甘肃省 | NaN | NaN | NaN | NaN | NaN | NaN | 2020-03-24 | NaN | NaN | NaN | 45.0 | 0.0 | 30.0 | 0.0 | . 117597 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 142.0 | 0.0 | 133.0 | 3.0 | 2020-03-24 | 境外输入 | NaN | 0.0 | 6.0 | 0.0 | 0.0 | 0.0 | . 117597 亚洲 | Asia | 中国 | China | 天津市 | Tianjin | 120000.0 | 142.0 | 0.0 | 133.0 | 3.0 | 2020-03-24 | 境外输入 | NaN | 0.0 | 6.0 | 0.0 | 0.0 | 0.0 | . 116164 亚洲 | Asia | 中国 | China | 甘肃省 | Gansu | 620000.0 | 136.0 | 0.0 | 119.0 | 2.0 | 2020-03-24 | 境外输入 | NaN | 0.0 | 45.0 | 0.0 | 30.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 150 亚洲 | Asia | 中国 | China | 江苏省 | Jiangsu | 320000.0 | 654.0 | 3.0 | 653.0 | 0.0 | 2020-06-23 | 境外输入 | NaN | 0.0 | 23.0 | 0.0 | 22.0 | 0.0 | . 136 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000.0 | 317.0 | 1.0 | 307.0 | 3.0 | 2020-06-23 | 境外输入 | NaN | 0.0 | 72.0 | 0.0 | 65.0 | 0.0 | . 91 NaN | NaN | NaN | NaN | 天津市 | NaN | NaN | NaN | NaN | NaN | NaN | 2020-06-23 | NaN | NaN | NaN | 61.0 | 0.0 | 59.0 | 0.0 | . 136 亚洲 | Asia | 中国 | China | 陕西省 | Shaanxi | 610000.0 | 317.0 | 1.0 | 307.0 | 3.0 | 2020-06-23 | 境外输入 | NaN | 0.0 | 72.0 | 0.0 | 65.0 | 0.0 | . 201 亚洲 | Asia | 中国 | China | 北京市 | Beijing | 110000.0 | 843.0 | 164.0 | 584.0 | 9.0 | 2020-06-23 | 境外输入 | NaN | 0.0 | 174.0 | 3.0 | 173.0 | 0.0 | . 2524 rows × 19 columns . 分析：作copy()防止数据处理失误使得原数据丢失 . draft_i = imported.copy() . 分析：初始化一个省份数据，保证这个方法可行 . real_s = imported.loc[imported[&#39;provinceName&#39;] == province[0]] real_s.drop_duplicates(subset=&#39;updateTime&#39;, keep=&#39;first&#39;, inplace=True) draft_i = real_s for p in province: real_data = imported.loc[imported[&#39;provinceName&#39;] == p] real_data.drop_duplicates(subset=&#39;updateTime&#39;, keep=&#39;first&#39;, inplace=True) #imported = pd.concat([real_data, china],sort=False) draft_i = pd.concat([real_data,draft_i],sort=False) . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . 分析：确认方法无误，对余下省份进行相同的处理 . imported = draft_i . imported = imported.set_index(&#39;provinceName&#39;) imported = imported.reset_index() . 分析：进行各个省份的数据合并。 . list_p = [] list_d = [] list_e = [] for p in range(0,32): try: con_0 = imported.loc[imported[&#39;updateTime&#39;] == date[2]].loc[imported[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;city_confirmedCount&#39;])#该日每省的累计确诊人数 except: continue list_d.append(sum(list_p)) list_e.append(str(date[0])) date_new_foreign_confirmed = pd.DataFrame(list_d,index=list_e) date_new_foreign_confirmed.index.name=&quot;date&quot; date_new_foreign_confirmed.columns=[&quot;imported_confirmedCount&quot;] date_new_foreign_confirmed . imported_confirmedCount . date . 2020-01-24 0 | . l = 0 for i in date[3:]: list_p = [] list_d = [] list_e = [] l +=1 for p in range(0,32): try: con_0 = imported.loc[imported[&#39;updateTime&#39;] == date[l]].loc[imported[&#39;provinceName&#39;] == province[p]].iloc[[0]].iloc[0] list_p.append(con_0[&#39;city_confirmedCount&#39;])#该日每省的累计确诊人数 except: continue #con_0 = imported.loc[imported[&#39;updateTime&#39;] == date[0]].loc[imported[&#39;provinceName&#39;] == &#39;河北省&#39;].loc[[0]].iloc[0] #list_p.append(con_0[&#39;city_confirmedCount&#39;])#该日每省的累计确诊人数 list_d.append(sum(list_p)) list_e.append(str(date[l])) confirmed = pd.DataFrame(list_d, index=list_e) confirmed.index.name=&quot;date&quot; confirmed.columns=[&quot;imported_confirmedCount&quot;] date_new_foreign_confirmed = pd.concat([date_new_foreign_confirmed,confirmed],sort=False) date_new_foreign_confirmed . imported_confirmedCount . date . 2020-01-24 0.0 | . 2020-01-25 0.0 | . 2020-01-26 0.0 | . 2020-01-27 0.0 | . 2020-01-28 0.0 | . ... ... | . 2020-06-17 848.0 | . 2020-06-18 800.0 | . 2020-06-19 800.0 | . 2020-06-20 802.0 | . 2020-06-21 775.0 | . 150 rows × 1 columns . fig = plt.figure( figsize=(16,4), dpi=100) ax = fig.add_subplot(1,1,1) x = date_new_foreign_confirmed.index y = date_new_foreign_confirmed.values plot = ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39;,label=&#39;date_new_foreign_confirmed&#39; ) ax.set_xticks( range(0,len(x),10)) plt.xlabel(&#39;日期&#39;,fontsize=20) plt.ylabel(&#39;人数&#39;,fontsize=20) plt.title(&#39;COVID-19——新增境外输入&#39;,fontsize=30) ax.legend( loc=0, frameon=True ) . &lt;matplotlib.legend.Legend at 0x25553ca5f28&gt; . 分析总结：境外输入病例自3月末开始激增，到5月初增速趋于平缓，到6月初开始增速减缓。 . &#65288;&#22235;&#65289;&#20320;&#25152;&#22312;&#30340;&#30465;&#24066;&#24773;&#20917;&#22914;&#20309;&#65311; . 分析：首先取出广东省的所有时间序列,转换成string类型,然后进行排序 . m_dates = list(set(myhome[&#39;updateTime&#39;])) aid_d = m_dates.copy() for d in aid_d: a = str(d) m_dates.remove(d) m_dates.append(a) m_dates.sort() . myhome = myhome.set_index(&#39;provinceName&#39;) myhome = myhome.reset_index() . 分析：遍历我的城市对应的省份的时间构建对应的dataframe . list_g = [] for i in range(0,len(m_dates)): try: con_m = myhome.loc[myhome[&#39;updateTime&#39;] == date[i]].loc[myhome[&#39;cityName&#39;] == &#39;茂名&#39;].iloc[[0]].iloc[0] list_g.append(con_m[&#39;province_confirmedCount&#39;]) except: list_g.append(0) continue g_date_confirmed = pd.DataFrame(list_g, index=m_dates) g_date_confirmed.index.name=&quot;date&quot; g_date_confirmed.columns=[&quot;g_confirmed&quot;] g_date_confirmed=g_date_confirmed[~g_date_confirmed[&#39;g_confirmed&#39;].isin([0])] #广东省累计治愈人数 list_g = [] for i in range(0,len(m_dates)): try: con_m = myhome.loc[myhome[&#39;updateTime&#39;] == date[i]].loc[myhome[&#39;cityName&#39;] == &#39;茂名&#39;].iloc[[0]].iloc[0] list_g.append(con_m[&#39;province_curedCount&#39;]) except: list_g.append(0) continue g_date_cured = pd.DataFrame(list_g, index=m_dates) g_date_cured.index.name=&quot;date&quot; g_date_cured.columns=[&quot;g_cured&quot;] g_date_cured=g_date_cured[~g_date_cured[&#39;g_cured&#39;].isin([0])] #广东省累计死亡人数 list_g = [] for i in range(0,len(m_dates)): try: con_m = myhome.loc[myhome[&#39;updateTime&#39;] == date[i]].loc[myhome[&#39;cityName&#39;] == &#39;茂名&#39;].iloc[[0]].iloc[0] list_g.append(con_m[&#39;province_deadCount&#39;]) except: list_g.append(0) continue g_date_dead = pd.DataFrame(list_g, index=m_dates) g_date_dead.index.name=&quot;date&quot; g_date_dead.columns=[&quot;g_dead&quot;] g_date_dead=g_date_dead[~g_date_dead[&#39;g_dead&#39;].isin([0])] . 分析：作折线图表现疫情随时间变化趋势 . plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] x= g_date_confirmed.index y1 = g_date_confirmed.values y2 = g_date_cured.values y3 = g_date_dead #font_manager = font_manager.FontProperties(fname = &#39;C:/Windows/Fonts/simsun.ttc&#39;,size = 18) plt.figure(figsize=(20,10),dpi = 80) plt.plot(x,y1,color = r_hex,label = &#39;confirmed&#39;) plt.plot(x,y2,color = g_hex,label = &#39;cured&#39;) x_major_locator=MultipleLocator(12) ax=plt.gca() ax.xaxis.set_major_locator(x_major_locator) plt.title(&#39;COVID-19 —— 广东省&#39;,fontsize=30) plt.xlabel(&#39;日期&#39;,fontsize=20) plt.ylabel(&#39;人数&#39;,fontsize=20) plt.legend(loc=1, bbox_to_anchor=(1.00,0.90), bbox_transform=ax.transAxes) . &lt;matplotlib.legend.Legend at 0x25553d02a90&gt; . plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] fig = plt.figure( figsize=(16,4), dpi=100) ax = fig.add_subplot(1,1,1) x = g_date_dead.index y = g_date_dead.values plot = ax.plot( x, y, color=dt_hex, linewidth=2, linestyle=&#39;-&#39;,label=&#39;dead&#39; ) ax.set_xticks( range(0,len(x),10)) plt.xlabel(&#39;日期&#39;,fontsize=20) plt.ylabel(&#39;人数&#39;,fontsize=20) plt.title(&#39;COVID-19——广东省&#39;,fontsize=30) ax.legend( loc=0, frameon=True ) . &lt;matplotlib.legend.Legend at 0x25553d94940&gt; . 分析：广东省的数据补全很成功，真实性高。 分析：从折线图来看，广东省自1月末起感染人数激增，直到2月中旬趋于平缓，3月初开始由于检测普及以及统计因素，短期确诊患者人数小幅度增加。广东省自2月初开始治愈人数激增，直到6月初开始因为新增感染人数趋于平缓，所以治愈人数趋于平缓。广东省自3月初开始不再有新增死亡患者。 . &#65288;&#20116;&#65289;&#22269;&#22806;&#30123;&#24773;&#24577;&#21183;&#22914;&#20309;&#65311; . 分析：数据去除空值 . world.dropna(axis=1, how=&#39;any&#39;, inplace=True) #world.set_index(&#39;updateTime&#39;) . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . 分析：创建国家列表country，创建日期列表date_y . country = list(set(world[&#39;provinceName&#39;])) date_y = [] for dt in world.loc[world[&#39;provinceName&#39;] == country[0]][&#39;updateTime&#39;]: date_y.append(str(dt)) date_y = list(set(date_0)) date_y.sort() . 分析：遍历国家列表对world中的updateTime进行处理并去重。 . for c in country: world.loc[world[&#39;provinceName&#39;] == c].sort_values(by = &#39;updateTime&#39;) world.dropna(subset=[&#39;provinceName&#39;],inplace=True) world.updateTime = pd.to_datetime(world.updateTime,format=&quot;%Y-%m-%d&quot;,errors=&#39;coerce&#39;).dt.date . D: Anaconda envs python32 lib site-packages ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This is separate from the ipykernel package so we can avoid doing imports until D: Anaconda envs python32 lib site-packages pandas core generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . 分析：取前15个国家的province_confirmedCount透视构成world_confirmed，并进行数据补全处理 . world_confirmed = world.loc[world[&#39;provinceName&#39;] == world.head(15)[&#39;provinceName&#39;][0]].pivot_table(index=&#39;updateTime&#39;, columns=&#39;provinceName&#39;, values=&#39;province_confirmedCount&#39;,aggfunc=np.mean) for i in world.head(15)[&#39;provinceName&#39;][1:]: draft_c = world.loc[world[&#39;provinceName&#39;] == i].pivot_table(index=&#39;updateTime&#39;, columns=&#39;provinceName&#39;, values=&#39;province_confirmedCount&#39;,aggfunc=np.mean) world_confirmed = pd.merge(world_confirmed,draft_c,on=&#39;updateTime&#39;, how=&#39;outer&#39;,sort=True) world_confirmed.fillna(0,inplace=True,limit = 1) world_confirmed.fillna(method=&quot;ffill&quot;,inplace=True) world_confirmed . provinceName 美国 巴西 英国 俄罗斯 智利 印度 巴基斯坦 秘鲁 西班牙 孟加拉国 法国 沙特阿拉伯 瑞典 南非 厄瓜多尔 . updateTime . 2020-01-27 5.000000e+00 | 0.00 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.00 | 0.00 | 3.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 2020-01-29 0.000000e+00 | 0.00 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.00 | 0.00 | 4.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 2020-01-30 0.000000e+00 | 0.00 | 0.000000 | 0.0 | 0.0 | 1.000000 | 0.000000 | 0.000000 | 0.00 | 0.00 | 5.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 2020-01-31 6.000000e+00 | 0.00 | 2.000000 | 2.0 | 0.0 | 1.000000 | 0.000000 | 0.000000 | 0.00 | 0.00 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 2020-02-01 6.000000e+00 | 0.00 | 2.000000 | 2.0 | 0.0 | 1.000000 | 0.000000 | 0.000000 | 4.00 | 0.00 | 5.500000 | 0.000000 | 1.000000 | 0.0 | 0.000000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2020-06-19 2.184912e+06 | 976906.50 | 300469.000000 | 563084.0 | 225103.0 | 371474.666667 | 162935.600000 | 243518.000000 | 245268.00 | 102292.00 | 158641.000000 | 145991.000000 | 55672.750000 | 83020.5 | 48256.400000 | . 2020-06-20 2.221982e+06 | 1038568.00 | 302138.750000 | 573007.5 | 231393.0 | 390209.333333 | 169464.666667 | 247925.000000 | 245665.75 | 105535.00 | 159452.000000 | 151277.250000 | 56201.500000 | 87715.0 | 49519.666667 | . 2020-06-21 2.253118e+06 | 1068977.25 | 303284.428571 | 579160.0 | 236748.0 | 399451.714286 | 174346.222222 | 251338.000000 | 245938.00 | 109657.75 | 160093.000000 | 154715.714286 | 56360.000000 | 92681.0 | 49731.000000 | . 2020-06-22 2.279603e+06 | 1084312.25 | 304331.000000 | 587720.0 | 243276.6 | 416389.400000 | 179148.750000 | 254336.333333 | 246272.00 | 112306.00 | 160336.428571 | 158177.500000 | 57346.000000 | 96377.8 | 50092.600000 | . 2020-06-23 2.299650e+06 | 1106470.00 | 305289.000000 | 592280.0 | 246963.0 | 425282.000000 | 182562.666667 | 257447.000000 | 246504.00 | 115786.00 | 160750.000000 | 161005.000000 | 59060.666667 | 101590.0 | 50487.666667 | . 144 rows × 15 columns . 分析：作前15个国家的疫情随时间变动表 . fig = plt.figure(figsize=(16,10)) plt.plot(world_confirmed) plt.legend(world_confirmed.columns) plt.title(&#39;前15个国家累计确诊人数&#39;,fontsize=20) plt.xlabel(&#39;日期&#39;,fontsize=20) plt.ylabel(&#39;人数/百万&#39;,fontsize=20); . 分析：国外数据的补全较为成功，有一定的真实性。 分析：国外新冠确诊人数自3月末开始激增，排名前四的国家的疫情没有受到控制的趋势，国外疫情的趋势为确诊人数继续激增。 . &#65288;&#20845;&#65289;&#32467;&#21512;&#20320;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#22312;&#25239;&#20987;&#30123;&#24773;&#26041;&#38754;&#26377;&#20309;&#24314;&#35758;&#65311; . 从国内疫情折线图来看，从4月末开始疫情趋于平缓，相反，国外疫情从4月初开始爆发，至今没有看到平缓的趋势。 从境外输入案例来看，我们需要谨防境外输入病例，遏制国内新冠再次传播，一切都不能放松警惕。 对于个人，我们要避免到人员密集的区域，外出一定要戴好口罩，回家要做全面的消毒。 对于社会，在交通发达区域和人员密集区域，需要普及病毒检测和场所消毒措施，切断病毒的传播途径，维护我国疫情防控的成果。 . &#38468;&#21152;&#20998;&#26512;(&#36873;&#20570;&#65292;&#20294;&#20570;&#30340;&#20986;&#24425;&#20250;&#21152;&#20998;&#21734;) . 附加分析，所使用的库不限，比如可以使用seaborn、pyecharts等库。 . 童鞋们，自由发挥吧！！ .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/2020/07/01/Cov2019Analysis.html",
            "relUrl": "/2020/07/01/Cov2019Analysis.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "爬虫流程及方法14(正则表达式篇)",
            "content": "前言 . re库的实用实例如下 . import requests import re import os a = True while a: #创建一个文件夹，保存所有图片 if not os.path.exists(&#39;./tupianLibs&#39;): os.mkdir(&#39;./tupianLibs&#39;) headers = { &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39; } url = &quot;https://www.pexels.com/&quot; #使用通用爬虫对整张页面进行爬取 page_text = requests.get(url=url, headers=headers).text #使用聚焦爬虫将页面中所有的图片进行解析/提取 #正则.*?表示一切内容 #re.S单行匹配 ex = &#39;&lt;a class=&quot;js-photo-link photo-item__link&quot; style.*? &gt;.*?&lt;img srcset=&quot;(.*?)&quot; class.*?&gt;&lt;/div&gt;&#39; image_src_list = re.findall(ex, page_text, re.S ) for src in image_src_list: src = &#39;https:&#39;+ src #拼接出一个完整的图片url image_data = requests.get(url=src, headers=headers).content #请求到了图片的二进制数据 image_name = src.split(&#39;/&#39;)[-1] #生成图片名称 imgPath = &#39;./tupianlbs/&#39; + image_name #图片最终存储的路径 with open(imgPath, &#39;W&#39;) as fp: fp.write(image_data) print(&#39;下载成功&#39;) a = False . 正则表达式详情 . . . raw string类型区别于原生字符串类型（不包含转义字符） . r&#39;[1-9] d{5}&#39; . . re.search(pattern, string, flags=0) . 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象。 . pattern:正则表达式的字符串或原生字符串表示 string:待匹配字符串 flags:正则表达式使用时的控制标记 . . re.split(pattern, string, maxsplit=0, flags=0) . 将一个字符串按照正则表达式匹配结果进行分割,返回列表类型。 . pattern:正则表达式的字符串或原生字符串表示 string:待匹配字符串 maxsplit:最大分割数，剩余部分作为最后一个元素输出 flags:正则表达式使用时的控制标记 . re.finditer(pattern, string, flags=0) . 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象。 . pattern:正则表达式的字符串或原生字符串表示 string:待匹配字符串 flags:正则表达式使用时的控制标记 . re.sub(pattern, repl, string, count=0, flags=0) . 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串。 . pattern:正则表达式的字符串或原生字符串表示 repl:替换匹配字符串的字符串 string:待匹配字符串 flags:正则表达式使用时的控制标记 . regex = re.compile(pattern, flags=0) . 将正则表达式的字符串形式编译成正则表达式对象 . pattern:正则表达式的字符串或原生字符串表示 flags:正则表达式使用时的控制标记 Match:对象的属性 . . match = re.search(r&#39;PY.*N&#39;,&#39; PYANBNCNDN&#39; ) match.group(0) . Re库默认采用贪婪匹配，即输出匹配最长的子串。 . 输出’PYANBNCNDN’ . match = re.search(r&#39;PY. *?N&#39;, &#39; PYANBNCNDN&#39;) match group(0) . 输出’PYAN ‘ . . import re restr=&quot;&lt;td data-v-80203e10=&quot;&quot;&gt;（ d+）&lt;/td&gt;&quot;#括号表示只取数据（数字） regex=re.compile(restr,re.IGNORECASE) mylist=regex.findall(province) .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/05/03/%E7%88%AC%E8%99%AB14.html",
            "relUrl": "/markdown/2020/05/03/%E7%88%AC%E8%99%AB14.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "爬虫流程及方法13(xpath解析页面)",
            "content": "前言 . xpath解析原理: 1.实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。 2.调用et ree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。 . 环境的安装: . pip install Lxml . 如何实例化一个etree对象: . from Lxml import etree . 1.将本地的html文档中的源码数据加载到etree对象中: . etree.parse(fiLePath) . 2.可以将从互联网上获取的源码数据加裁到该对象中 . etree.HTML( &#39;page_ text&#39; ) . xpath(&#39;xpath表达式&#39;) . #3.7版本后引入etree模块如下，3.5版本以下可以直接从lxml中引入 from lxml import html etree = html.etree parser = etree.HTMLParser(encoding=&quot;utf-8&quot;) #实例化好了一个etree对象，且将被解析的源码加载到了该对象中 tree = etree.parse(&#39;bs4练习.html&#39;, parser=parser) # r = tree.xpath(&#39;/html/body/div&#39;) # r = tree.xpath(&#39;/html//div&#39;) # r = tree.xpath(&#39;//div&#39;) . /表示的是从根节点开始定位，一个/x/表示一个层级，//表示跨越多个层级,可以表示从任意位置开始定位 . r = tree.xpath(&#39;//div[@class=&quot;song&quot;]&#39;) . 属性定位： . //div[@class=&quot;song&quot;] tag[@attrName=&quot;attrValue&quot;] . r = tree.xpath(&#39;//div[@class=&quot;song&quot;]/p[3]&#39;) . 索引定位： . &#39;//div[@class=&quot;song&quot;]/p[3]&#39; #这里索引以1开始 . r = tree.xpath(&#39;//div[@class=&quot;tang&quot;]//li[5]/a/text()&#39;)[0] 取文本： . /text() #获取的是标签中直系的文本内容 . . //text() #获取标签中非直系文本内容（所有文本内容） . python r = tree.xpath(&#39;//div[@class=&quot;tang&quot;]/text()&#39;) . 取属性： . /@attrName ==&gt;img/src . r = tree.xpath(&#39;//div[@class=&quot;song&quot;]/img/@src&#39;) . 实际案例： . #3.7版本后引入etree模块如下，3.5版本以下可以直接从lxml中引入 from lxml import html import requests etree = html.etree a = True while a: #爬取页面源码数据 url = &#39;https://mm.58.com/ershoufang/&#39; headers = { &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39; } page_text = requests.get(url=url, headers=headers).text #数据解析 tree = etree.HTML(page_text) li_list = tree.xpath(&#39;//ul[@class=&quot;house-list-wrap&quot;]/li&#39;) fp = open(&#39;58.txt&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) for li in li_list: #页面数据局部解析 title = li.xpath(&#39;./div[2]/h2/a/text()&#39;)[0]#./表示从前面的li开始（局部开始） print(title) fp.write(title+&#39; n&#39;) a = False .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/05/03/%E7%88%AC%E8%99%AB13.html",
            "relUrl": "/markdown/2020/05/03/%E7%88%AC%E8%99%AB13.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "爬虫流程及方法12(高性能异步爬虫)",
            "content": "前言 . 目的:在爬虫中使用异步实现高性能的数据爬取操作。 异步爬虫的方式: –多线程，多进程(不建议): 好处:可以为相关阻塞的操作单独开启线程或者进程，阻塞操作就可以异步执行。 弊端:无法无限制的开启多线程或者多进程。 . ps:get方法与post方法是阻塞的方法 . –线程池、进程池(适当的使用)： 好处:我们可以降低系统对进程或者线程创建和销毁的一个频率，从而很好的降低系统的开销。 弊端:池中线程或进程的数量是有上限。| . 模拟多线程操作 . 单线程模拟： . import time #使用单线程串行方式执行 def get_page(str): print(&quot;正在下载: &quot;, str) time.sleep(2) print(&#39;下载成功: &#39;, str) name_list = [&#39;xiaozi&#39;, &#39;aa&#39;,&#39;bb&#39;,&#39;cc&#39;] start_time = time.time() for i in range(len(name_list)): get_page(name_list[i]) end_time = time.time() print(&#39;%d second&#39; % (end_time-start_time)) . 结果:8s . 线程池模拟： . import time #导入线程池模块对应的类 from multiprocessing.dummy import Pool #使用线程池方式进行 #导入线程池所对应的pool start_time = time.time()#程序开始时计时 def get_page(str): print(&quot;正在下载: &quot;, str) time.sleep(2) print(&#39;下载成功: &#39;, str) name_list = [&#39;xiaozi&#39;, &#39;aa&#39;,&#39;bb&#39;,&#39;cc&#39;]#可迭代对象 #实例化一个线程对象 pool = Pool(4)#线程池开启4个线程 #将列表中每一个列表元素传递给get_page进行处理 pool.map(get_page, name_list)#若有返回值返回的是列表，因为多次传入到map end_time = time.time()#程序结束时结束计时 print(end_time-start_time) . 结果:2s . 实际案例 . 提取js动态加载内容，使用re正则匹配 . js源码 . var contId=&quot;1671755&quot;,liveStatusUrl=&quot;liveStatus.jsp&quot;,liveSta=&quot;&quot;,playSta=&quot;1&quot;,autoPlay=!1,isLiving=!1,isVrVideo=!1,hdflvUrl=&quot;&quot;,sdflvUrl=&quot;&quot;,hdUrl=&quot;&quot;,sdUrl=&quot;&quot;,ldUrl=&quot;&quot;,srcUrl=&quot;https://video.pearvideo.com/mp4/third/20200429/cont-1671755-11742488-084919-hd.mp4&quot;,vdoUrl=srcUrl,skinRes=&quot;//www.pearvideo.com/domain/skin&quot;,videoCDN=&quot;//video.pearvideo.com&quot; . 正则匹配 . ex = ‘srcUrL=”( .*? )”,vdoUrl’ . 分组操作提取链接 . import requests from bs4 import BeautifulSoup import re from multiprocessing import Pool def get_video_data(dic): headers = { &#39;User-Agent&#39;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&quot; } #使用线程池对视频数据进行请求（较为耗时的堵塞操作） url = dic[&#39;url&#39;] print(dic[&#39;name&#39;], &#39;正在下载&#39;) data = requests.get(url=url, headers=headers, timeout=0.5).content #持久化存储操作 with open(dic[&#39;name&#39;], &#39;wb&#39;) as fp: fp.write(data) print(dic[&#39;name&#39;], &#39;下载成功&#39;) if __name__ == &#39;__main__&#39;: headers = { &#39;User-Agent&#39;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&quot; } url = &#39;https://www.pearvideo.com/category_5&#39; page_text = requests.get(url=url, headers=headers).text soup = BeautifulSoup(page_text, &#39;lxml&#39;) li_urls = soup.select(&#39;.vervideo-bd&#39;) urls = []#存储所有视频的链接和名字 i = 1 for li in li_urls: try: i = i + 1 detail_url = &#39;https://www.pearvideo.com/&#39; + li.a[&#39;href&#39;] name = soup.select(&#39;.vervideo-title&#39;)[i].text+&#39;.mp4&#39; detail_page_text = requests.get(url=detail_url, headers=headers, timeout=0.5).text ex = &#39;srcUrl=&quot;(.*?)&quot;,vdoUrl&#39; video_url = re.findall(ex, detail_page_text)[0] dic = { &#39;name&#39;: name, &#39;url&#39;: video_url } urls.append(dic) except: continue pool = Pool(4) pool.map(get_video_data, urls) pool.close() pool.join() . 总结： . 1.windows环境下需要将主函数放在以下代码下方 . if __name__ == &#39;__main__&#39;: . mac环境下不需要此操作 . 2.下载时下载二进制数据，使用’wb’而不是’w’。 . 3.如果下载视频过多(爬取大量数据)，网站要求验证证书，大量爬取需要使用其他方法应对ssl反爬策略。 . ps：感谢csdn学院提供的案例支持 .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/05/01/%E7%88%AC%E8%99%AB12.html",
            "relUrl": "/markdown/2020/05/01/%E7%88%AC%E8%99%AB12.html",
            "date": " • May 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "爬虫流程及方法11(PyQuery解析网页篇)(全)",
            "content": "前言 . 本篇鸣谢 清华——尹成 的整理收集 . PyQuery文档https://www.osgeo.cn/pyquery/index.html . PyQuery库也是一个非常强大又灵活的网页解析库，如果你有前端开发经验的，都应该接触过jQuery,那么PyQuery就是你非常绝佳的选择，PyQuery 是 Python 仿照 jQuery 的严格实现。语法与 jQuery 几乎完全相同，所以不用再去费心去记一些奇怪的方法了。 官网地址:http://pyquery.readthedocs.io/en/latest/ jQuery参考文档:http://jquery.cuishifeng.cn/ . 初始化 . 初始化的时候一般有三种传入方式：传入字符串，传入url,传入文件 . 字符串初始化 . html = &#39;&#39;&#39; &lt;div&gt; &lt;ul&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) print(doc) print(type(doc)) print(doc(&#39;li&#39;)) . 结果如下： . . 由于PyQuery写起来比较麻烦，所以我们导入的时候都会添加别名： from pyquery import PyQuery as pq 这里我们可以知道上述代码中的doc其实就是一个pyquery对象，我们可以通过doc可以进行元素的选择，其实这里就是一个css选择器，所以CSS选择器的规则都可以用，直接doc(标签名)就可以获取所有的该标签的内容，如果想要获取class 则doc(‘.class_name’),如果是id则doc(‘#id_name’)…. . URL初始化 . from pyquery import PyQuery as pq doc = pq(url=&quot;http://www.baidu.com&quot;,encoding=&#39;utf-8&#39;) print(doc(&#39;head&#39;)) . 文件初始化 . 我们在pq()这里可以传入url参数也可以传入文件参数，当然这里的文件通常是一个html文件。 例如：pq(filename=’index.html’) . 基本的CSS选择器 . html = &#39;&#39;&#39; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) print(doc(&#39;#container .list li&#39;)) . 这里我们需要注意的一个地方是doc(‘#container .list li’)，这里的三者之间的并不是必须要挨着，只要是层级关系就可以,下面是常用的CSS选择器方法： . . 查找元素 . 子元素 children,find 代码例子： . html = &#39;&#39;&#39; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) items = doc(&#39;.list&#39;) print(type(items)) print(items) lis = items.find(&#39;li&#39;) print(type(lis)) print(lis) . 运行结果如下 . . 从结果里我们也可以看出通过pyquery找到结果其实还是一个pyquery对象，可以继续查找，上述中的代码中的items.find(‘li’) 则表示查找ul里的所有的li标签 当然这里通过children可以实现同样的效果,并且通过.children方法得到的结果也是一个pyquery对象 . li = items.children() print(type(li)) print(li) . 同时在children里也可以用CSS选择器 li2 = items.children(‘.active’) print(li2) 父元素 parent,parents方法 通过.parent就可以找到父元素的内容，例子如下： . html = &#39;&#39;&#39; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) items = doc(&#39;.list&#39;) container = items.parent() print(type(container)) print(container) . 通过.parents就可以找到祖先节点的内容，例子如下： . html = &#39;&#39;&#39; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) items = doc(&#39;.list&#39;) parents = items.parents() print(type(parents)) print(parents) . 结果如下： 从结果我们可以看出返回了两部分内容，一个是的父节点的信息，一个是父节点的父节点的信息即祖先节点的信息 . . 同样我们通过.parents查找的时候也可以添加css选择器来进行内容的筛选 . 兄弟元素 . siblings . html = &#39;&#39;&#39; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) li = doc(&#39;.list .item-0.active&#39;) print(li.siblings()) . 代码中doc(‘.list .item-0.active’) 中的.tem-0和.active是紧挨着的，所以表示是并的关系，这样满足条件的就剩下一个了：thired item的那个标签了 这样在通过.siblings就可以获取所有的兄弟标签，当然这里是不包括自己的 同样的在.siblings()里也是可以通过CSS选择器进行筛选 . 遍历 . 单个元素 . html = &#39;&#39;&#39; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) li = doc(&#39;.item-0.active&#39;) print(li) lis = doc(&#39;li&#39;).items() print(type(lis)) for li in lis: print(type(li)) print(li) . 运行结果如下： . . 从结果中我们可以看出通过items()可以得到一个生成器，并且我们通过for循环得到的每个元素依然是一个pyquery对象。 . 获取信息 . 获取属性 . pyquery对象.attr(属性名) pyquery对象.attr.属性名 . html = &#39;&#39;&#39; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) a = doc(&#39;.item-0.active a&#39;) print(a) print(a.attr(&#39;href&#39;)) print(a.attr.href) . . 所以这里我们也可以知道获得属性值的时候可以直接a.attr(属性名)或者a.attr.属性名 . 获取文本 . 在很多时候我们是需要获取被html标签包含的文本信息,通过.text()就可以获取文本信息 . html = &#39;&#39;&#39; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) a = doc(&#39;.item-0.active a&#39;) print(a) print(a.text()) . 结果如下： . . 获取html . 我们通过.html()的方式可以获取当前标签所包含的html信息，例子如下 . html = &#39;&#39;&#39; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) li = doc(&#39;.item-0.active&#39;) print(li) print(li.html()) . 结果如下： . . DOM操作 . addClass、removeClass 熟悉前端操作的话，通过这两个操作可以添加和删除属性 . html = &#39;&#39;&#39; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) li = doc(&#39;.item-0.active&#39;) print(li) li.removeClass(&#39;active&#39;) print(li) li.addClass(&#39;active&#39;) print(li) . attr,css 同样的我们可以通过attr给标签添加和修改属性， 如果之前没有该属性则是添加，如果有则是修改 我们也可以通过css添加一些css属性，这个时候，标签的属性里会多一个style属性 . html = &#39;&#39;&#39; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) li = doc(&#39;.item-0.active&#39;) print(li) li.attr(&#39;name&#39;, &#39;link&#39;) print(li) li.css(&#39;font-size&#39;, &#39;14px&#39;) print(li) . 结果如下 . . remove 有时候我们获取文本信息的时候可能并列的会有一些其他标签干扰，这个时候通过remove就可以将无用的或者干扰的标签直接删除，从而方便操作 . html = &#39;&#39;&#39; &lt;div class=&quot;wrap&quot;&gt; Hello, World &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt; &#39;&#39;&#39; from pyquery import PyQuery as pq doc = pq(html) wrap = doc(&#39;.wrap&#39;) print(wrap.text()) wrap.find(&#39;p&#39;).remove() print(wrap.text()) . 结果如下： . .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/04/30/%E7%88%AC%E8%99%AB11.html",
            "relUrl": "/markdown/2020/04/30/%E7%88%AC%E8%99%AB11.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "爬虫流程及方法10(爬虫伪装专题篇)",
            "content": "原网页链接萌新论坛 . requests 伪装 headers 发送请求 . headers中空着的可能有也可能无，user-agent基本得有 . 在chrome中找到网页的请求头，图片如下 . . headers = { &quot;Accept&quot;: &quot; &quot;, &quot;Accept-Encoding&quot;: &quot; &quot;, &quot;Accept-Language&quot;: &quot; &quot;, &quot;Host&quot;: &quot; &quot;, &#39;user-agent&#39;:&#39;粘贴1处&#39; } #在页面中点击右键选择检查，调出网页自带的抓包工具，打开Network后刷新当前页面抓包找到user-agent的项复制粘贴1 . cookie的三种传参方法 . headers = {&quot;User_Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&quot;, &quot;Cookie&quot; : &quot; &quot;, &quot;Refer&quot; : &quot; 从哪个网页来的(url)&quot; } #三种Cookie请求方式 &#39;&#39;&#39;第一种：cookie放在headers中&#39;&#39;&#39; headers = {&quot;User_Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&quot;, &quot;Cookie&quot; : &quot; &quot; } &#39;&#39;&#39;第二种：cookie字典传给cookies参数&#39;&#39;&#39; &#39;&#39;&#39;第三种 先发送post请求，获取cookie，带上cookie请求登陆之后的页面&#39;&#39;&#39; # 如果没有的就要抓包了Network -&gt; preserve log -&gt; login包 -&gt;requesy seesion = requests.seesion() # 用户名作为键， 真正的密码作为值 模拟登陆 post_data = {&quot;email&quot;:&quot;xxxx&quot;, &quot;password&quot;:&quot;xxxx&quot;} seesion.post(url=url, data=post_data, headers=headers) # 服务器设置在本地的cookie会保存在本地 seesion.get(url) # 会带上之前保存在seesion中的cookie，能够请求成功 . requests 伪装 params 传输数据 . params = { &#39;某名字&#39;:&#39;某值&#39;, &#39;粘贴2&#39;: &#39;黏贴2&#39; } #在页面中点击右键选择检查，调出网页自带的抓包工具，打开Network后刷新当前页面抓包找到Query String Parameters的项复制粘贴2（记得加符号&#39;粘贴2&#39;） . User-Agent：这里面存放浏览器的信息。可以看到上面的参数值，它表示我是通过Windows的Chrome浏览器，访问的这个服务器。如果我们不设置这个参数，用Python程序直接发送GET请求，服务器接受到的User-Agent信息就会是一个包含python字样的User-Agent。如果后台设计者验证这个User-Agent参数是否合法，不让带Python字样的User-Agent访问，这样就起到了反爬虫的作用。这是一个最简单的，最常用的反爬虫手段。 . Referer：这个参数也可以用于反爬虫，它表示这个请求是从哪发出的。可以看到我们通过浏览器访问网站，这个请求是从哪个页面来的(那个页面包含该链接)，这个地址发出的。如果后台设计者，验证这个参数，对于不是从这个地址跳转过来的请求一律禁止访问，这样就也起到了反爬虫的作用。 . ps: authorization：这个参数是基于AAA模型中的身份验证信息允许访问一种资源的行为。在我们用浏览器访问的时候，服务器会为访问者分配这个用户ID。如果后台设计者，验证这个参数，对于没有用户ID的请求一律禁止访问，这样就又起到了反爬虫的作用。 . UserAgent伪装集合详见资源页面 . selenium 模拟使用浏览器伪装 headers . 使用自动化测试工具 selenium 可以模拟使用浏览器访问网站。使用的selenium版本大都支持 Chrome 和 Firefox 浏览器。要使用该库浏览器需要下载对应版本到电脑上。 . 使用 webdriver 访问本身自带浏览器的 headers。 . import selenium import selenium.webdriver import ssl def get_url_text(url): driver = selenium.webdriver.Chrome()#模拟调用谷歌游览器（模拟你电脑有的游览器操作） driver.get(url)#访问链接 pagesource=driver.page_source#抓取网页源代码 #你要执行的预处理写这里 driver.close() return #返回值 . ssl处理(仅针对使用urllib与urllib3) . urllib库爬虫 . import ssl context = ssl._create_unverified_context() #忽略安全 . requests库爬虫 . 忽略ssl验证使得网页访问得以顺利通过 . verify=False 代表不做证书验证 . import requests from requests.packages import urllib3 urllib3.disable_warnings() #关闭警告 respone=requests.get(&#39;https://www.12306.cn&#39;,verify=False) print(respone.status_code) . 不做证书验证的情况，在某些情况下是行不通的的，这需要其他处理方式 . requests 使用 ip 代理发送请求 . 查询自己的ip，网址https://httpbin.org/ip . import requests a = True while a: try: url = &#39;https://httpbin.org/ip&#39; response = requests.get(url) print(response.text) except: print(&quot;爬取失败&quot;) a = False . 使用代理ip与伪装headers的方式相似,只需要传入proxies参数。 比如现在我有一个代理ip:111.164.20.86:8111，使用如下： . import requests a = True while a: url = &#39;https://httpbin.org/ip&#39; proxy = { &#39;http&#39;: &#39;111.164.20.86:8111&#39;, } #或者填入https请求的&#39;https&#39;: &#39; &#39; response = requests.get(url=url, proxies=proxy) print(response.text) #返回值是代理ip地址，更换url即可使用代理ip爬虫 a = False . 支持socks代理,安装输入： . pip install requests[socks] . 实例： . import requests proxies = { &#39;http&#39;: &#39;socks5://user:pass@host:port&#39;, &#39;https&#39;: &#39;socks5://user:pass@host:port&#39; } respone=requests.get(&#39;https://www.12306.cn&#39;, proxies=proxies) print(respone.status_code) . 如果没考虑好用http/https，建议两个都写入字典proxy里面，就不会报错 . 代理免费ip可能会时不时中断，建议淘宝购买(支持验证那种)，当然如果购买阿里云端口也是可以的 . 重复执行报错处理 . import requests from retrying import retry #@retry(stop_max_attempt_number = 10) &#39;&#39;&#39;让被装饰的函数反复执行10次，10次全部报错才会报错， 中间有一次正常就继续往下走&#39;&#39;&#39; url = &quot;http://www.baidu.com&quot; response = requests.get(url,timeout=0.01)#timeout=0.01 代表请求+接收服务端数据的总时间 #如果想明确控制链接与等待接收服务端数据的时间则写 timeout=(1,2) #timeout=(1,2)&gt;1代表链接超时时间 2代表接收数据的超时时间 print(response.content.decode()) headers = {&quot;User_Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&quot;, } .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/04/30/%E7%88%AC%E8%99%AB10.html",
            "relUrl": "/markdown/2020/04/30/%E7%88%AC%E8%99%AB10.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "爬虫流程及方法09(动态加载页面)(ajax请求)(Json实例)",
            "content": "动态加载数据ajax . 首页中对应企业数据通过ajax请求得到 . 详情页url只有id不同其余相同 . id从json中获取，域名与id拼接新url . 详情页的数据也是动态加载出来的 . 详情页的url也是相同的只有id不同 . 爬取的原网站药妆局 . 推荐使用chrome网页抓包工具查看请求类型 首页与详情页都使用post请求传输json数据包，需要json解析 . import requests import json a = True while a: headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&quot; } #批量获取企业详情页对应id url = &quot;http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList&quot; id_list = [] # 存储企业id all_data_list = [] #参数封装 for page in range(1,6): data = { &#39;on&#39;: &#39;true&#39;, &#39;page&#39;: page, &#39;pageSize&#39;: &#39; 15&#39;, &#39;productName&#39;: &#39;&#39;, &#39;conditionType&#39;: &#39; 1&#39;, &#39;applyname&#39;: &#39;&#39;, &#39;applysn&#39;: &#39;&#39; } json_ids = requests.post(url=url, headers=headers,data=data).json() for i in json_ids[&#39;list&#39;]: id_list.append(i[&#39;ID&#39;]) # 存储所有企业详情数据 post_url = &quot;http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList&quot; for id in id_list: data = { &#39;id&#39;: id } detail_json = requests.post(url=post_url, headers=headers, data=data).json() all_data_list.append(detail_json) #持久化存储 fp = open(&#39;./alldata.json&#39;,&#39;w&#39;,encoding=&#39;utf-8&#39;) json.dump(all_data_list, fp=fp, ensure_ascii=False) print(&#39;over&#39;) a = False . 总结：爬虫检查网页的传输方式，选择get请求还是post请求，注意是否为ajax请求，注意解析数据包的格式 .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/04/29/%E7%88%AC%E8%99%AB09.html",
            "relUrl": "/markdown/2020/04/29/%E7%88%AC%E8%99%AB09.html",
            "date": " • Apr 29, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "爬虫流程及方法08(BeautifulSoup实例)(非ajax请求)",
            "content": "import requests from bs4 import BeautifulSoup a = True while a: headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39; } params = { &#39;_v&#39;: &#39;5.12.0&#39; } fp = open(&#39;./萌新论坛爬虫.text&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) urls = &#39;http://www.lolichan.vip/&#39; response = requests.get(url=urls, params=params, headers=headers).text soup = BeautifulSoup(response, &#39;lxml&#39;) class_list= soup.select(&#39;.node-title&#39;) for li in class_list: try: detail_url = &#39;http://www.lolichan.vip/&#39; + li.a[&#39;href&#39;] detail_page_text = requests.get(url=detail_url, params=params, headers=headers).text detail_soup = BeautifulSoup(detail_page_text, &#39;lxml&#39;) page_list = detail_soup.select(&#39;.structItem-title&#39;) print(&#39;抓取页面成功&#39;) except: page_list = &#39;--&#39; for i in page_list: try: page_title = i.a.string page_url = &#39;http://www.lolichan.vip/&#39; + i.a[&#39;href&#39;] page_text = requests.get(url=page_url, headers=headers).text detail_soup = BeautifulSoup(page_text, &#39;lxml&#39;) div_tag = detail_soup.find(&#39;div&#39;, class_=&#39;bbWrapper&#39;) content = div_tag.text fp.write(page_title + &#39;:&#39; + content + &#39; n&#39;) print(&#39;爬取页面成功&#39;) except: continue a = False . ajax请求请参考爬虫流程及方法09(动态加载页面)(ajax请求)(Json实例) .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/04/29/%E7%88%AC%E8%99%AB08.html",
            "relUrl": "/markdown/2020/04/29/%E7%88%AC%E8%99%AB08.html",
            "date": " • Apr 29, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "爬虫流程及方法07(爬虫技术路线整理)(实时更新)",
            "content": "技术路线 . 1.requests-BeautifulSoup 2.scrapy(5+2结构) 3.scrapy + requests-Beautiful-re + PhantomJS —&gt;表单提交、爬取周期、入库存储(js处理) 4.requests-xpath 5.requests-ccs 6.requests库可与urllib库互换 .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/04/19/%E7%88%AC%E8%99%AB07.html",
            "relUrl": "/markdown/2020/04/19/%E7%88%AC%E8%99%AB07.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "爬虫流程及方法06(Scrapy进阶爬虫)(实时更新)",
            "content": "提高爬取速度的方法 . 1.在setting.py文件里修改并发选项 . . 2.使用scrapy-*的高级补充库，特化某方面，提升速度 . . 3.选择合适的技术路线进行爬虫 .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/04/19/%E7%88%AC%E8%99%AB06.html",
            "relUrl": "/markdown/2020/04/19/%E7%88%AC%E8%99%AB06.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "爬虫流程及方法05(Scrapy入门级爬虫)",
            "content": "Request类 . class scrapy.http.Request() . Request对象表示一个HTTP请求。 由Spider生成，由Downloader执行。 常用属性： . Response类 . class scrapy.http.Response() . Response对象表示一个HTTP响应。 由Downloader生成，由Spider处理。 常用属性与方法： . Item类 . class scrapy.item.Item() . Item对象表示一个从HTML页面中提取的信息内容。 由Spider生成，由Item Pipeline处理。 Item类似字典类型，可以按照字典类型操作。 . Scrapy爬虫支持多种HTML信息提取方法 . Beautiful Soup lxml re XPath Selector CSS Selector . CSS Selector的基本形式: . 步骤 . 步骤1:建立工程和Spider模板 步骤2:编写Spider(实际爬虫) 步骤3:编写ITEM Pipelines(爬虫数据处理) . 步骤3:编写Pipelines: . 配置pipelines.py文件 定义对爬取项(Scraped Item)的处理类 配置ITEM PIPELINES选项(配置setting.py文件) . PS：以下代码仅供参考，不具备运行基础 . #工程文档为以下内容 import scrapy import re class StocksSpider (scrapy . Spider) : name = &quot;stock s&quot; start_urls = [&#39;http://quote.eastmoney.com/stocklist.html&#39;] def parse(self, response): for href in response.css (&#39;a: :attr (href) &#39;).extract(): try: stock = re.findall(r&quot;[s][hz] d{6}&quot;, href)[0] url = &#39;https://gupiao.baidu.com/stock/&#39; + stock + &#39;.html&#39; yield scrapy.Request(url, callback=self.parse_stock) except: continue def parse_stock(self, response): infoDict = { } stockInfo = response.css(&#39;.stock-bets&#39;) name = stockInfo.css(&#39;.bets-name&#39;).extract()[0] keyList = stockInfo.css(&#39;dt&#39;).extract() valueList = stockInfo.css(&#39;dd&#39;).extract() for i in range(len(keyList)): key = re.findall(r&#39;&gt;.*&lt;/dt&gt;&#39;, keyList[i])[0][1:-5] try: val = re.findall(r&#39; d+ .?.*&lt;/dd&gt;&#39;, valueList[1])[0][0:-5] except: val = &#39;-&#39; infoDict[key] = val infoDict.update({&#39;股票名称&#39;: re.findall(&#39; s.* (&#39;, name)[0].split()[0] + re.findall(&#39; &gt;.* &lt;&#39;, name)[0][1:-1]}) yield infoDict . #pipeline文档下的内容 class BaidustocksPipeline(object): def process_item(self, item, spider): return item class BaidustocksInfoPipeline(object): def open_spider(self, spider): self.f = open(&#39;BaidustockInfo.txt&#39;, &#39;W&#39;) def close_spider(self, spider): self.f.close() def process_item(self, item, spider): try: line = str(dict(item)) + &#39; n&#39; self.f.write(line) except: pass return item . 总结： . 技术路线： requests-bs4-re scrapy(5+2结构) scrapy + requests-bs4-re + PhantomJS —&gt;表单提交、爬取周期、入库存储(js处理) .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/04/19/%E7%88%AC%E8%99%AB05.html",
            "relUrl": "/markdown/2020/04/19/%E7%88%AC%E8%99%AB05.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "爬虫流程及方法04(Scrapy框架)",
            "content": "安装scrapy . pycharm安装步骤: 1.打开左上角file 2.打开Other Setting下的Setting for New Project 3.在Project Interpreter选择Project Interpreter里你使用的编译器后，点击加号(+)添加包 . . 4.修改Manage Repositories(参考第三方下载包修改篇) 5.在搜索框里搜索以下包名xxx(注意字母大小写不同) . Scrapy Twisted pywin32 wheel . . 6.在terminal窗口输入scrapy确认出现版本信息及命令提示 . . step1: . 建立工程文档 终端terminal输入： . scrapy startproject 工程文档名 . 创建得到的文档结构： . 工程文档名/ --&gt;外层目录 scrapy.cfg --&gt;部署scrapy爬虫的配置文件 工程文档名/ -&gt;scrapy框架的用户自定义的python代码 _init_.py --&gt;初始化脚本 items.py --&gt;items代码模块（继承类） middlewares.py --&gt;middlewares代码模块（继承类） pipelines.py &gt;pipelines代码模板（继承类） setting.py &gt;scrapy爬虫的配置文件 spiders/ &gt;代码模板目录(继承类) . step2: . 产生爬虫 终端terminal输入：(cmd内或pycharm里面的terminal) . cd 工程文档名 scrapy genspider demo 爬取页面的url . 或者： . 直接在含spider的目录下新建demo.py文件 写入以下代码 . import scrapy class DemoSpider(scrapy.Spider): name = &quot;demo&quot; allowed_domain = [&quot;python123.io&quot;]#爬取该域名下的链接 start_urls = [&#39;https://python123.io/&#39;]#爬取页面的初始页面 def parse(self, response):#解析页面方法类，形成字典类型或发现新链接 pass . step3: . 配置产生的spider爬虫(具体修改demo文件) eg： . def parse(self, response): fname = response.url.split(&#39;/&#39;)[-1] with open(fname, &#39;wb&#39;) as f: f.write(response.body) self.log(&#39;Saved file %s.&#39; % fname) . step4: . 终端terminal运行： 输入以下代码 . scrapy crawl 文件名 eg：scrapy crawl demo . ps:(爬虫的另一种框架) . import scrapy: class DemoSpider(scrapy.Spider): name = &quot;demo&quot; def start requests(se1f): urls = [ &#39;http://python123.io/ws/demo.html &#39; ] for url in urls: yield scrapy.Request(ur1=ur1,callback=self.parse) def parse(self,response): fname = response.url.split(&#39;/&#39;)[-1] with open (fname,&#39;wb&#39;) as f: f. write (response.body) self.log(&#39;Saved file %s.&#39; % fname) . yield关键字 . yield —-&gt;生成器 优势：占用存储少，响应速度快 生成器是一个不断产生值的函数，包含yield语句的函数是一个生成器 生成器每次产生一个值(yield语句)，函数被冻结，被唤醒后再产生一个值。 生成器写法 . def gen(n): for i in range(n): yield i**2 for 1 in gen(5): print(1,&quot; &quot;,end=&quot;&quot;) . 结果 . &gt;&gt;&gt;0 1 4 9 16 . 一般写法 . def square(n) : ls =[i**2 for i in range (n) ] return ls for i in square (5) : print(i,&quot; &quot;,end=&quot;&quot;) . 结果 . &gt;&gt;&gt;0 1 4 9 16 .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/04/17/%E7%88%AC%E8%99%AB04.html",
            "relUrl": "/markdown/2020/04/17/%E7%88%AC%E8%99%AB04.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "爬虫流程及方法03(搜索引擎爬取)",
            "content": "#!/usr/bin/env python3 #本篇介绍抓取含搜索引擎的爬虫 #UA检测：门户网站检测对应请求的身份标识 #UA：useragent（请求载体的身份标识） #UA伪装：伪装游览器 import requests a = True while a: # UA伪装：伪装游览器,将对应user-agent封装到字典headers中 headers = { &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39; }#这里可以使用各种headers，包括ios或者安卓，模拟手机或平板登录 url = &quot;https://www.sogou.com/web?&quot; kw = input(&#39;key words:&#39;)#输入搜索所需要的关键词 # step1:处理url携带的参数：封装到字典中 param = { &#39;query&#39;: kw } res = requests.get(url=url, params=param, headers=headers)#几乎所有大型搜索引擎都是get请求，若属于某些私密的网址搜索可能需要post请求等加密传输方式 # 对指定url发起的请求对应的url是带参数的，请求过程中处理了参数 page_text = res.text #text处理数据 filename = kw + &#39;.html&#39;#命名文件 with open(filename,&#39;w&#39;,encoding=&#39;utf-8&#39;) as fp: #该语法自动创建文件并自动打开关闭文件，需要注意的是该文件在本爬虫文件所在文档内，需要更精确的存储位置推荐使用os库 fp.write(page_text)#输入数据 print(filename, &#39;保存成功&#39;) a = False .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/04/01/%E7%88%AC%E8%99%AB03.html",
            "relUrl": "/markdown/2020/04/01/%E7%88%AC%E8%99%AB03.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "爬虫流程及方法02(Beautiful Soup解析页面)",
            "content": "#!/usr/bin/env python3 #对某论坛的爬取 import requests from bs4 import BeautifulSoup import time #需求：爬取网站标题及详情页的文本 a = True while a:#可转变成实时循环#对首页的页面数据进行爬取 url = &quot;https://www.lolichan.vip/&quot; headers = { &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39; } page_text = requests.get(url=url, headers=headers).text#获取响应数据得加text，不然获取的是响应对象 #在首页中解析出章节标题和详情页URL #1.实例化BeautifulSoup对象，需要将页面源码加载到该对象中 soup = BeautifulSoup(page_text, &#39;lxml&#39;) #解析章节标题与详情页url div_list = soup.select(&#39;.node-body &gt; div &gt; h3 &gt; a&#39;)#使用select层级选择器 fp = open(&#39;./xiangqing.text&#39;,&#39;w&#39;,encoding=&#39;utf-8&#39;)#创建文档及设定只写w和编码utf-8 for div in div_list: time.sleep(填入休息时间)#防止频繁的请求链接失去响应 title = div.string#获得该标签下的所有文本 detail_url = &#39;https://www.lolichan.vip/&#39; + div[&#39;href&#39;]#获得详情页的url #对详情页发起请求，解析出章节内容 detail_page_text = requests.get(url=detail_url, headers=headers).text #解析出详情页中相关的章节内容 detail_soup = BeautifulSoup(detail_page_text,&#39;lxml&#39;) div_tag = detail_soup.select(&#39;.structItem-title &gt; a&#39;) #原来的class属性得用class_表示，不然会报错（class是保留字） content = []#设定空列表 for a in div_tag: content.append(a.text)#往空列表内装填 fp.write(title+&#39;:&#39;+str(content)+&#39; n&#39;)#str()使content对象变为字符串形式 print(title, &#39;爬取成功&#39;)#响应成功 a = False . #号符号是python注释的前置符号 . pycharm的热键： ctrl+z 键回撤你的对代码的改动 ctrl+/键注释与代码的转换 ctrl+c复制 ctrl+v粘贴 . #!/usr/bin/env python3 soup.tagName print(soup.meta)#第一组a标签 . soup.tagName返回的是html中第一次出现的tagName标签 . #!/usr/bin/env python3 soup.find print(soup.find(&#39;meta&#39;))=print(soup.meta) . 属性定位,定位一定class加空格再= #属性可以是class 或id 或attr . #!/usr/bin/env python3 print(soup.find(&#39;div&#39;, class = &#39;song&#39;)) . #!/usr/bin/env python3 soup.find_all(&#39;tagName&#39;) . 找到符合标准的所有标签,返回一个列表 . #!/usr/bin/env python3 print(soup.find_all(&#39;meta&#39;)) . #!/usr/bin/env python3 select(&#39;某种选择器&#39;). . 表示class类选择器 id选择器 标签选择器 返回的是一个列表 . #!/usr/bin/env python3 soup.select(&#39;xx &gt; xx xx&#39;) . 或 . #!/usr/bin/env python3 soup.select(&#39;xx &gt; xx &gt; xx&#39;) . #!/usr/bin/env python3 print(soup.select(&#39;.tang&#39;))#&lt;div class=&#39;tang&#39;&gt; . 部分html代码 . &lt;div class=&#39;tang&#39;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://....&quot; title=&quot;qingming&quot;&gt;mutongyao&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://....&quot; title=&quot;chunjie&quot;&gt;...&lt;/a&gt;&lt;/li&gt; &lt;ul&gt; . print(soup.select(&#39;.tang &gt; ul &gt; li &gt; a&#39;)) . 结果： . &lt;a href=&quot;http://....&quot; title=&quot;qingming&quot;&gt;...&lt;/a&gt;， &lt;a href=&quot;http://....&quot; title=&quot;chunjie&quot;&gt;...&lt;/a&gt; . 层级选择器：此时返回一个列表，此时’.class的内容 &gt; ul &gt; li &gt; a’中大于号&gt;表示一个层级 . #!/usr/bin/env python3 print(soup.select(&#39;.tang &gt; ul &gt; li &gt; a&#39;)[0]) #结果：&lt;a href=&quot;http://....&quot; title=&quot;qingming&quot;&gt;mutongyao&lt;/a&gt; #多个层级&#39;.tang &gt; ul a&#39; 这里空格表示多个层级，大于号&gt;表示一个层级 . #!/usr/bin/env python3 print(soup.select(&#39;.tang &gt; ul &gt; li &gt; a&#39;)[0])=print(soup.select(&#39;.tang &gt; ul a&#39;)[0]) . 获取标签之间的文本数据 . #!/usr/bin/env python3 soup.a.text/string/get_text() print(soup.select(&#39;. class的内容&gt; ul a&#39;)[0].text) . 结果： . mutongyao . #!/usr/bin/env python3 text/get_text() . get_text()可以获取一个标签中‘所有的’文本内容（即使不属于该标签直系文本内容） . string:只可以获得该标签下直系文本内容 . 获取标签中的属性值 . #!/usr/bin/env python3 print(soup.select(&#39;.class的内容 &gt; ul a&#39;)[0][&#39;href&#39;]) . 下行遍历（这里两个方法必须是循环中使用） . #!/usr/bin/env python3 for child in soup.body.children: print(child) . 遍历儿子节点 . #!/usr/bin/env python3 for child in soup.body.descendants: print(child) . 遍历子孙节点 . 上行遍历 . #!/usr/bin/env python3 for sibling in soup.a.next_siblings: print(sibling) . 遍历后续节点 . #!/usr/bin/env python3 for sibling in soup.a.previous_siblings: print(sibling) . 遍历前续节点 . soup.prettify() . 每个节点一个换行符 . &lt;&gt; .find_all(name, attrs, recursive, string, **kwargs） . 返回一个列表类型，存储查找的结果。 . name:对标签名称的检索字符串。 attrs:对标签属性值的检索字符串，可标注属性检索。 recursive:是否对子孙全部检索，默认True。 string: &lt;&gt;…&lt;/&gt;中字符串区域的检索字符串。 .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/03/31/%E7%88%AC%E8%99%AB02.html",
            "relUrl": "/markdown/2020/03/31/%E7%88%AC%E8%99%AB02.html",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "爬虫流程及方法01(入门准备及Request库使用)",
            "content": "爬虫究竟是合法还是违法的? . 在法律中是不被禁止 具有违法风险 . 请善意爬虫 . 切勿恶意爬虫 . 爬虫带来的风险可以体现在如下2方面: . 爬虫干扰了被访问网站的正常运营 爬虫抓取了受到法律保护的特定类型的数据或信息 . 如何在使用编写爬虫的过程中避免进入局子的厄运呢? . 时常的优化自己的程序，避免干扰被访问网站的正常运行 在使用传播爬取到的数据时，审查抓取到的内容，如果发现了涉及到用户的商业机密等敏感内容需要及时停止爬取或传播 . 爬虫的矛与盾 . 反爬机制:门户网站，可以通过制定相应的策略或者技术手段，防止爬虫程序进行网站数据的爬取。 反反爬策略:爬虫程序可以通过制定相关的策略或者技术手段，破解]户网站中具备的反爬机制，从而可以获取门户网 . robots. txt协议:? . 君子协议:规定了网站中哪些数据可以被爬虫爬取哪些数据不可以被爬取。 . 例如这个：https://www.bilibili.com/robots.txt . 正文： . ### . 软件：pycharm/thonny/自带IDLE,明白python的基本语法，缩进/字典/元组/列表/循环结构/函数运用/文件存储（绝对路径/相对路径） . step1.安装第三方模块：调出cmd窗口输入以下字符 . （每一行是一个包，等待下载完后再进行下一个包的下载） . python -m pip install --upgrade pip pip install requests pip install bs4 pip install lxml pip install urllib . step2.确认爬虫流程： . 1.指定url 2.请求前进行UA伪装（模拟游览器发出请求） 3.选择post还是get请求 4.请求发送 5.获取响应数据 6.进行存储 . step3.实际代码的编写： . import requests #每使用一个包的方法就得导入一个包 #（引入包后空两行，语法的正确书写习惯） a = True while a: #这里可以改成循环结构对网页进行实时爬取，每次爬取覆盖上次的成果 url = &quot;网址&quot; #指定所爬取页面的网址 headers = { &#39;user-agent&#39;:&#39;粘贴1处&#39; } #在页面中点击右键选择检查，调出网页自带的抓包工具，在network中刷新当前页面抓包找到user-agent的项复制粘贴1，找到Query String Parameters的项复制粘贴2（记得加符号’粘贴2‘） params = { &#39;某名字&#39;:&#39;某值&#39;, &#39;粘贴2&#39;:&#39;黏贴2&#39; } res = requests.get(url=url,params=params,headers=headers) #这里是对网页发起请求并内置参数 dict_object = res.text #对返回内容进行text处理并赋值给一个字典 with open(&quot;./lolichan.html&quot;,&quot;w&quot;,encoding=&#39;utf-8&#39;) as fp: #打开或生成一个文档并选定为写w的状态，转换字符的编码为utf-8 fp.write(dict_object)#往文档内存储爬取的网页源码 print(&#39;over&#39;) #存储成功提示 a = False . step4.实际运用： . 安装而未用到的包下次再讲，剩下的包用于数据的解析定位 如果想要看效果视频，参照B站视频av92683334 更多内容: 【官方文档】opencv-python中文文档 .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/03/20/%E7%88%AC%E8%99%AB01.html",
            "relUrl": "/markdown/2020/03/20/%E7%88%AC%E8%99%AB01.html",
            "date": " • Mar 20, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "安装python第三方库的小技巧",
            "content": "直接放代码 . import os libs = { &quot;requests&quot;,&quot;jieba&quot;,&quot;beautifulsoup4&quot;, &quot;django&quot;,&quot;flask&quot;, &quot;此处填写你需要下载的库的名称，注意大小写并拼写正确，样式如上面例子&quot;,&quot;pandas&quot; } try: for lib in libs: os.system(&#39;pip install &#39;+lib) print(&quot;Successful&quot;) except: print(&#39;error&#39;) . os.system(command) . command 为要执行的命令，近似于Windows下cmd窗口中输入的命令。 .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/03/15/%E5%AE%89%E8%A3%85python%E5%BA%93%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7.html",
            "relUrl": "/markdown/2020/03/15/%E5%AE%89%E8%A3%85python%E5%BA%93%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Markdown文档书写简单示例",
            "content": "markdown文档书写的简单示例 . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "本博客展示本人的Python学习笔记。 [^1]:This website is powered by fastpages . [^2].a blogging platform that natively supports Jupyter notebooks in addition to other formats. .",
          "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": "Thanks for your reading, this is my study notes for Python, if you find something wrong in my essay, please leave a comment and I will see. . Posts .",
          "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Thanks for your reading, this is my study notes for Python, if you find something wrong in my essay, please leave a comment and I will see. . Posts .",
          "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/page2/",
          "relUrl": "/page2/",
          "date": ""
      }
      
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://spiritlhl.github.io/jupyter_notebook_blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}