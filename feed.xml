<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://spiritlhl.github.io/jupyter_notebook_blogs/feed.xml" rel="self" type="application/atom+xml" /><link href="https://spiritlhl.github.io/jupyter_notebook_blogs/" rel="alternate" type="text/html" /><updated>2021-01-16T03:11:33-06:00</updated><id>https://spiritlhl.github.io/jupyter_notebook_blogs/feed.xml</id><title type="html">SpiritLHL’s Python blog</title><subtitle>Personally explore Python's study notes.</subtitle><entry><title type="html">Python新线程使用</title><link href="https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/08/31/%E5%88%9B%E5%BB%BA%E6%96%B0%E7%BA%BF%E7%A8%8B(%E9%80%9A%E7%94%A8).html" rel="alternate" type="text/html" title="Python新线程使用" /><published>2020-08-31T00:00:00-05:00</published><updated>2020-08-31T00:00:00-05:00</updated><id>https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/08/31/%E5%88%9B%E5%BB%BA%E6%96%B0%E7%BA%BF%E7%A8%8B(%E9%80%9A%E7%94%A8)</id><author><name></name></author><category term="markdown" /><summary type="html">创建并使用多线程 print('主线程执行代码') # 从 threading 库中导入Thread类 from threading import Thread from time import sleep # 定义一个函数，作为新线程执行的入口函数 def threadFunc(arg1,arg2): print('子线程 开始') print(f'线程函数参数是：{arg1}, {arg2}') sleep(5) print('子线程 结束') # 创建 Thread 类的实例对象， 并且指定新线程的入口函数，此时并没有执行 thread = Thread(target=threadFunc, args=('参数1', '参数2') ) #target=threadFunc对应执行的函数threadFunc #args=('参数1', '参数2')这样新进程添加参数 # 执行start 方法，就会创建新线程， # 并且新线程会去执行入口函数里面的代码。 # 这时候这个进程有两个线程了。↓ thread.start() # 主线程的代码执行 子线程对象的join方法， # 就会等待子线程结束，才继续执行下面的代码 thread.join() print('主线程结束') 运行该程序，解释器执行到下面代码时 thread = Thread(target=threadFunc, args=('参数1', '参数2') ) 创建了一个Thread实例对象，其中，Thread类的初始化参数 有两个 target参数 是指定新线程的 入口函数， 新线程创建后就会 执行该入口函数里面的代码， args 指定了 传给 入口函数threadFunc 的参数。 线程入口函数 参数，必须放在一个元组里面，里面的元素依次作为入口函数的参数。 注意，上面的代码只是创建了一个Thread实例对象， 但这时，新的线程还没有创建。 要创建线程，必须要调用 Thread 实例对象的 start方法 。也就是执行完下面代码的时候 thread.start() 新的线程才创建成功，并开始执行 入口函数threadFunc 里面的代码。 有的时候， 一个线程需要等待其它的线程结束，比如需要根据其他线程运行结束后的结果进行处理。 这时可以使用 Thread对象的 join 方法 thread.join() 如果一个线程A的代码调用了 对应线程B的Thread对象的 join 方法，线程A就会停止继续执行代码，等待线程B结束。 线程B结束后，线程A才继续执行后续的代码。 所以主线程在执行上面的代码时，就暂停在此处， 一直要等到 新线程执行完毕，退出后，才会继续执行后续的代码。 ```python #错误示例！！！！ thread = Thread(target=threadFunc('参数1', '参数2')) ↑如果这样写无法创建新线程并执行，这样target传入不是函数，传入的是运行结果(null)，而且是在主线程运行完了，并不是在子线程里运行。 共享数据的访问控制 做多线程开发，经常遇到这样的情况：多个线程里面的代码需要访问同一个公共的数据对象。 这个公共的数据对象可以是任何类型， 比如一个列表、字典、或者自定义类的对象。 有的时候，程序需要防止线程的代码同时操作公共数据对象。否则，就有可能导致数据的访问互相冲突影响。 请看一个例子。 我们用一个简单的程序模拟一个银行系统，用户可以往自己的帐号上存钱。 对应代码如下： from threading import Thread from time import sleep bank = { 'byhy' : 0 } # 定义一个函数，作为新线程执行的入口函数 def deposit(theadidx,amount): balance = bank['byhy'] # 执行一些任务，耗费了0.1秒 sleep(0.1) bank['byhy'] = balance + amount print(f'子线程 {theadidx} 结束') theadlist = [] for idx in range(10): thread = Thread(target = deposit, args = (idx,1) ) thread.start() # 把线程对象都存储到 threadlist中 theadlist.append(thread) for thread in theadlist: thread.join() print('主线程结束') print(f'最后我们的账号余额为 {bank[&quot;byhy&quot;]}') 上面的代码中，一起执行 开始的时候， 该帐号的余额为0，随后我们启动了10个线程， 每个线程都deposit函数，往帐号byhy上存1元钱。 可以预期，执行完程序后，该帐号的余额应该为 10。 然而，我们运行程序后，发现结果如下 子线程 0 结束 子线程 3 结束 子线程 2 结束 子线程 4 结束 子线程 1 结束 子线程 7 结束 子线程 5 结束 子线程 9 结束 子线程 6 结束 子线程 8 结束 主线程结束 最后我们的账号余额为 1 为什么是 1 呢？ 而不是 10 呢？ 如果在我们程序代码中，只有一个线程，如下所示 from time import sleep bank = { 'byhy' : 0 } # 定义一个函数，作为新线程执行的入口函数 def deposit(theadidx,amount): balance = bank['byhy'] # 执行一些任务，耗费了0.1秒 sleep(0.1) bank['byhy'] = balance + amount for idx in range(10): deposit (idx,1) print(f'最后我们的账号余额为 {bank[&quot;byhy&quot;]}') 代码都是串行执行的。不存在多线程同时访问bank对象的问题，运行结果一切都是正常的。 现在我们程序代码中，有多个线程，并且在这个几个线程中都会去调用deposit，就有可能同时操作这个bank对象，就有可能出一个线程覆盖另外一个线程的结果的问题。 这时，可以使用threading库里面的锁对象Lock去保护。 我们修改多线程代码，如下： from threading import Thread,Lock from time import sleep bank = { 'byhy' : 0 } bankLock = Lock() # 定义一个函数，作为新线程执行的入口函数 def deposit(theadidx,amount): # 操作共享数据前，申请获取锁 bankLock.acquire() balance = bank['byhy'] # 执行一些任务，耗费了0.1秒 sleep(0.1) bank['byhy'] = balance + amount print(f'子线程 {theadidx} 结束') # 操作完共享数据后，申请释放锁 bankLock.release() theadlist = [] for idx in range(10): thread = Thread(target = deposit, args = (idx,1) ) thread.start() # 把线程对象都存储到 threadlist中 theadlist.append(thread) for thread in theadlist: thread.join() print('主线程结束') print(f'最后我们的账号余额为 {bank[&quot;byhy&quot;]}') 执行一下，结果如下 子线程 0 结束 子线程 1 结束 子线程 2 结束 子线程 3 结束 子线程 4 结束 子线程 5 结束 子线程 6 结束 子线程 7 结束 子线程 8 结束 子线程 9 结束 主线程结束 最后我们的账号余额为 10 正确了。 每个线程在操作共享数据对象之前，都应该申请获取操作权，也就是调用该共享数据对象对应的锁对象的acquire方法。 如果线程A执行如下代码，调用acquire方法的时候， ```python bankLock.acquire() 别的线程B已经申请到了这个锁，并且还没有释放，那么线程A的代码就在此处等待线程B释放锁，不去执行后面的代码。 直到线程B执行了锁的release方法释放了这个锁，线程A才可以获取这个锁，就可以执行下面的代码了。 如果这时线程B又执行这个锁的acquire方法，就需要等待线程A执行该锁对象的release方法释放锁，否则也会等待，不去执行后面的代码。 daemon线程 from threading import Thread from time import sleep def threadFunc(): sleep(2) print('子线程 结束') thread = Thread(target=threadFunc) thread.start() print('主线程结束') 可以发现，主线程先结束，要过个2秒钟，等子线程运行完，整个程序才会结束退出。 因为： Python程序中当所有的 非daemon线程 结束了，整个程序才会结束 主线程是非daemon线程，启动的子线程缺省也是非daemon线程线程。 所以，要等到主线程和子线程都结束，程序才会结束。 我们可以在创建线程的时候，设置daemon参数值为True，如下 from threading import Thread from time import sleep def threadFunc(): sleep(2) print('子线程 结束') thread = Thread(target=threadFunc, daemon=True # 设置新线程为daemon线程 ) thread.start() print('主线程结束') 再次运行，可以发现，只要主线程结束了，整个程序就结束了。因为只有主线程是非daemon线程。</summary></entry><entry><title type="html">2019新型冠状病毒（COVID-19/2019-nCoV）疫情分析</title><link href="https://spiritlhl.github.io/jupyter_notebook_blogs/2020/07/01/Cov2019Analysis.html" rel="alternate" type="text/html" title="2019新型冠状病毒（COVID-19/2019-nCoV）疫情分析" /><published>2020-07-01T00:00:00-05:00</published><updated>2020-07-01T00:00:00-05:00</updated><id>https://spiritlhl.github.io/jupyter_notebook_blogs/2020/07/01/Cov2019Analysis</id><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">爬虫流程及方法14(正则表达式篇)</title><link href="https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/05/03/%E7%88%AC%E8%99%AB14.html" rel="alternate" type="text/html" title="爬虫流程及方法14(正则表达式篇)" /><published>2020-05-03T00:00:00-05:00</published><updated>2020-05-03T00:00:00-05:00</updated><id>https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/05/03/%E7%88%AC%E8%99%AB14</id><author><name></name></author><category term="markdown" /><summary type="html">前言 re库的实用实例如下 import requests import re import os a = True while a: #创建一个文件夹，保存所有图片 if not os.path.exists('./tupianLibs'): os.mkdir('./tupianLibs') headers = { 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36' } url = &quot;https://www.pexels.com/&quot; #使用通用爬虫对整张页面进行爬取 page_text = requests.get(url=url, headers=headers).text #使用聚焦爬虫将页面中所有的图片进行解析/提取 #正则.*?表示一切内容 #re.S单行匹配 ex = '&amp;lt;a class=&quot;js-photo-link photo-item__link&quot; style.*? &amp;gt;.*?&amp;lt;img srcset=&quot;(.*?)&quot; class.*?&amp;gt;&amp;lt;/div&amp;gt;' image_src_list = re.findall(ex, page_text, re.S ) for src in image_src_list: src = 'https:'+ src #拼接出一个完整的图片url image_data = requests.get(url=src, headers=headers).content #请求到了图片的二进制数据 image_name = src.split('/')[-1] #生成图片名称 imgPath = './tupianlbs/' + image_name #图片最终存储的路径 with open(imgPath, 'W') as fp: fp.write(image_data) print('下载成功') a = False 正则表达式详情 raw string类型区别于原生字符串类型（不包含转义字符） r'[1-9]\d{5}' re.search(pattern, string, flags=0) 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象。 pattern:正则表达式的字符串或原生字符串表示 string:待匹配字符串 flags:正则表达式使用时的控制标记 re.split(pattern, string, maxsplit=0, flags=0) 将一个字符串按照正则表达式匹配结果进行分割,返回列表类型。 pattern:正则表达式的字符串或原生字符串表示 string:待匹配字符串 maxsplit:最大分割数，剩余部分作为最后一个元素输出 flags:正则表达式使用时的控制标记 re.finditer(pattern, string, flags=0) 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象。 pattern:正则表达式的字符串或原生字符串表示 string:待匹配字符串 flags:正则表达式使用时的控制标记 re.sub(pattern, repl, string, count=0, flags=0) 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串。 pattern:正则表达式的字符串或原生字符串表示 repl:替换匹配字符串的字符串 string:待匹配字符串 flags:正则表达式使用时的控制标记 regex = re.compile(pattern, flags=0) 将正则表达式的字符串形式编译成正则表达式对象 pattern:正则表达式的字符串或原生字符串表示 flags:正则表达式使用时的控制标记 Match:对象的属性 match = re.search(r'PY.*N',' PYANBNCNDN' ) match.group(0) Re库默认采用贪婪匹配，即输出匹配最长的子串。 输出’PYANBNCNDN’ match = re.search(r'PY. *?N', ' PYANBNCNDN') match group(0) 输出’PYAN ‘ import re restr=&quot;&amp;lt;td data-v-80203e10=&quot;&quot;&amp;gt;（\\d+）&amp;lt;/td&amp;gt;&quot;#括号表示只取数据（数字） regex=re.compile(restr,re.IGNORECASE) mylist=regex.findall(province)</summary></entry><entry><title type="html">爬虫流程及方法13(xpath解析页面)</title><link href="https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/05/03/%E7%88%AC%E8%99%AB13.html" rel="alternate" type="text/html" title="爬虫流程及方法13(xpath解析页面)" /><published>2020-05-03T00:00:00-05:00</published><updated>2020-05-03T00:00:00-05:00</updated><id>https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/05/03/%E7%88%AC%E8%99%AB13</id><author><name></name></author><category term="markdown" /><summary type="html">前言 xpath解析原理: 1.实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。 2.调用et ree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。 环境的安装: pip install Lxml 如何实例化一个etree对象: from Lxml import etree 1.将本地的html文档中的源码数据加载到etree对象中: etree.parse(fiLePath) 2.可以将从互联网上获取的源码数据加裁到该对象中 etree.HTML( 'page_ text' ) xpath('xpath表达式') #3.7版本后引入etree模块如下，3.5版本以下可以直接从lxml中引入 from lxml import html etree = html.etree parser = etree.HTMLParser(encoding=&quot;utf-8&quot;) #实例化好了一个etree对象，且将被解析的源码加载到了该对象中 tree = etree.parse('bs4练习.html', parser=parser) # r = tree.xpath('/html/body/div') # r = tree.xpath('/html//div') # r = tree.xpath('//div') /表示的是从根节点开始定位，一个/x/表示一个层级，//表示跨越多个层级,可以表示从任意位置开始定位 r = tree.xpath('//div[@class=&quot;song&quot;]') 属性定位： //div[@class=&quot;song&quot;] tag[@attrName=&quot;attrValue&quot;] r = tree.xpath('//div[@class=&quot;song&quot;]/p[3]') 索引定位： '//div[@class=&quot;song&quot;]/p[3]' #这里索引以1开始 r = tree.xpath('//div[@class=&quot;tang&quot;]//li[5]/a/text()')[0] 取文本： /text() #获取的是标签中直系的文本内容 //text() #获取标签中非直系文本内容（所有文本内容） ```python r = tree.xpath('//div[@class=&quot;tang&quot;]/text()') 取属性： /@attrName ==&amp;gt;img/src r = tree.xpath('//div[@class=&quot;song&quot;]/img/@src') 实际案例： #3.7版本后引入etree模块如下，3.5版本以下可以直接从lxml中引入 from lxml import html import requests etree = html.etree a = True while a: #爬取页面源码数据 url = 'https://mm.58.com/ershoufang/' headers = { 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36' } page_text = requests.get(url=url, headers=headers).text #数据解析 tree = etree.HTML(page_text) li_list = tree.xpath('//ul[@class=&quot;house-list-wrap&quot;]/li') fp = open('58.txt', 'w', encoding='utf-8') for li in li_list: #页面数据局部解析 title = li.xpath('./div[2]/h2/a/text()')[0]#./表示从前面的li开始（局部开始） print(title) fp.write(title+'\n') a = False</summary></entry><entry><title type="html">爬虫流程及方法12(高性能异步爬虫)</title><link href="https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/05/01/%E7%88%AC%E8%99%AB12.html" rel="alternate" type="text/html" title="爬虫流程及方法12(高性能异步爬虫)" /><published>2020-05-01T00:00:00-05:00</published><updated>2020-05-01T00:00:00-05:00</updated><id>https://spiritlhl.github.io/jupyter_notebook_blogs/markdown/2020/05/01/%E7%88%AC%E8%99%AB12</id><author><name></name></author><category term="markdown" /><summary type="html">前言 目的:在爬虫中使用异步实现高性能的数据爬取操作。 异步爬虫的方式: –多线程，多进程(不建议): 好处:可以为相关阻塞的操作单独开启线程或者进程，阻塞操作就可以异步执行。 弊端:无法无限制的开启多线程或者多进程。 ps:get方法与post方法是阻塞的方法 –线程池、进程池(适当的使用)： 好处:我们可以降低系统对进程或者线程创建和销毁的一个频率，从而很好的降低系统的开销。 弊端:池中线程或进程的数量是有上限。| 模拟多线程操作 单线程模拟： import time #使用单线程串行方式执行 def get_page(str): print(&quot;正在下载: &quot;, str) time.sleep(2) print('下载成功: ', str) name_list = ['xiaozi', 'aa','bb','cc'] start_time = time.time() for i in range(len(name_list)): get_page(name_list[i]) end_time = time.time() print('%d second' % (end_time-start_time)) 结果:8s 线程池模拟： import time #导入线程池模块对应的类 from multiprocessing.dummy import Pool #使用线程池方式进行 #导入线程池所对应的pool start_time = time.time()#程序开始时计时 def get_page(str): print(&quot;正在下载: &quot;, str) time.sleep(2) print('下载成功: ', str) name_list = ['xiaozi', 'aa','bb','cc']#可迭代对象 #实例化一个线程对象 pool = Pool(4)#线程池开启4个线程 #将列表中每一个列表元素传递给get_page进行处理 pool.map(get_page, name_list)#若有返回值返回的是列表，因为多次传入到map end_time = time.time()#程序结束时结束计时 print(end_time-start_time) 结果:2s 实际案例 提取js动态加载内容，使用re正则匹配 js源码 var contId=&quot;1671755&quot;,liveStatusUrl=&quot;liveStatus.jsp&quot;,liveSta=&quot;&quot;,playSta=&quot;1&quot;,autoPlay=!1,isLiving=!1,isVrVideo=!1,hdflvUrl=&quot;&quot;,sdflvUrl=&quot;&quot;,hdUrl=&quot;&quot;,sdUrl=&quot;&quot;,ldUrl=&quot;&quot;,srcUrl=&quot;https://video.pearvideo.com/mp4/third/20200429/cont-1671755-11742488-084919-hd.mp4&quot;,vdoUrl=srcUrl,skinRes=&quot;//www.pearvideo.com/domain/skin&quot;,videoCDN=&quot;//video.pearvideo.com&quot; 正则匹配 ex = ‘srcUrL=”( .*? )”,vdoUrl’ 分组操作提取链接 import requests from bs4 import BeautifulSoup import re from multiprocessing import Pool def get_video_data(dic): headers = { 'User-Agent': &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&quot; } #使用线程池对视频数据进行请求（较为耗时的堵塞操作） url = dic['url'] print(dic['name'], '正在下载') data = requests.get(url=url, headers=headers, timeout=0.5).content #持久化存储操作 with open(dic['name'], 'wb') as fp: fp.write(data) print(dic['name'], '下载成功') if __name__ == '__main__': headers = { 'User-Agent':&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&quot; } url = 'https://www.pearvideo.com/category_5' page_text = requests.get(url=url, headers=headers).text soup = BeautifulSoup(page_text, 'lxml') li_urls = soup.select('.vervideo-bd') urls = []#存储所有视频的链接和名字 i = 1 for li in li_urls: try: i = i + 1 detail_url = 'https://www.pearvideo.com/' + li.a['href'] name = soup.select('.vervideo-title')[i].text+'.mp4' detail_page_text = requests.get(url=detail_url, headers=headers, timeout=0.5).text ex = 'srcUrl=&quot;(.*?)&quot;,vdoUrl' video_url = re.findall(ex, detail_page_text)[0] dic = { 'name': name, 'url': video_url } urls.append(dic) except: continue pool = Pool(4) pool.map(get_video_data, urls) pool.close() pool.join() 总结： 1.windows环境下需要将主函数放在以下代码下方 if __name__ == '__main__': mac环境下不需要此操作 2.下载时下载二进制数据，使用’wb’而不是’w’。 3.如果下载视频过多(爬取大量数据)，网站要求验证证书，大量爬取需要使用其他方法应对ssl反爬策略。 ps：感谢csdn学院提供的案例支持</summary></entry></feed>